{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Handwash Full Pipeline (Kaggle) - QAT\nSelf contained notebook for Kaggle with quantization-aware training.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install dependencies\n!pip install -q --no-cache-dir scikit-learn pandas numpy opencv-python-headless matplotlib seaborn tqdm requests tensorflow-model-optimization\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import matplotlib.pyplot as plt\nfrom IPython.display import clear_output, display\nfrom tensorflow import keras\n\n# =========================\n# Standard library\n# =========================\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\n\n# =========================\n# Third-party\n# =========================\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\n\n# =========================\n# TensorFlow / Keras\n# =========================\nimport tensorflow as tf\nimport tensorflow_model_optimization as tfmot\n\nRANDOM_SEED = 42\nIMG_SIZE = (224, 224)\nNUM_CLASSES = 7\nCLASS_NAMES = ['Other', 'Step1_PalmToPalm', 'Step2_PalmOverDorsum', 'Step3_InterlacedFingers', 'Step4_BackOfFingers', 'Step5_ThumbRub', 'Step6_Fingertips']\nKAGGLE_URL = 'https://github.com/atiselsts/data/raw/master/kaggle-dataset-6classes.tar'\nKAGGLE_CLASS_MAPPING = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, 'step1': 1, 'step2': 2, 'step3': 3, 'step4': 4, 'step5': 5, 'step6': 6, 'other': 0}\nTRAIN_RATIO = 0.7\nVAL_RATIO = 0.15\nTEST_RATIO = 0.15\n\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\ntry:\n    tf.random.set_seed(RANDOM_SEED)\nexcept Exception:\n    pass\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os, sys, json, time, math, random, shutil, subprocess\nfrom pathlib import Path\n\nRUN_NAME = os.environ.get(\"RUN_NAME\", \"handwash_qat_run\")\nKAGGLE_WORKING = Path(\"/kaggle/working\")\nif not KAGGLE_WORKING.exists():\n    KAGGLE_WORKING = Path.cwd()\n\nWORK_DIR = KAGGLE_WORKING / \"handwash_runs\" / RUN_NAME\nDATA_DIR = KAGGLE_WORKING / \"handwash_data\"\n\nRAW_DIR = DATA_DIR / \"raw\"\nPROCESSED_DIR = DATA_DIR / \"processed\"\nMODELS_DIR = WORK_DIR / \"models\"\nCKPT_DIR = WORK_DIR / \"checkpoints\"\nLOGS_DIR = WORK_DIR / \"logs\"\n\nfor p in [WORK_DIR, DATA_DIR, RAW_DIR, PROCESSED_DIR, MODELS_DIR, CKPT_DIR, LOGS_DIR]:\n    p.mkdir(parents=True, exist_ok=True)\n\nprint(\"WORK_DIR:\", WORK_DIR)\nprint(\"DATA_DIR:\", DATA_DIR)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Configuration\nAll options are user editable.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# User config (edit these)\nDATASETS = [\"kaggle\"]\n\n# Training config\nBATCH_SIZE = 32\nEPOCHS_FLOAT = 1\nEPOCHS_QAT = 5\nLEARNING_RATE_FLOAT = 1e-4\nLEARNING_RATE_QAT = 1e-5\n\n# Quantization config\nDISABLE_PER_CHANNEL = False\n\n# Data sampling config\nFRAME_STRIDE = 2\nMAX_FRAMES_PER_VIDEO = 8\n\n# Mixed precision\nMIXED_PRECISION = False\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Auto-tune and mixed precision\nif MIXED_PRECISION:\n    from tensorflow.keras import mixed_precision\n    mixed_precision.set_global_policy(\"mixed_float16\")\n    print(\"Mixed precision enabled\")\n\n# Multi-GPU strategy\n_gpus = tf.config.list_physical_devices('GPU')\nif len(_gpus) > 1:\n    STRATEGY = tf.distribute.MirroredStrategy()\nelse:\n    STRATEGY = tf.distribute.get_strategy()\nNUM_REPLICAS = STRATEGY.num_replicas_in_sync\nprint('Using strategy:', STRATEGY, 'replicas:', NUM_REPLICAS)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Download and preprocess\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import requests\nfrom tqdm import tqdm\nimport tarfile\nfrom IPython.display import Video, display\n\nVIDEO_EXTS = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n\nKAGGLE_DIR = RAW_DIR / \"kaggle\"\nKAGGLE_TAR = KAGGLE_DIR / \"kaggle-dataset-6classes.tar\"\nKAGGLE_EXTRACTED = KAGGLE_DIR / \"kaggle-dataset-6classes\"\n\n\ndef download_with_progress(url: str, dest: Path):\n    dest.parent.mkdir(parents=True, exist_ok=True)\n    if dest.exists():\n        print(\"skip\", dest)\n        return\n    with requests.get(url, stream=True, timeout=30) as r:\n        r.raise_for_status()\n        total = int(r.headers.get(\"content-length\", 0))\n        with open(dest, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dest.name) as pbar:\n            for chunk in r.iter_content(chunk_size=1024 * 1024):\n                if not chunk:\n                    continue\n                f.write(chunk)\n                pbar.update(len(chunk))\n\n\ndef extract_tar(tar_path: Path, extract_root: Path):\n    extract_root.mkdir(parents=True, exist_ok=True)\n    with tarfile.open(tar_path, \"r\") as tar:\n        tar.extractall(path=extract_root)\n\n\ndef find_kaggle_input_dataset():\n    root = Path(\"/kaggle/input\")\n    if not root.exists():\n        return None\n    for item in root.iterdir():\n        if not item.is_dir():\n            continue\n        candidate = item / \"kaggle-dataset-6classes\"\n        if candidate.exists():\n            return candidate\n        if all((item / str(i)).exists() for i in range(NUM_CLASSES)):\n            return item\n    return None\n\n\nDATA_ROOT = find_kaggle_input_dataset()\nif DATA_ROOT is not None:\n    print(\"Using Kaggle input dataset:\", DATA_ROOT)\nelse:\n    print(\"Kaggle input dataset not found. Downloading...\")\n    if not KAGGLE_EXTRACTED.exists():\n        download_with_progress(KAGGLE_URL, KAGGLE_TAR)\n        extract_tar(KAGGLE_TAR, KAGGLE_DIR)\n    DATA_ROOT = KAGGLE_EXTRACTED\n\nprint(\"Dataset root:\", DATA_ROOT)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from sklearn.model_selection import train_test_split\n\n\ndef kaggle_class_id_from_folder(name: str) -> int:\n    name_lower = name.lower()\n    if name_lower in KAGGLE_CLASS_MAPPING:\n        return int(KAGGLE_CLASS_MAPPING[name_lower])\n    digits = \"\".join(ch for ch in name_lower if ch.isdigit())\n    if digits:\n        class_id = int(digits)\n        if 0 <= class_id < len(CLASS_NAMES):\n            return class_id\n    raise ValueError(f\"Unknown Kaggle class folder: {name}\")\n\n\ndef collect_videos(dataset_root: Path) -> pd.DataFrame:\n    rows = []\n    for class_dir in sorted(dataset_root.iterdir()):\n        if not class_dir.is_dir():\n            continue\n        class_id = kaggle_class_id_from_folder(class_dir.name)\n        for vid in class_dir.iterdir():\n            if vid.suffix.lower() not in VIDEO_EXTS:\n                continue\n            rows.append({\"video_path\": str(vid), \"class_id\": class_id})\n    df = pd.DataFrame(rows)\n    if df.empty:\n        raise RuntimeError(f\"No videos found in {dataset_root}\")\n    return df\n\n\nvideos_df = collect_videos(DATA_ROOT)\nprint(\"Total videos:\", len(videos_df))\n\ntrain_df, temp_df = train_test_split(\n    videos_df,\n    test_size=(VAL_RATIO + TEST_RATIO),\n    stratify=videos_df[\"class_id\"],\n    random_state=RANDOM_SEED,\n)\nval_size = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=(1.0 - val_size),\n    stratify=temp_df[\"class_id\"],\n    random_state=RANDOM_SEED,\n)\n\nprint(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_v2_preprocess\n\n\ndef _load_random_frame_py(video_path_bytes, label):\n    video_path = video_path_bytes.decode(\"utf-8\")\n    cap = cv2.VideoCapture(video_path)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if frame_count <= 0:\n        cap.release()\n        raise RuntimeError(f\"No frames in {video_path}\")\n    target_idx = np.random.randint(0, frame_count)\n    cap.set(cv2.CAP_PROP_POS_FRAMES, target_idx)\n    ok, frame = cap.read()\n    cap.release()\n    if not ok:\n        raise RuntimeError(f\"Failed reading frame from {video_path}\")\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame = cv2.resize(frame, IMG_SIZE)\n    frame = frame.astype(np.float32)\n    frame = mobilenet_v2_preprocess(frame)\n    return frame, np.int32(label)\n\n\ndef _load_random_frame(video_path, label):\n    frame, label = tf.py_function(\n        _load_random_frame_py,\n        inp=[video_path, label],\n        Tout=[tf.float32, tf.int32],\n    )\n    frame.set_shape((*IMG_SIZE, 3))\n    label.set_shape(())\n    return frame, label\n\n\ndef make_dataset(df, batch_size=32, shuffle=True):\n    ds = tf.data.Dataset.from_tensor_slices((df[\"video_path\"].values, df[\"class_id\"].values))\n    if shuffle:\n        ds = ds.shuffle(len(df), seed=RANDOM_SEED, reshuffle_each_iteration=True)\n    ds = ds.map(_load_random_frame, num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds\n\n\ntrain_ds = make_dataset(train_df, batch_size=BATCH_SIZE, shuffle=True)\nval_ds = make_dataset(val_df, batch_size=BATCH_SIZE, shuffle=False)\ntest_ds = make_dataset(test_df, batch_size=BATCH_SIZE, shuffle=False)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Build QAT model\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from tensorflow.keras import layers, models\n\n\ndef build_float_model():\n    base = tf.keras.applications.MobileNetV2(\n        input_shape=(*IMG_SIZE, 3),\n        include_top=False,\n        weights=\"imagenet\",\n    )\n    base.trainable = True\n    inputs = layers.Input(shape=(*IMG_SIZE, 3))\n    x = base(inputs, training=True)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n    model = models.Model(inputs, outputs)\n    return model\n\n\nwith STRATEGY.scope():\n    float_model = build_float_model()\n    float_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_FLOAT),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[\"accuracy\"],\n    )\n\nfloat_model.summary()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Warmup (float) and QAT fine-tune\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "history_float = float_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS_FLOAT,\n)\n\nwith STRATEGY.scope():\n    quantize_model = tfmot.quantization.keras.quantize_model\n    qat_model = quantize_model(float_model)\n    qat_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_QAT),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[\"accuracy\"],\n    )\n\nqat_model.summary()\n\nhistory_qat = qat_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS_QAT,\n)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Evaluate\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "qat_eval = qat_model.evaluate(test_ds, verbose=1)\nprint(\"QAT test metrics:\", dict(zip(qat_model.metrics_names, qat_eval)))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Export INT8 TFLite\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from typing import Iterable\n\nqat_model_path = MODELS_DIR / \"mobilenetv2_qat.keras\"\nqat_model.save(qat_model_path)\nprint(\"Saved QAT model:\", qat_model_path)\n\n\ndef representative_dataset_from_ds(ds, max_batches=10) -> Iterable[list[np.ndarray]]:\n    count = 0\n    for batch, _ in ds:\n        batch_np = batch.numpy().astype(np.float32)\n        for i in range(batch_np.shape[0]):\n            yield [batch_np[i : i + 1]]\n        count += 1\n        if count >= max_batches:\n            break\n\n\ntflite_path = MODELS_DIR / \"mobilenetv2_qat_int8.tflite\"\nconverter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = lambda: representative_dataset_from_ds(train_ds)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.int8\nconverter.inference_output_type = tf.int8\nif DISABLE_PER_CHANNEL:\n    converter._experimental_disable_per_channel = True\n\ntry:\n    tflite_model = converter.convert()\n    tflite_path.write_bytes(tflite_model)\n    print(\"Saved TFLite:\", tflite_path)\nexcept Exception as exc:\n    print(\"INT8 conversion failed:\", exc)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "if tflite_path.exists():\n    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n    interpreter.allocate_tensors()\n    input_detail = interpreter.get_input_details()[0]\n    output_detail = interpreter.get_output_details()[0]\n    print(\"Input dtype:\", input_detail[\"dtype\"], \"quant:\", input_detail.get(\"quantization\"))\n    print(\"Output dtype:\", output_detail[\"dtype\"], \"quant:\", output_detail.get(\"quantization\"))\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}