{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Handwash Full Pipeline (Kaggle) - QAT\nSelf contained notebook for Kaggle with quantization-aware training.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install dependencies\n!pip install -q --no-cache-dir scikit-learn pandas numpy opencv-python-headless matplotlib seaborn tqdm requests tensorflow-model-optimization\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os\nos.environ.setdefault(\"TF_USE_LEGACY_KERAS\", \"1\")\nprint(\"TF_USE_LEGACY_KERAS=\", os.environ.get(\"TF_USE_LEGACY_KERAS\"))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import matplotlib.pyplot as plt\nfrom IPython.display import clear_output, display\nfrom tensorflow import keras\n\n# =========================\n# Standard library\n# =========================\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\n\n# =========================\n# Third-party\n# =========================\nimport numpy as np\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\n\n# =========================\n# TensorFlow / Keras\n# =========================\nimport tensorflow as tf\n\nRANDOM_SEED = 42\nIMG_SIZE = (224, 224)\nNUM_CLASSES = 7\nCLASS_NAMES = ['Unused', 'Step1_PalmToPalm', 'Step2_PalmOverDorsum', 'Step3_InterlacedFingers', 'Step4_BackOfFingers', 'Step5_ThumbRub', 'Step6_Fingertips']\nKAGGLE_URL = 'https://github.com/atiselsts/data/raw/master/kaggle-dataset-6classes.tar'\nKAGGLE_CLASS_MAPPING = {'0': -1, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, 'step1': 1, 'step2': 2, 'step3': 3, 'step4': 4, 'step5': 5, 'step6': 6, 'other': -1}\nTRAIN_RATIO = 0.7\nVAL_RATIO = 0.15\nTEST_RATIO = 0.15\n\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\ntry:\n    tf.random.set_seed(RANDOM_SEED)\nexcept Exception:\n    pass\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import sys\nimport subprocess\n\n\ndef _pip_install(pkg):\n    return subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-cache-dir\", pkg])\n\ntry:\n    import tensorflow_model_optimization as tfmot\nexcept ModuleNotFoundError:\n    for candidate in (\n        \"tensorflow-model-optimization==0.8.0\",\n        \"tensorflow-model-optimization==0.7.5\",\n        \"tensorflow-model-optimization\",\n    ):\n        code = _pip_install(candidate)\n        if code == 0:\n            break\n    import tensorflow_model_optimization as tfmot\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os, sys, json, time, math, random, shutil, subprocess\nfrom pathlib import Path\n\nRUN_NAME = os.environ.get(\"RUN_NAME\", \"handwash_qat_run\")\nKAGGLE_WORKING = Path(\"/kaggle/working\")\nif not KAGGLE_WORKING.exists():\n    KAGGLE_WORKING = Path.cwd()\n\nWORK_DIR = KAGGLE_WORKING / \"handwash_runs\" / RUN_NAME\nDATA_DIR = KAGGLE_WORKING / \"handwash_data\"\n\nRAW_DIR = DATA_DIR / \"raw\"\nPROCESSED_DIR = DATA_DIR / \"processed\"\nMODELS_DIR = WORK_DIR / \"models\"\nCKPT_DIR = WORK_DIR / \"checkpoints\"\nLOGS_DIR = WORK_DIR / \"logs\"\nSAMPLES_DIR = WORK_DIR / \"samples\"\n\nfor p in [WORK_DIR, DATA_DIR, RAW_DIR, PROCESSED_DIR, MODELS_DIR, CKPT_DIR, LOGS_DIR, SAMPLES_DIR]:\n    p.mkdir(parents=True, exist_ok=True)\n\nprint(\"WORK_DIR:\", WORK_DIR)\nprint(\"DATA_DIR:\", DATA_DIR)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Configuration\nAll options are user editable.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# User config (edit these)\nDATASETS = [\"kaggle\"]\n\n# Training config\nBATCH_SIZE = 32\nEPOCHS_FLOAT = 2\nEPOCHS_QAT = 50\nLEARNING_RATE_FLOAT = 1e-4\nLEARNING_RATE_QAT = 1e-5\n\n# Quantization config\nDISABLE_PER_CHANNEL = False\n\n# Data sampling config\nFRAME_STRIDE = 2\nMAX_FRAMES_PER_VIDEO = 8\n\n# Augmentation config\nAUGMENT_TRAIN = True\nAUGMENT_CONFIG = {\n    \"hflip\": True,\n    \"rotation\": 15,\n    \"zoom\": 0.1,\n    \"shear\": 0.1,\n    \"shift\": 0.1,\n    \"brightness\": (0.9, 1.1),\n    \"contrast\": (0.9, 1.1),\n    \"gamma\": (0.9, 1.1),\n    \"shadow\": True,\n}\n\n# Mixed precision\nMIXED_PRECISION = False\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Auto-tune and mixed precision\nif MIXED_PRECISION:\n    from tensorflow.keras import mixed_precision\n    mixed_precision.set_global_policy(\"mixed_float16\")\n    print(\"Mixed precision enabled\")\n\n# Multi-GPU strategy\n_gpus = tf.config.list_physical_devices('GPU')\nif len(_gpus) > 1:\n    STRATEGY = tf.distribute.MirroredStrategy()\nelse:\n    STRATEGY = tf.distribute.get_strategy()\nNUM_REPLICAS = STRATEGY.num_replicas_in_sync\nprint('Using strategy:', STRATEGY, 'replicas:', NUM_REPLICAS)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Download and preprocess\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import requests\nfrom tqdm import tqdm\nimport tarfile\nfrom IPython.display import Video, display\n\nVIDEO_EXTS = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n\nKAGGLE_DIR = RAW_DIR / \"kaggle\"\nKAGGLE_TAR = KAGGLE_DIR / \"kaggle-dataset-6classes.tar\"\nKAGGLE_EXTRACTED = KAGGLE_DIR / \"kaggle-dataset-6classes\"\n\n\ndef download_with_progress(url: str, dest: Path):\n    dest.parent.mkdir(parents=True, exist_ok=True)\n    if dest.exists():\n        print(\"skip\", dest)\n        return\n    with requests.get(url, stream=True, timeout=30) as r:\n        r.raise_for_status()\n        total = int(r.headers.get(\"content-length\", 0))\n        with open(dest, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dest.name) as pbar:\n            for chunk in r.iter_content(chunk_size=1024 * 1024):\n                if not chunk:\n                    continue\n                f.write(chunk)\n                pbar.update(len(chunk))\n\n\ndef extract_tar(tar_path: Path, extract_root: Path):\n    extract_root.mkdir(parents=True, exist_ok=True)\n    with tarfile.open(tar_path, \"r\") as tar:\n        tar.extractall(path=extract_root)\n\n\ndef find_kaggle_input_dataset():\n    root = Path(\"/kaggle/input\")\n    if not root.exists():\n        return None\n    for item in root.iterdir():\n        if not item.is_dir():\n            continue\n        candidate = item / \"kaggle-dataset-6classes\"\n        if candidate.exists():\n            return candidate\n        if all((item / str(i)).exists() for i in range(NUM_CLASSES)):\n            return item\n    return None\n\n\nDATA_ROOT = find_kaggle_input_dataset()\nif DATA_ROOT is not None:\n    print(\"Using Kaggle input dataset:\", DATA_ROOT)\nelse:\n    print(\"Kaggle input dataset not found. Downloading...\")\n    if not KAGGLE_EXTRACTED.exists():\n        download_with_progress(KAGGLE_URL, KAGGLE_TAR)\n        extract_tar(KAGGLE_TAR, KAGGLE_DIR)\n    DATA_ROOT = KAGGLE_EXTRACTED\n\nprint(\"Dataset root:\", DATA_ROOT)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from sklearn.model_selection import train_test_split\n\n\ndef kaggle_class_id_from_folder(name: str):\n    name_lower = name.lower()\n    if name_lower in KAGGLE_CLASS_MAPPING:\n        class_id = KAGGLE_CLASS_MAPPING[name_lower]\n        return class_id if class_id and class_id > 0 else None\n    digits = \"\".join(ch for ch in name_lower if ch.isdigit())\n    if digits:\n        class_id = int(digits)\n        if class_id > 0 and class_id < len(CLASS_NAMES):\n            return class_id\n    return None\n\n\ndef collect_videos(dataset_root: Path) -> pd.DataFrame:\n    rows = []\n    skipped = 0\n    for class_dir in sorted(dataset_root.iterdir()):\n        if not class_dir.is_dir():\n            continue\n        class_id = kaggle_class_id_from_folder(class_dir.name)\n        if class_id is None:\n            skipped += 1\n            continue\n        for vid in class_dir.iterdir():\n            if vid.suffix.lower() not in VIDEO_EXTS:\n                continue\n            rows.append({\"video_path\": str(vid), \"class_id\": class_id})\n    df = pd.DataFrame(rows)\n    if df.empty:\n        raise RuntimeError(f\"No videos found in {dataset_root}\")\n    print(\"Skipped class folders:\", skipped)\n    return df\n\n\nvideos_df = collect_videos(DATA_ROOT)\nprint(\"Total videos:\", len(videos_df))\n\ntrain_df, temp_df = train_test_split(\n    videos_df,\n    test_size=(VAL_RATIO + TEST_RATIO),\n    stratify=videos_df[\"class_id\"],\n    random_state=RANDOM_SEED,\n)\nval_size = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=(1.0 - val_size),\n    stratify=temp_df[\"class_id\"],\n    random_state=RANDOM_SEED,\n)\n\nprint(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from collections import Counter\n\n\ndef print_dataset_info(df, title):\n    counts = Counter(df[\"class_id\"].tolist())\n    total = sum(counts.values())\n    print(f\"{title} total videos: {total}\")\n    for class_id in sorted(counts):\n        name = CLASS_NAMES[class_id] if class_id < len(CLASS_NAMES) else str(class_id)\n        pct = counts[class_id] / total * 100 if total else 0\n        print(f\"  {class_id} {name}: {counts[class_id]} ({pct:.2f}%)\")\n\n\ndef plot_class_distribution(df, title):\n    counts = df[\"class_id\"].value_counts().sort_index()\n    labels = [CLASS_NAMES[i] if i < len(CLASS_NAMES) else str(i) for i in counts.index]\n    plt.figure(figsize=(8, 4))\n    plt.bar(labels, counts.values)\n    plt.title(title)\n    plt.xlabel(\"Class\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=30, ha=\"right\")\n    plt.tight_layout()\n    plt.show()\n\n\nprint_dataset_info(train_df, \"Train\")\nprint_dataset_info(val_df, \"Val\")\nprint_dataset_info(test_df, \"Test\")\n\nplot_class_distribution(train_df, \"Train Class Distribution\")\nplot_class_distribution(val_df, \"Val Class Distribution\")\nplot_class_distribution(test_df, \"Test Class Distribution\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import math\nimport random\nfrom IPython.display import clear_output\n\n\ndef random_shadow(img):\n    h, w = img.shape[:2]\n    x1, y1 = np.random.randint(0, w), 0\n    x2, y2 = np.random.randint(0, w), h\n    mask = np.zeros((h, w), dtype=np.uint8)\n    cv2.fillPoly(mask, [np.array([[x1, y1], [x2, y2], [0, h], [w, h]])], 255)\n    shadow = np.stack([mask] * 3, axis=-1)\n    alpha = np.random.uniform(0.5, 0.9)\n    return np.where(shadow > 0, (img * alpha).astype(np.uint8), img)\n\n\ndef sample_aug_params():\n    hflip_enabled = any(\n        AUGMENT_CONFIG.get(k, False)\n        for k in (\"hflip\", \"mid_flip\", \"horizontal_flip\")\n    )\n    params = {\n        \"hflip\": hflip_enabled and random.random() < 0.5,\n        \"angle\": 0.0,\n        \"zoom\": 1.0,\n        \"shear\": 0.0,\n        \"tx\": 0,\n        \"ty\": 0,\n        \"brightness\": None,\n        \"contrast\": None,\n        \"gamma\": None,\n        \"shadow\": False,\n    }\n\n    if AUGMENT_CONFIG.get(\"rotation\", 0) > 0:\n        params[\"angle\"] = random.uniform(-AUGMENT_CONFIG[\"rotation\"], AUGMENT_CONFIG[\"rotation\"])\n\n    if AUGMENT_CONFIG.get(\"zoom\", 0) > 0:\n        params[\"zoom\"] = random.uniform(1 - AUGMENT_CONFIG[\"zoom\"], 1 + AUGMENT_CONFIG[\"zoom\"])\n\n    if AUGMENT_CONFIG.get(\"shear\", 0) > 0:\n        params[\"shear\"] = random.uniform(-AUGMENT_CONFIG[\"shear\"], AUGMENT_CONFIG[\"shear\"])\n\n    if AUGMENT_CONFIG.get(\"shift\", 0) > 0:\n        params[\"tx\"] = int(random.uniform(-AUGMENT_CONFIG[\"shift\"], AUGMENT_CONFIG[\"shift\"]) * IMG_SIZE[0])\n        params[\"ty\"] = int(random.uniform(-AUGMENT_CONFIG[\"shift\"], AUGMENT_CONFIG[\"shift\"]) * IMG_SIZE[1])\n\n    if AUGMENT_CONFIG.get(\"brightness\"):\n        params[\"brightness\"] = random.uniform(*AUGMENT_CONFIG[\"brightness\"])\n\n    if AUGMENT_CONFIG.get(\"contrast\"):\n        params[\"contrast\"] = random.uniform(*AUGMENT_CONFIG[\"contrast\"])\n\n    if AUGMENT_CONFIG.get(\"gamma\"):\n        params[\"gamma\"] = random.uniform(*AUGMENT_CONFIG[\"gamma\"])\n\n    if AUGMENT_CONFIG.get(\"shadow\") and random.random() < 0.5:\n        params[\"shadow\"] = True\n\n    return params\n\n\ndef apply_aug(img, params):\n    if params.get(\"hflip\"):\n        img = cv2.flip(img, 1)\n\n    angle = params.get(\"angle\", 0.0)\n    if angle:\n        M = cv2.getRotationMatrix2D((IMG_SIZE[0] / 2, IMG_SIZE[1] / 2), angle, 1.0)\n        img = cv2.warpAffine(img, M, IMG_SIZE, borderMode=cv2.BORDER_REFLECT)\n\n    zoom = params.get(\"zoom\", 1.0)\n    if zoom != 1.0:\n        h, w = IMG_SIZE\n        img_resized = cv2.resize(img, (int(w * zoom), int(h * zoom)))\n        if zoom > 1:\n            startx = (img_resized.shape[1] - w) // 2\n            starty = (img_resized.shape[0] - h) // 2\n            img = img_resized[starty : starty + h, startx : startx + w]\n        else:\n            pad_x = (w - img_resized.shape[1]) // 2\n            pad_y = (h - img_resized.shape[0]) // 2\n            img = cv2.copyMakeBorder(\n                img_resized,\n                pad_y,\n                h - img_resized.shape[0] - pad_y,\n                pad_x,\n                w - img_resized.shape[1] - pad_x,\n                cv2.BORDER_REFLECT,\n            )\n\n    tx, ty = params.get(\"tx\", 0), params.get(\"ty\", 0)\n    if tx or ty:\n        M = np.float32([[1, 0, tx], [0, 1, ty]])\n        img = cv2.warpAffine(img, M, IMG_SIZE, borderMode=cv2.BORDER_REFLECT)\n\n    shear = params.get(\"shear\", 0.0)\n    if shear:\n        M = np.float32([[1, shear, 0], [0, 1, 0]])\n        img = cv2.warpAffine(img, M, IMG_SIZE, borderMode=cv2.BORDER_REFLECT)\n\n    brightness = params.get(\"brightness\")\n    if brightness is not None:\n        img = np.clip(img.astype(np.float32) * brightness, 0, 255).astype(np.uint8)\n\n    contrast = params.get(\"contrast\")\n    if contrast is not None:\n        img = np.clip(128 + contrast * (img.astype(np.float32) - 128), 0, 255).astype(np.uint8)\n\n    gamma = params.get(\"gamma\")\n    if gamma is not None:\n        img = np.clip(((img / 255.0) ** gamma) * 255.0, 0, 255).astype(np.uint8)\n\n    if params.get(\"shadow\"):\n        img = random_shadow(img)\n\n    return img\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_v2_preprocess\n\n\ndef _load_random_frame_py(video_path_bytes, label, augment):\n    if hasattr(video_path_bytes, \"numpy\"):\n        video_path_bytes = video_path_bytes.numpy()\n    if hasattr(label, \"numpy\"):\n        label = int(label.numpy())\n    if hasattr(augment, \"numpy\"):\n        augment = bool(augment.numpy())\n    video_path = video_path_bytes.decode(\"utf-8\")\n    cap = cv2.VideoCapture(video_path)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if frame_count <= 0:\n        cap.release()\n        raise RuntimeError(f\"No frames in {video_path}\")\n    target_idx = np.random.randint(0, frame_count)\n    cap.set(cv2.CAP_PROP_POS_FRAMES, target_idx)\n    ok, frame = cap.read()\n    cap.release()\n    if not ok:\n        raise RuntimeError(f\"Failed reading frame from {video_path}\")\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame = cv2.resize(frame, IMG_SIZE)\n    if augment:\n        params = sample_aug_params()\n        frame = apply_aug(frame, params)\n    frame = frame.astype(np.float32)\n    frame = mobilenet_v2_preprocess(frame)\n    return frame, np.int32(label)\n\n\ndef _load_random_frame(video_path, label, augment):\n    frame, label = tf.py_function(\n        _load_random_frame_py,\n        inp=[video_path, label, augment],\n        Tout=[tf.float32, tf.int32],\n    )\n    frame.set_shape((*IMG_SIZE, 3))\n    label.set_shape(())\n    return frame, label\n\n\ndef make_dataset(df, batch_size=32, shuffle=True, augment=False):\n    ds = tf.data.Dataset.from_tensor_slices((df[\"video_path\"].values, df[\"class_id\"].values))\n    if shuffle:\n        ds = ds.shuffle(len(df), seed=RANDOM_SEED, reshuffle_each_iteration=True)\n    aug_tensor = tf.constant(augment)\n    ds = ds.map(lambda p, y: _load_random_frame(p, y, aug_tensor), num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds\n\n\ntrain_ds = make_dataset(train_df, batch_size=BATCH_SIZE, shuffle=True, augment=AUGMENT_TRAIN)\nval_ds = make_dataset(val_df, batch_size=BATCH_SIZE, shuffle=False, augment=False)\ntest_ds = make_dataset(test_df, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Build QAT model\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from tensorflow.keras import layers, models\n\n\ndef build_float_model():\n    base = tf.keras.applications.MobileNetV2(\n        input_shape=(*IMG_SIZE, 3),\n        include_top=False,\n        weights=\"imagenet\",\n    )\n    base.trainable = True\n    inputs = layers.Input(shape=(*IMG_SIZE, 3))\n    x = base(inputs, training=True)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n    model = models.Model(inputs, outputs)\n    return model\n\n\nwith STRATEGY.scope():\n    float_model = build_float_model()\n    float_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_FLOAT),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[\"accuracy\"],\n    )\n\nfloat_model.summary()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Warmup (float) and QAT fine-tune\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "history_float = float_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS_FLOAT,\n)\n\nwith STRATEGY.scope():\n    quantize_model = tfmot.quantization.keras.quantize_model\n    qat_model = quantize_model(float_model)\n    qat_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_QAT),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[\"accuracy\"],\n    )\n\nqat_model.summary()\n\nhistory_qat = qat_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS_QAT,\n)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Evaluate\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "qat_eval = qat_model.evaluate(test_ds, verbose=1)\nprint(\"QAT test metrics:\", dict(zip(qat_model.metrics_names, qat_eval)))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Sample inference on test videos\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\n\n\ndef iter_video_frames(video_path: str, stride: int = 2, max_frames=24):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    idx = 0\n    while True:\n        ok, frame = cap.read()\n        if not ok:\n            break\n        if idx % stride == 0:\n            frames.append(frame)\n            if max_frames is not None and len(frames) >= max_frames:\n                break\n        idx += 1\n    cap.release()\n    return frames\n\n\ndef run_sample_inference(model, df, num_videos=3, frame_stride=2, max_frames=12):\n    sample_df = df.sample(min(num_videos, len(df)), random_state=RANDOM_SEED)\n    for row in sample_df.itertuples(index=False):\n        frames = iter_video_frames(row.video_path, stride=frame_stride, max_frames=max_frames)\n        if not frames:\n            continue\n        inputs = []\n        for frame in frames:\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_rgb = cv2.resize(frame_rgb, IMG_SIZE)\n            frame_rgb = frame_rgb.astype(np.float32)\n            frame_rgb = tf.keras.applications.mobilenet_v2.preprocess_input(frame_rgb)\n            inputs.append(frame_rgb)\n        inputs = np.stack(inputs)\n        preds = model.predict(inputs, verbose=0)\n        pred_ids = np.argmax(preds, axis=1)\n        majority = int(np.bincount(pred_ids).argmax())\n\n        label_name = CLASS_NAMES[row.class_id]\n        pred_name = CLASS_NAMES[majority]\n        print(f\"Video: {Path(row.video_path).name} | GT: {row.class_id} {label_name} | Pred: {majority} {pred_name}\")\n\n        frame0 = frames[0].copy()\n        cv2.putText(frame0, f\"GT: {label_name}\", (10, 24), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n        cv2.putText(frame0, f\"Pred: {pred_name}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n        out_path = SAMPLES_DIR / f\"sample_{Path(row.video_path).stem}.jpg\"\n        cv2.imwrite(str(out_path), frame0)\n\n        plt.figure(figsize=(4, 3))\n        plt.imshow(cv2.cvtColor(frame0, cv2.COLOR_BGR2RGB))\n        plt.title(Path(row.video_path).name)\n        plt.axis(\"off\")\n        plt.show()\n\n\nrun_sample_inference(qat_model, test_df)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Export INT8 TFLite\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from typing import Iterable\n\nqat_model_path = MODELS_DIR / \"mobilenetv2_qat.keras\"\nqat_model.save(qat_model_path)\nprint(\"Saved QAT model:\", qat_model_path)\n\n\ndef representative_dataset_from_ds(ds, max_batches=10) -> Iterable[list[np.ndarray]]:\n    count = 0\n    for batch, _ in ds:\n        batch_np = batch.numpy().astype(np.float32)\n        for i in range(batch_np.shape[0]):\n            yield [batch_np[i : i + 1]]\n        count += 1\n        if count >= max_batches:\n            break\n\n\ntflite_path = MODELS_DIR / \"mobilenetv2_qat_int8.tflite\"\nconverter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = lambda: representative_dataset_from_ds(train_ds)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.int8\nconverter.inference_output_type = tf.int8\nif DISABLE_PER_CHANNEL:\n    converter._experimental_disable_per_channel = True\n\ntry:\n    tflite_model = converter.convert()\n    tflite_path.write_bytes(tflite_model)\n    print(\"Saved TFLite:\", tflite_path)\nexcept Exception as exc:\n    print(\"INT8 conversion failed:\", exc)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "if tflite_path.exists():\n    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n    interpreter.allocate_tensors()\n    input_detail = interpreter.get_input_details()[0]\n    output_detail = interpreter.get_output_details()[0]\n    print(\"Input dtype:\", input_detail[\"dtype\"], \"quant:\", input_detail.get(\"quantization\"))\n    print(\"Output dtype:\", output_detail[\"dtype\"], \"quant:\", output_detail.get(\"quantization\"))\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}