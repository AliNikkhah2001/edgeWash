{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwash Full Pipeline (Colab)\n",
    "Self contained notebook for Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q --no-cache-dir scikit-learn pandas numpy opencv-python-headless matplotlib seaborn tqdm requests gdown zenodo-get ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, sys, json, time, math, random, shutil, subprocess\n",
    "from pathlib import Path\n",
    "from google.colab import drive, output\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "RUN_NAME = os.environ.get(\"RUN_NAME\", \"handwash_run\")\n",
    "USE_DRIVE = True\n",
    "\n",
    "if USE_DRIVE:\n",
    "    WORK_DIR = Path(\"/content/drive/MyDrive/handwash_runs\") / RUN_NAME\n",
    "    DATA_DIR = Path(\"/content/drive/MyDrive/handwash_data\")\n",
    "else:\n",
    "    WORK_DIR = Path(\"/content/handwash_runs\") / RUN_NAME\n",
    "    DATA_DIR = Path(\"/content/handwash_data\")\n",
    "\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "MODELS_DIR = WORK_DIR / \"models\"\n",
    "CKPT_DIR = WORK_DIR / \"checkpoints\"\n",
    "LOGS_DIR = Path(\"/content/edgeWash/logs\")\n",
    "\n",
    "for p in [WORK_DIR, DATA_DIR, RAW_DIR, PROCESSED_DIR, MODELS_DIR, CKPT_DIR, LOGS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "All options are user editable.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# User config (edit these)\n",
    "DATASETS = [\"kaggle\", \"pskus\", \"metc\", \"synthetic_blender_rozakar\"]\n",
    "MODELS = [\"mobilenetv2\", \"resnet50\", \"efficientnetb0\", \"lstm\", \"gru\", \"3d_cnn\"]\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "NUM_CLASSES = 7\n",
    "CLASS_NAMES = [\n",
    "    \"Other\",\n",
    "    \"Step1_PalmToPalm\",\n",
    "    \"Step2_PalmOverDorsum\",\n",
    "    \"Step3_InterlacedFingers\",\n",
    "    \"Step4_BackOfFingers\",\n",
    "    \"Step5_ThumbRub\",\n",
    "    \"Step6_Fingertips\",\n",
    "]\n",
    "\n",
    "FRAME_SKIP = 2\n",
    "SEQUENCE_LENGTH = 16\n",
    "SEQUENCE_STRIDE = 1\n",
    "MAX_SEQUENCES_PER_VIDEO = 200\n",
    "\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "BATCH_MOBILENET = 128\n",
    "BATCH_SEQUENCE = 64\n",
    "AUTO_TUNE_BATCH = True\n",
    "MIXED_PRECISION = True\n",
    "TB_PORT = 6008\n",
    "\n",
    "# Augmentation\n",
    "USE_OFFLINE_AUGMENT = True  # generate augmented samples on disk\n",
    "USE_ON_THE_FLY_AUGMENT = False  # apply aug during loading\n",
    "AUGMENT_MULTIPLIER = 4  # how many total samples per original (1 = none)\n",
    "AUGMENT_MAX_PER_SAMPLE = 3  # cap for offline augment per original\n",
    "CONSISTENT_VIDEO_AUG = True  # keep the same aug per video\n",
    "AUGMENT_CONFIG = {\n",
    "    \"rotation\": 15,\n",
    "    \"zoom\": 0.15,\n",
    "    \"shift\": 0.1,\n",
    "    \"shear\": 0.1,\n",
    "    \"brightness\": (0.8, 1.2),\n",
    "    \"contrast\": (0.8, 1.2),\n",
    "    \"gamma\": (0.8, 1.2),\n",
    "    \"hflip\": True,\n",
    "    \"mid_flip\": True,\n",
    "    \"shadow\": True,\n",
    "    \"reverse_sequence\": True,\n",
    "}\n",
    "\n",
    "# Cleanup\n",
    "SKIP_DOWNLOAD_IF_PRESENT = True\n",
    "CLEANUP_RAW = True\n",
    "CLEANUP_TRAIN = True\n",
    "KEEP_VAL_TEST = True\n",
    "\n",
    "# Dataset sources\n",
    "KAGGLE_URL = \"https://github.com/atiselsts/data/raw/master/kaggle-dataset-6classes.tar\"\n",
    "PSKUS_ZENODO = \"4537209\"\n",
    "METC_ZENODO = \"4537342\"\n",
    "SYNTHETIC_LINKS = [\n",
    "    \"https://drive.google.com/uc?id=1EW3JQvElcuXzawxEMRkA8YXwK_Ipiv-p&export=download\",\n",
    "    \"https://drive.google.com/uc?id=163TsrDe4q5KTQGCv90JRYFkCs7AGxFip&export=download\",\n",
    "    \"https://drive.google.com/uc?id=1GxyTYfSodumH78NbjWdmbjm8JP8AOkAY&export=download\",\n",
    "    \"https://drive.google.com/uc?id=1IoRsgBBr8qoC3HO-vEr6E7K4UZ6ku6-1&export=download\",\n",
    "    \"https://drive.google.com/uc?id=1svCYnwDazy5FN1DYSgqbGscvDKL_YnID&export=download\",\n",
    "]\n",
    "\n",
    "print(\"Datasets:\", DATASETS)\n",
    "print(\"Models:\", MODELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Auto-tune and mixed precision\n",
    "if MIXED_PRECISION:\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    print(\"Mixed precision enabled\")\n",
    "\n",
    "\n",
    "def get_gpu_mem_mb():\n",
    "    try:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=memory.total\",\n",
    "            \"--format=csv,noheader,nounits\",\n",
    "        ])\n",
    "        return int(out.decode().strip().splitlines()[0])\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "if AUTO_TUNE_BATCH:\n",
    "    mem_mb = get_gpu_mem_mb()\n",
    "    if mem_mb > 0:\n",
    "        BATCH_MOBILENET = max(64, min(256, int(mem_mb / 120)))\n",
    "        BATCH_SEQUENCE = max(32, min(128, int(mem_mb / 240)))\n",
    "        print(\"Auto batch sizes:\", BATCH_MOBILENET, BATCH_SEQUENCE)\n",
    "    else:\n",
    "        print(\"GPU memory not detected; using configured batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Start TensorBoard\n",
    "import subprocess\n",
    "\n",
    "tb_proc = subprocess.Popen([\n",
    "    \"tensorboard\",\n",
    "    \"--logdir\", str(LOGS_DIR),\n",
    "    \"--host\", \"0.0.0.0\",\n",
    "    \"--port\", str(TB_PORT),\n",
    "    \"--load_fast=false\",\n",
    "], stdout=open(LOGS_DIR / \"tensorboard.out\", \"w\"), stderr=subprocess.STDOUT)\n",
    "\n",
    "output.serve_kernel_port_as_window(TB_PORT)\n",
    "print(\"TensorBoard PID:\", tb_proc.pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/edgeWash/logs --host 0.0.0.0 --port 6008\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import tarfile, zipfile\n",
    "from IPython.display import Video, display\n",
    "\n",
    "LABEL_TOKENS = {\n",
    "    \"step1\": 1,\n",
    "    \"step2\": 2,\n",
    "    \"step3\": 3,\n",
    "    \"step4\": 4,\n",
    "    \"step5\": 5,\n",
    "    \"step6\": 6,\n",
    "    \"other\": 0,\n",
    "}\n",
    "\n",
    "VIDEO_EXTS = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n",
    "IMAGE_EXTS = (\".jpg\", \".jpeg\", \".png\")\n",
    "\n",
    "\n",
    "def download_with_progress(url: str, dest: Path):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dest.exists():\n",
    "        print(\"skip\", dest)\n",
    "        return\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"content-length\", 0))\n",
    "        with open(dest, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dest.name) as pbar:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "\n",
    "\n",
    "def extract_tar(tar_path: Path, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with tarfile.open(tar_path) as tfp:\n",
    "        tfp.extractall(out_dir)\n",
    "    tar_path.unlink(missing_ok=True)\n",
    "\n",
    "\n",
    "def extract_zip(zip_path: Path, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        zf.extractall(out_dir)\n",
    "    zip_path.unlink(missing_ok=True)\n",
    "\n",
    "\n",
    "def download_kaggle():\n",
    "    out_dir = RAW_DIR / \"kaggle\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tar_path = out_dir / \"kaggle-dataset-6classes.tar\"\n",
    "    download_with_progress(KAGGLE_URL, tar_path)\n",
    "    print(\"Extracting kaggle...\")\n",
    "    extract_tar(tar_path, out_dir)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def download_zenodo(zenodo_id: str, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cmd = [\"zenodo_get\", \"-r\", zenodo_id, \"-o\", str(out_dir)]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def download_pskus():\n",
    "    return download_zenodo(PSKUS_ZENODO, RAW_DIR / \"pskus\")\n",
    "\n",
    "\n",
    "def download_metc():\n",
    "    return download_zenodo(METC_ZENODO, RAW_DIR / \"metc\")\n",
    "\n",
    "\n",
    "def download_synthetic():\n",
    "    out_dir = RAW_DIR / \"synthetic_blender_rozakar\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, link in enumerate(SYNTHETIC_LINKS, 1):\n",
    "        out_zip = out_dir / f\"synth_{i}.zip\"\n",
    "        if not out_zip.exists():\n",
    "            subprocess.check_call([\"gdown\", \"-q\", link, \"-O\", str(out_zip)])\n",
    "        extract_zip(out_zip, out_dir)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def ensure_dataset(name: str):\n",
    "    raw_dir = RAW_DIR / name\n",
    "    if SKIP_DOWNLOAD_IF_PRESENT and raw_dir.exists():\n",
    "        print(\"skip download, exists\", raw_dir)\n",
    "        return raw_dir\n",
    "    if name == \"kaggle\":\n",
    "        return download_kaggle()\n",
    "    if name == \"pskus\":\n",
    "        return download_pskus()\n",
    "    if name == \"metc\":\n",
    "        return download_metc()\n",
    "    if name == \"synthetic_blender_rozakar\":\n",
    "        return download_synthetic()\n",
    "    raise ValueError(\"Unknown dataset \" + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "from typing import List, Dict, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Video, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def infer_label_from_path(p: Path) -> int:\n",
    "    parts = [part for part in Path(p).parts]\n",
    "    for part in reversed(parts):\n",
    "        if part.isdigit():\n",
    "            class_id = int(part)\n",
    "            if 0 <= class_id < len(CLASS_NAMES):\n",
    "                return class_id\n",
    "    text = str(p).lower()\n",
    "    for token, idx in LABEL_TOKENS.items():\n",
    "        if token in text:\n",
    "            return idx\n",
    "    return 0\n",
    "\n",
    "\n",
    "def _majority_vote(labels, total_movements):\n",
    "    counts = [0] * total_movements\n",
    "    for el in labels:\n",
    "        counts[int(el)] += 1\n",
    "    best = 0\n",
    "    for i in range(1, total_movements):\n",
    "        if counts[best] < counts[i]:\n",
    "            best = i\n",
    "    majority = (len(labels) + 2) // 2\n",
    "    if counts[best] < majority:\n",
    "        return -1\n",
    "    return best\n",
    "\n",
    "\n",
    "def _discount_reaction_indeterminacy(labels, reaction_frames):\n",
    "    new_labels = [u for u in labels]\n",
    "    n = len(labels) - 1\n",
    "    for i in range(n):\n",
    "        if i == 0 or labels[i] != labels[i + 1] or i == n - 1:\n",
    "            start = max(0, i - reaction_frames)\n",
    "            end = i\n",
    "            for j in range(start, end):\n",
    "                new_labels[j] = -1\n",
    "            start = i\n",
    "            end = min(n + 1, i + reaction_frames)\n",
    "            for j in range(start, end):\n",
    "                new_labels[j] = -1\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def _select_frames_to_save(is_washing, codes, movement0_prop=1.0):\n",
    "    old_code = -1\n",
    "    old_saved = False\n",
    "    num_snippets = 0\n",
    "    mapping = {}\n",
    "    current_snippet = {}\n",
    "    for i in range(len(is_washing)):\n",
    "        new_code = codes[i]\n",
    "        new_saved = (is_washing[i] == 2 and new_code != -1)\n",
    "        if new_saved != old_saved:\n",
    "            if new_saved:\n",
    "                num_snippets += 1\n",
    "                current_snippet = {}\n",
    "            else:\n",
    "                if old_code != 0 or np.random.rand() < movement0_prop:\n",
    "                    for key in current_snippet:\n",
    "                        mapping[key] = current_snippet[key]\n",
    "        if new_saved:\n",
    "            current_snippet_frame = len(current_snippet)\n",
    "            current_snippet[i] = (current_snippet_frame, num_snippets, new_code)\n",
    "        old_saved = new_saved\n",
    "        old_code = new_code\n",
    "    if old_saved:\n",
    "        if old_code != 0 or np.random.rand() < movement0_prop:\n",
    "            for key in current_snippet:\n",
    "                mapping[key] = current_snippet[key]\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def _find_annotations_dir(video_path: Path) -> Path | None:\n",
    "    for parent in video_path.parents:\n",
    "        ann_dir = parent / \"Annotations\"\n",
    "        if ann_dir.exists():\n",
    "            return ann_dir\n",
    "    return None\n",
    "\n",
    "\n",
    "def _load_frame_annotations(video_path: Path, annotator_prefix: str, total_annotators: int):\n",
    "    ann_dir = _find_annotations_dir(video_path)\n",
    "    if not ann_dir:\n",
    "        return [], 0\n",
    "    annotations = []\n",
    "    for a in range(1, total_annotators + 1):\n",
    "        annotator_dir = ann_dir / f\"{annotator_prefix}{a}\"\n",
    "        json_path = annotator_dir / f\"{video_path.stem}.json\"\n",
    "        if not json_path.exists():\n",
    "            continue\n",
    "        try:\n",
    "            with open(json_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            a_annotations = [(data['labels'][i]['is_washing'], data['labels'][i]['code']) for i in range(len(data['labels']))]\n",
    "            annotations.append(a_annotations)\n",
    "        except Exception as exc:\n",
    "            print(\"Failed to load\", json_path, exc)\n",
    "    return annotations, len(annotations)\n",
    "\n",
    "\n",
    "def _frame_labels_from_annotations(annotations, total_movements, reaction_frames):\n",
    "    num_annotators = len(annotations)\n",
    "    if num_annotators == 0:\n",
    "        return [], []\n",
    "    num_frames = len(annotations[0])\n",
    "    is_washing, codes = [], []\n",
    "    for frame_num in range(num_frames):\n",
    "        frame_annotations = [annotations[a][frame_num] for a in range(num_annotators)]\n",
    "        frame_is_washing_any = any(frame_annotations[a][0] for a in range(num_annotators))\n",
    "        frame_is_washing_all = all(frame_annotations[a][0] for a in range(num_annotators))\n",
    "        frame_codes = [frame_annotations[a][1] for a in range(num_annotators)]\n",
    "        frame_codes = [0 if code == 7 else code for code in frame_codes]\n",
    "        if frame_is_washing_all:\n",
    "            frame_is_washing = 2\n",
    "        elif frame_is_washing_any:\n",
    "            frame_is_washing = 1\n",
    "        else:\n",
    "            frame_is_washing = 0\n",
    "        is_washing.append(frame_is_washing)\n",
    "        if frame_is_washing:\n",
    "            codes.append(_majority_vote(frame_codes, total_movements))\n",
    "        else:\n",
    "            codes.append(-1)\n",
    "    is_washing = _discount_reaction_indeterminacy(is_washing, reaction_frames)\n",
    "    codes = _discount_reaction_indeterminacy(codes, reaction_frames)\n",
    "    return is_washing, codes\n",
    "\n",
    "\n",
    "def _load_pskus_split(pskus_dir: Path):\n",
    "    csv_path = pskus_dir / \"statistics-with-locations.csv\"\n",
    "    if not csv_path.exists():\n",
    "        print(\"PSKUS split CSV not found; will use random split later\")\n",
    "        return set(), set()\n",
    "    testfiles, trainvalfiles = set(), set()\n",
    "    with open(csv_path, \"r\") as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        for row in reader:\n",
    "            if row and row[0] == \"filename\":\n",
    "                continue\n",
    "            if not row:\n",
    "                continue\n",
    "            filename = row[0]\n",
    "            location = row[1] if len(row) > 1 else \"\"\n",
    "            if location == \"Reanim\u0101cija\":\n",
    "                testfiles.add(filename)\n",
    "            elif location != \"unknown\":\n",
    "                trainvalfiles.add(filename)\n",
    "    return testfiles, trainvalfiles\n",
    "\n",
    "\n",
    "def extract_frames_from_video(video_path: Path, out_dir: Path, frame_skip: int) -> List[Dict]:\n",
    "    rows = []\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        return rows\n",
    "    base = video_path.stem\n",
    "    label = infer_label_from_path(video_path)\n",
    "    idx = 0\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_idx % frame_skip == 0:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, IMG_SIZE)\n",
    "            out_path = out_dir / f\"{base}_{idx:06d}.jpg\"\n",
    "            cv2.imwrite(str(out_path), frame[:, :, ::-1])\n",
    "            rows.append({\"frame_path\": str(out_path), \"class_id\": label, \"video_id\": base, \"frame_idx\": idx})\n",
    "            idx += 1\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    return rows\n",
    "\n",
    "\n",
    "def preprocess_images(image_paths: List[Path], out_dir: Path) -> List[Dict]:\n",
    "    rows = []\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for img_path in tqdm(image_paths, desc=\"images\"):\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, IMG_SIZE)\n",
    "        label = infer_label_from_path(img_path)\n",
    "        out_path = out_dir / f\"{img_path.stem}.jpg\"\n",
    "        cv2.imwrite(str(out_path), img[:, :, ::-1])\n",
    "        rows.append({\"frame_path\": str(out_path), \"class_id\": label, \"video_id\": img_path.parent.name, \"frame_idx\": 0})\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _split_train_val_by_video(df, train_ratio=0.7, val_ratio=0.15):\n",
    "    unique_videos = df[\"video_id\"].unique()\n",
    "    video_to_class = df.groupby(\"video_id\")[\"class_id\"].first()\n",
    "    val_size = val_ratio / (train_ratio + val_ratio)\n",
    "    train_videos, val_videos = train_test_split(\n",
    "        unique_videos,\n",
    "        test_size=val_size,\n",
    "        random_state=42,\n",
    "        stratify=video_to_class[unique_videos],\n",
    "    )\n",
    "    train_df = df[df[\"video_id\"].isin(train_videos)].reset_index(drop=True)\n",
    "    val_df = df[df[\"video_id\"].isin(val_videos)].reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def split_and_save(df: pd.DataFrame, out_dir: Path) -> Dict[str, Path]:\n",
    "    if \"split\" in df.columns and df[\"split\"].notna().any():\n",
    "        test_df = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "        trainval_df = df[df[\"split\"] != \"test\"].reset_index(drop=True)\n",
    "        if not trainval_df.empty:\n",
    "            train_df, val_df = _split_train_val_by_video(trainval_df)\n",
    "        else:\n",
    "            train_df, val_df = df, df.iloc[0:0].copy()\n",
    "    else:\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        n = len(df)\n",
    "        train_end = int(0.7 * n)\n",
    "        val_end = int(0.85 * n)\n",
    "        train_df, val_df, test_df = df.iloc[:train_end], df.iloc[train_end:val_end], df.iloc[val_end:]\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_csv = out_dir / \"train.csv\"\n",
    "    val_csv = out_dir / \"val.csv\"\n",
    "    test_csv = out_dir / \"test.csv\"\n",
    "    train_df.to_csv(train_csv, index=False)\n",
    "    val_df.to_csv(val_csv, index=False)\n",
    "    test_df.to_csv(test_csv, index=False)\n",
    "    return {\"train\": train_csv, \"val\": val_csv, \"test\": test_csv}\n",
    "\n",
    "\n",
    "def preprocess_pskus_dataset(pskus_dir: Path, out_dir: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    testfiles, trainvalfiles = _load_pskus_split(pskus_dir)\n",
    "    has_split = bool(testfiles or trainvalfiles)\n",
    "    movement0_prop = 0.2\n",
    "    total_annotators = 8\n",
    "    total_movements = 8\n",
    "    fps = 30\n",
    "    reaction_frames = fps // 2\n",
    "\n",
    "    for video_path in pskus_dir.rglob(\"*.mp4\"):\n",
    "        filename = video_path.name\n",
    "        if has_split:\n",
    "            if filename in testfiles:\n",
    "                split = \"test\"\n",
    "            elif filename in trainvalfiles:\n",
    "                split = \"trainval\"\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            split = None\n",
    "\n",
    "        annotations, num_annotators = _load_frame_annotations(video_path, \"Annotator\", total_annotators)\n",
    "        if num_annotators <= 1:\n",
    "            continue\n",
    "        is_washing, codes = _frame_labels_from_annotations(annotations, total_movements, reaction_frames)\n",
    "        mapping = _select_frames_to_save(is_washing, codes, movement0_prop)\n",
    "        if not mapping:\n",
    "            continue\n",
    "        frames_dir = out_dir / \"pskus\" / (split or \"trainval\")\n",
    "        vidcap = cv2.VideoCapture(str(video_path))\n",
    "        is_success, image = vidcap.read()\n",
    "        frame_number = 0\n",
    "        while is_success:\n",
    "            if frame_number in mapping:\n",
    "                new_frame_num, snippet_num, code = mapping[frame_number]\n",
    "                out_sub = frames_dir / str(code)\n",
    "                out_sub.mkdir(parents=True, exist_ok=True)\n",
    "                filename_out = f\"frame_{new_frame_num}_snippet_{snippet_num}_{video_path.stem}.jpg\"\n",
    "                save_path = out_sub / filename_out\n",
    "                cv2.imwrite(str(save_path), image)\n",
    "                row = {\n",
    "                    \"frame_path\": str(save_path),\n",
    "                    \"class_id\": int(code),\n",
    "                    \"video_id\": video_path.stem,\n",
    "                    \"frame_idx\": new_frame_num,\n",
    "                }\n",
    "                if split:\n",
    "                    row[\"split\"] = split\n",
    "                rows.append(row)\n",
    "            is_success, image = vidcap.read()\n",
    "            frame_number += 1\n",
    "        vidcap.release()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def preprocess_metc_dataset(metc_dir: Path, out_dir: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    total_annotators = 1\n",
    "    total_movements = 7\n",
    "    fps = 16\n",
    "    reaction_frames = fps // 2\n",
    "    test_proportion = 0.25\n",
    "    for video_path in metc_dir.rglob(\"*.mp4\"):\n",
    "        split = \"test\" if np.random.rand() < test_proportion else \"trainval\"\n",
    "        annotations, num_annotators = _load_frame_annotations(video_path, \"Annotator_\", total_annotators)\n",
    "        if num_annotators == 0:\n",
    "            continue\n",
    "        is_washing, codes = _frame_labels_from_annotations(annotations, total_movements, reaction_frames)\n",
    "        mapping = _select_frames_to_save(is_washing, codes, movement0_prop=1.0)\n",
    "        if not mapping:\n",
    "            continue\n",
    "        frames_dir = out_dir / \"metc\" / split\n",
    "        vidcap = cv2.VideoCapture(str(video_path))\n",
    "        is_success, image = vidcap.read()\n",
    "        frame_number = 0\n",
    "        while is_success:\n",
    "            if frame_number in mapping:\n",
    "                new_frame_num, snippet_num, code = mapping[frame_number]\n",
    "                out_sub = frames_dir / str(code)\n",
    "                out_sub.mkdir(parents=True, exist_ok=True)\n",
    "                filename_out = f\"frame_{new_frame_num}_snippet_{snippet_num}_{video_path.stem}.jpg\"\n",
    "                save_path = out_sub / filename_out\n",
    "                cv2.imwrite(str(save_path), image)\n",
    "                rows.append({\n",
    "                    \"frame_path\": str(save_path),\n",
    "                    \"class_id\": int(code),\n",
    "                    \"video_id\": video_path.stem,\n",
    "                    \"frame_idx\": new_frame_num,\n",
    "                    \"split\": split,\n",
    "                })\n",
    "            is_success, image = vidcap.read()\n",
    "            frame_number += 1\n",
    "        vidcap.release()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def preprocess_dataset(name: str) -> Path:\n",
    "    raw_dir = RAW_DIR / name\n",
    "    out_dir = PROCESSED_DIR / name\n",
    "    frames_dir = out_dir / \"frames\"\n",
    "    frames_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if name == \"pskus\":\n",
    "        df = preprocess_pskus_dataset(raw_dir, out_dir)\n",
    "    elif name == \"metc\":\n",
    "        df = preprocess_metc_dataset(raw_dir, out_dir)\n",
    "    else:\n",
    "        video_files = [p for p in raw_dir.rglob(\"*\") if p.suffix.lower() in VIDEO_EXTS]\n",
    "        image_files = [p for p in raw_dir.rglob(\"*\") if p.suffix.lower() in IMAGE_EXTS]\n",
    "        rows = []\n",
    "        if video_files:\n",
    "            for vp in tqdm(video_files, desc=\"videos\"):\n",
    "                rows.extend(extract_frames_from_video(vp, frames_dir, FRAME_SKIP))\n",
    "        elif image_files:\n",
    "            rows.extend(preprocess_images(image_files, frames_dir))\n",
    "        else:\n",
    "            raise RuntimeError(\"No video or image files found in \" + str(raw_dir))\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No frames extracted for \" + name)\n",
    "    split_and_save(df, out_dir)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def show_random_video(raw_dir: Path):\n",
    "    videos = [p for p in raw_dir.rglob(\"*\") if p.suffix.lower() in VIDEO_EXTS]\n",
    "    if not videos:\n",
    "        print(\"No videos found\")\n",
    "        return\n",
    "    print(\"Video sample:\", videos[0])\n",
    "    display(Video(str(videos[0]), embed=True))\n",
    "\n",
    "\n",
    "def show_random_samples(df: pd.DataFrame, title: str, n: int = 12):\n",
    "    import matplotlib.pyplot as plt\n",
    "    sample = df.sample(n, replace=True)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, row in enumerate(sample.itertuples(), 1):\n",
    "        img = cv2.imread(row.frame_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.subplot(3, 4, i)\n",
    "        plt.imshow(img)\n",
    "        label = CLASS_NAMES[int(row.class_id)] if int(row.class_id) < len(CLASS_NAMES) else str(row.class_id)\n",
    "        plt.title(label, fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_class_distribution(df: pd.DataFrame, label: str) -> None:\n",
    "    col = \"class_name\" if \"class_name\" in df.columns else \"class_id\"\n",
    "    counts = df[col].value_counts()\n",
    "    print(f\"{label} class distribution:\")\n",
    "    print(counts.to_string())\n",
    "    if len(counts) == 1:\n",
    "        print(\"WARNING: single-class dataset; check labeling.\")\n",
    "    if \"Other\" in counts.index:\n",
    "        if counts[\"Other\"] / len(df) > 0.95:\n",
    "            print(\"WARNING: 'Other' exceeds 95% of samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def random_shadow(img):\n",
    "    h, w = img.shape[:2]\n",
    "    x1, y1 = np.random.randint(0, w), 0\n",
    "    x2, y2 = np.random.randint(0, w), h\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [np.array([[x1, y1], [x2, y2], [0, h], [w, h]])], 255)\n",
    "    shadow = np.stack([mask] * 3, axis=-1)\n",
    "    alpha = np.random.uniform(0.5, 0.9)\n",
    "    return np.where(shadow > 0, (img * alpha).astype(np.uint8), img)\n",
    "\n",
    "\n",
    "def sample_aug_params():\n",
    "    hflip_enabled = any(\n",
    "        AUGMENT_CONFIG.get(k, False)\n",
    "        for k in (\"hflip\", \"mid_flip\", \"horizontal_flip\")\n",
    "    )\n",
    "    params = {\n",
    "        \"hflip\": hflip_enabled and random.random() < 0.5,\n",
    "        \"angle\": 0.0,\n",
    "        \"zoom\": 1.0,\n",
    "        \"shear\": 0.0,\n",
    "        \"tx\": 0,\n",
    "        \"ty\": 0,\n",
    "        \"brightness\": None,\n",
    "        \"contrast\": None,\n",
    "        \"gamma\": None,\n",
    "        \"shadow\": False,\n",
    "        \"reverse_sequence\": AUGMENT_CONFIG.get(\"reverse_sequence\", False) and random.random() < 0.5,\n",
    "    }\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"rotation\", 0) > 0:\n",
    "        params[\"angle\"] = random.uniform(-AUGMENT_CONFIG[\"rotation\"], AUGMENT_CONFIG[\"rotation\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"zoom\", 0) > 0:\n",
    "        params[\"zoom\"] = random.uniform(1 - AUGMENT_CONFIG[\"zoom\"], 1 + AUGMENT_CONFIG[\"zoom\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"shear\", 0) > 0:\n",
    "        params[\"shear\"] = random.uniform(-AUGMENT_CONFIG[\"shear\"], AUGMENT_CONFIG[\"shear\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"shift\", 0) > 0:\n",
    "        params[\"tx\"] = int(random.uniform(-AUGMENT_CONFIG[\"shift\"], AUGMENT_CONFIG[\"shift\"]) * IMG_SIZE[0])\n",
    "        params[\"ty\"] = int(random.uniform(-AUGMENT_CONFIG[\"shift\"], AUGMENT_CONFIG[\"shift\"]) * IMG_SIZE[1])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"brightness\"):\n",
    "        params[\"brightness\"] = random.uniform(*AUGMENT_CONFIG[\"brightness\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"contrast\"):\n",
    "        params[\"contrast\"] = random.uniform(*AUGMENT_CONFIG[\"contrast\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"gamma\"):\n",
    "        params[\"gamma\"] = random.uniform(*AUGMENT_CONFIG[\"gamma\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"shadow\") and random.random() < 0.5:\n",
    "        params[\"shadow\"] = True\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def apply_aug(img, params):\n",
    "    if params.get(\"hflip\"):\n",
    "        img = cv2.flip(img, 1)\n",
    "\n",
    "    angle = params.get(\"angle\", 0.0)\n",
    "    if angle:\n",
    "        M = cv2.getRotationMatrix2D((IMG_SIZE[0] / 2, IMG_SIZE[1] / 2), angle, 1.0)\n",
    "        img = cv2.warpAffine(img, M, IMG_SIZE, borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    zoom = params.get(\"zoom\", 1.0)\n",
    "    if zoom != 1.0:\n",
    "        h, w = IMG_SIZE\n",
    "        img_resized = cv2.resize(img, (int(w * zoom), int(h * zoom)))\n",
    "        if zoom > 1:\n",
    "            startx = (img_resized.shape[1] - w) // 2\n",
    "            starty = (img_resized.shape[0] - h) // 2\n",
    "            img = img_resized[starty : starty + h, startx : startx + w]\n",
    "        else:\n",
    "            pad_x = (w - img_resized.shape[1]) // 2\n",
    "            pad_y = (h - img_resized.shape[0]) // 2\n",
    "            img = cv2.copyMakeBorder(\n",
    "                img_resized,\n",
    "                pad_y,\n",
    "                h - img_resized.shape[0] - pad_y,\n",
    "                pad_x,\n",
    "                w - img_resized.shape[1] - pad_x,\n",
    "                cv2.BORDER_REFLECT,\n",
    "            )\n",
    "\n",
    "    tx, ty = params.get(\"tx\", 0), params.get(\"ty\", 0)\n",
    "    if tx or ty:\n",
    "        M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        img = cv2.warpAffine(img, M, IMG_SIZE, borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    shear = params.get(\"shear\", 0.0)\n",
    "    if shear:\n",
    "        M = np.float32([[1, shear, 0], [0, 1, 0]])\n",
    "        img = cv2.warpAffine(img, M, IMG_SIZE, borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    brightness = params.get(\"brightness\")\n",
    "    if brightness is not None:\n",
    "        img = np.clip(img.astype(np.float32) * brightness, 0, 255).astype(np.uint8)\n",
    "\n",
    "    contrast = params.get(\"contrast\")\n",
    "    if contrast is not None:\n",
    "        img = np.clip(128 + contrast * (img.astype(np.float32) - 128), 0, 255).astype(np.uint8)\n",
    "\n",
    "    gamma = params.get(\"gamma\")\n",
    "    if gamma is not None:\n",
    "        img = np.clip(((img / 255.0) ** gamma) * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    if params.get(\"shadow\"):\n",
    "        img = random_shadow(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def show_augmented_samples(df: pd.DataFrame, n: int = 12):\n",
    "    import matplotlib.pyplot as plt\n",
    "    sample = df.sample(n, replace=True)\n",
    "    params_cache = {}\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, row in enumerate(sample.itertuples(), 1):\n",
    "        img = cv2.imread(row.frame_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        video_id = getattr(row, \"video_id\", None)\n",
    "        if CONSISTENT_VIDEO_AUG and video_id is not None:\n",
    "            params = params_cache.setdefault(video_id, sample_aug_params())\n",
    "        else:\n",
    "            params = sample_aug_params()\n",
    "        img = apply_aug(img, params)\n",
    "        plt.subplot(3, 4, i)\n",
    "        plt.imshow(img)\n",
    "        label = CLASS_NAMES[int(row.class_id)] if int(row.class_id) < len(CLASS_NAMES) else str(row.class_id)\n",
    "        plt.title(label, fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(\"Augmented samples\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class FrameGen(keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size, augment=False, augment_multiplier=1, shuffle=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augment\n",
    "        self.augment_multiplier = max(1, int(augment_multiplier))\n",
    "        self.shuffle = shuffle\n",
    "        self.consistent_video_aug = CONSISTENT_VIDEO_AUG and \"video_id\" in self.df.columns\n",
    "        self.video_aug_params = {}\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(math.floor(len(self.df) * self.augment_multiplier / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        if self.augment and self.consistent_video_aug:\n",
    "            self.video_aug_params = {\n",
    "                vid: sample_aug_params()\n",
    "                for vid in self.df[\"video_id\"].dropna().unique().tolist()\n",
    "            }\n",
    "\n",
    "    def _get_params(self, video_id=None):\n",
    "        if self.consistent_video_aug and video_id in self.video_aug_params:\n",
    "            return self.video_aug_params[video_id]\n",
    "        return sample_aug_params()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = np.random.choice(self.indices, size=self.batch_size, replace=True)\n",
    "        X = np.empty((self.batch_size, *IMG_SIZE, 3), np.float32)\n",
    "        y = np.empty((self.batch_size, NUM_CLASSES), np.float32)\n",
    "        for j, i in enumerate(ids):\n",
    "            row = self.df.iloc[i]\n",
    "            img = cv2.imread(row.frame_path)\n",
    "            if img is None:\n",
    "                img = np.zeros((*IMG_SIZE, 3), np.uint8)\n",
    "            else:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if self.augment:\n",
    "                video_id = row.get(\"video_id\") if self.consistent_video_aug else None\n",
    "                params = self._get_params(video_id)\n",
    "                img = apply_aug(img, params)\n",
    "            X[j] = img.astype(np.float32) / 255.0\n",
    "            y[j] = keras.utils.to_categorical(int(row.class_id), NUM_CLASSES)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def build_sequences(df: pd.DataFrame, seq_len: int, stride: int, max_per_video: int):\n",
    "    sequences = []\n",
    "    for vid, group in df.groupby(\"video_id\"):\n",
    "        group = group.sort_values(\"frame_idx\")\n",
    "        frames = group[\"frame_path\"].tolist()\n",
    "        labels = group[\"class_id\"].tolist()\n",
    "        count = 0\n",
    "        for start in range(0, len(frames) - seq_len + 1, stride):\n",
    "            seq = frames[start : start + seq_len]\n",
    "            label = int(round(np.mean(labels[start : start + seq_len])))\n",
    "            sequences.append((seq, label, vid))\n",
    "            count += 1\n",
    "            if count >= max_per_video:\n",
    "                break\n",
    "    return sequences\n",
    "\n",
    "\n",
    "class SequenceGen(keras.utils.Sequence):\n",
    "    def __init__(self, sequences, batch_size, augment=False, augment_multiplier=1, shuffle=True):\n",
    "        self.sequences = sequences\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augment\n",
    "        self.augment_multiplier = max(1, int(augment_multiplier))\n",
    "        self.shuffle = shuffle\n",
    "        self.consistent_video_aug = CONSISTENT_VIDEO_AUG\n",
    "        self.video_aug_params = {}\n",
    "        self.indices = np.arange(len(self.sequences))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(math.floor(len(self.sequences) * self.augment_multiplier / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        if self.augment and self.consistent_video_aug:\n",
    "            video_ids = {seq[2] for seq in self.sequences}\n",
    "            self.video_aug_params = {vid: sample_aug_params() for vid in video_ids}\n",
    "\n",
    "    def _get_params(self, video_id=None):\n",
    "        if self.consistent_video_aug and video_id in self.video_aug_params:\n",
    "            return self.video_aug_params[video_id]\n",
    "        return sample_aug_params()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = np.random.choice(self.indices, size=self.batch_size, replace=True)\n",
    "        X = np.empty((self.batch_size, SEQUENCE_LENGTH, *IMG_SIZE, 3), np.float32)\n",
    "        y = np.empty((self.batch_size, NUM_CLASSES), np.float32)\n",
    "        for j, i in enumerate(ids):\n",
    "            seq_paths, label, video_id = self.sequences[i]\n",
    "            params = self._get_params(video_id) if self.augment else None\n",
    "            if self.augment and params.get(\"reverse_sequence\"):\n",
    "                seq_paths = list(reversed(seq_paths))\n",
    "            frames = []\n",
    "            for p in seq_paths:\n",
    "                img = cv2.imread(p)\n",
    "                if img is None:\n",
    "                    img = np.zeros((*IMG_SIZE, 3), np.uint8)\n",
    "                else:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                if self.augment:\n",
    "                    img = apply_aug(img, params)\n",
    "                frames.append(img.astype(np.float32) / 255.0)\n",
    "            X[j] = np.stack(frames, axis=0)\n",
    "            y[j] = keras.utils.to_categorical(label, NUM_CLASSES)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def offline_augment_train(train_df: pd.DataFrame, out_dir: Path) -> pd.DataFrame:\n",
    "    if AUGMENT_MULTIPLIER <= 1:\n",
    "        return train_df\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    rows = []\n",
    "    if CONSISTENT_VIDEO_AUG and \"video_id\" in train_df.columns:\n",
    "        groups = train_df.groupby(\"video_id\")\n",
    "    else:\n",
    "        groups = [(None, train_df)]\n",
    "    for video_id, group in tqdm(groups, desc=\"offline augment\"):\n",
    "        for row in group.itertuples():\n",
    "            rows.append({\n",
    "                \"frame_path\": row.frame_path,\n",
    "                \"class_id\": row.class_id,\n",
    "                \"video_id\": getattr(row, \"video_id\", None),\n",
    "                \"frame_idx\": getattr(row, \"frame_idx\", 0),\n",
    "            })\n",
    "        num_aug = min(AUGMENT_MULTIPLIER - 1, AUGMENT_MAX_PER_SAMPLE)\n",
    "        for k in range(num_aug):\n",
    "            params = sample_aug_params()\n",
    "            for row in group.itertuples():\n",
    "                img = cv2.imread(row.frame_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                aug = apply_aug(img, params)\n",
    "                vid_tag = str(video_id) if video_id is not None else \"sample\"\n",
    "                out_path = out_dir / f\"aug_{vid_tag}_{k}_{row.Index}.jpg\"\n",
    "                cv2.imwrite(str(out_path), aug[:, :, ::-1])\n",
    "                rows.append({\n",
    "                    \"frame_path\": str(out_path),\n",
    "                    \"class_id\": row.class_id,\n",
    "                    \"video_id\": getattr(row, \"video_id\", None),\n",
    "                    \"frame_idx\": getattr(row, \"frame_idx\", 0),\n",
    "                })\n",
    "    out_df = pd.DataFrame(rows).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def build_frame_model(backbone: str, lr: float):\n",
    "    if backbone == \"mobilenetv2\":\n",
    "        base = keras.applications.MobileNetV2(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    elif backbone == \"resnet50\":\n",
    "        base = keras.applications.ResNet50(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    elif backbone == \"efficientnetb0\":\n",
    "        base = keras.applications.EfficientNetB0(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    else:\n",
    "        raise ValueError(backbone)\n",
    "    base.trainable = False\n",
    "    x = keras.layers.Dropout(0.5)(base.output)\n",
    "    out = keras.layers.Dense(NUM_CLASSES, activation=\"softmax\", dtype=\"float32\")(x)\n",
    "    model = keras.Model(base.input, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_temporal_model(kind: str, lr: float):\n",
    "    inp = keras.Input(shape=(SEQUENCE_LENGTH, *IMG_SIZE, 3))\n",
    "    base = keras.applications.MobileNetV2(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    base.trainable = False\n",
    "    x = keras.layers.TimeDistributed(base)(inp)\n",
    "    if kind == \"lstm\":\n",
    "        x = keras.layers.LSTM(128)(x)\n",
    "    elif kind == \"gru\":\n",
    "        x = keras.layers.GRU(128)(x)\n",
    "    else:\n",
    "        raise ValueError(kind)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    out = keras.layers.Dense(NUM_CLASSES, activation=\"softmax\", dtype=\"float32\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_3d_cnn(lr: float):\n",
    "    inp = keras.Input(shape=(SEQUENCE_LENGTH, *IMG_SIZE, 3))\n",
    "    x = keras.layers.Conv3D(32, (3, 3, 3), activation=\"relu\", padding=\"same\")(inp)\n",
    "    x = keras.layers.MaxPool3D((1, 2, 2))(x)\n",
    "    x = keras.layers.Conv3D(64, (3, 3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = keras.layers.MaxPool3D((2, 2, 2))(x)\n",
    "    x = keras.layers.Conv3D(128, (3, 3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = keras.layers.GlobalAveragePooling3D()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    out = keras.layers.Dense(NUM_CLASSES, activation=\"softmax\", dtype=\"float32\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "\n",
    "# Multi-GPU strategy\n",
    "_gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(_gpus) > 1:\n",
    "    STRATEGY = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    STRATEGY = tf.distribute.get_strategy()\n",
    "NUM_REPLICAS = STRATEGY.num_replicas_in_sync\n",
    "print('Using strategy:', STRATEGY, 'replicas:', NUM_REPLICAS)\n",
    "\n",
    "# Auto batch sizing target\n",
    "TARGET_GPU_UTIL = 0.9\n",
    "FRAME_MB_ESTIMATE = 8.0\n",
    "SEQ_MB_ESTIMATE = 24.0\n",
    "MAX_BATCH_FRAME = 512\n",
    "MAX_BATCH_SEQ = 128\n",
    "\n",
    "\n",
    "def _gpu_total_mb():\n",
    "    try:\n",
    "        out = subprocess.check_output([\n",
    "            'nvidia-smi',\n",
    "            '--query-gpu=memory.total',\n",
    "            '--format=csv,noheader,nounits',\n",
    "        ])\n",
    "        return int(out.decode().strip().splitlines()[0])\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def _auto_batch(base, per_sample_mb, max_bs):\n",
    "    if not AUTO_TUNE_BATCH:\n",
    "        return base\n",
    "    total_mb = _gpu_total_mb()\n",
    "    if total_mb <= 0:\n",
    "        return base\n",
    "    target_mb = total_mb * TARGET_GPU_UTIL\n",
    "    bs = int(target_mb / per_sample_mb)\n",
    "    bs = max(base, min(max_bs, bs))\n",
    "    bs = bs * max(1, NUM_REPLICAS)\n",
    "    return bs\n",
    "\n",
    "\n",
    "FRAME_BATCH = _auto_batch(BATCH_MOBILENET, FRAME_MB_ESTIMATE, MAX_BATCH_FRAME)\n",
    "SEQ_BATCH = _auto_batch(BATCH_SEQUENCE, SEQ_MB_ESTIMATE, MAX_BATCH_SEQ)\n",
    "print('Auto batch sizes -> frame:', FRAME_BATCH, 'sequence:', SEQ_BATCH)\n",
    "\n",
    "\n",
    "class LivePlot(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.history = {\"loss\": [], \"val_loss\": [], \"accuracy\": [], \"val_accuracy\": []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        for k in self.history:\n",
    "            if k in logs:\n",
    "                self.history[k].append(logs[k])\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        ax[0].plot(self.history[\"loss\"], label=\"loss\")\n",
    "        ax[0].plot(self.history[\"val_loss\"], label=\"val_loss\")\n",
    "        ax[0].legend(); ax[0].set_title(\"Loss\")\n",
    "        ax[1].plot(self.history[\"accuracy\"], label=\"acc\")\n",
    "        ax[1].plot(self.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "        ax[1].legend(); ax[1].set_title(\"Accuracy\")\n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "\n",
    "\n",
    "def cleanup_old_checkpoints(base_dir: Path, keep: int = 3):\n",
    "    if not base_dir.exists():\n",
    "        return\n",
    "    dirs = sorted([p for p in base_dir.iterdir() if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    for d in dirs[keep:]:\n",
    "        shutil.rmtree(d, ignore_errors=True)\n",
    "\n",
    "\n",
    "def _fit_with_batch(model_builder, train_gen_fn, val_gen_fn, batch_size, callbacks):\n",
    "    bs = batch_size\n",
    "    while bs >= 8:\n",
    "        try:\n",
    "            tf.keras.backend.clear_session()\n",
    "            with STRATEGY.scope():\n",
    "                model = model_builder()\n",
    "            train_gen = train_gen_fn(bs)\n",
    "            val_gen = val_gen_fn(bs)\n",
    "            model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, callbacks=callbacks, verbose=1)\n",
    "            return model, bs\n",
    "        except tf.errors.ResourceExhaustedError:\n",
    "            print('OOM at batch', bs, 'retrying with smaller batch')\n",
    "            bs = bs // 2\n",
    "    raise RuntimeError('No viable batch size')\n",
    "\n",
    "\n",
    "def train_and_eval_frame(model_name, train_df, val_df, test_df, dataset_name):\n",
    "    run_dir = CKPT_DIR / dataset_name / model_name / str(int(time.time()))\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_path = run_dir / 'best.keras'\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(str(ckpt_path), save_best_only=True, monitor='val_accuracy', mode='max'),\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-6),\n",
    "        keras.callbacks.TensorBoard(log_dir=str(LOGS_DIR / f\"{dataset_name}_{model_name}_{int(time.time())}\")),\n",
    "        LivePlot(),\n",
    "    ]\n",
    "    def model_builder():\n",
    "        return build_frame_model(model_name, LR)\n",
    "    def train_gen_fn(bs):\n",
    "        return FrameGen(train_df, bs, augment=USE_ON_THE_FLY_AUGMENT, augment_multiplier=1)\n",
    "    def val_gen_fn(bs):\n",
    "        return FrameGen(val_df, bs, augment=False, augment_multiplier=1)\n",
    "\n",
    "    model, used_bs = _fit_with_batch(model_builder, train_gen_fn, val_gen_fn, FRAME_BATCH, callbacks)\n",
    "\n",
    "    model_dir = MODELS_DIR / dataset_name\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    final_path = model_dir / f\"{model_name}_final.keras\"\n",
    "    model.save(final_path)\n",
    "\n",
    "    test_gen = FrameGen(test_df, used_bs, augment=False, augment_multiplier=1)\n",
    "    loss, acc = model.evaluate(test_gen, verbose=1)\n",
    "    print(model_name, 'test acc', acc)\n",
    "\n",
    "    cleanup_old_checkpoints(CKPT_DIR / dataset_name / model_name, keep=3)\n",
    "    return final_path\n",
    "\n",
    "\n",
    "def train_and_eval_sequence(model_name, train_df, val_df, test_df, dataset_name):\n",
    "    sequences_train = build_sequences(train_df, SEQUENCE_LENGTH, SEQUENCE_STRIDE, MAX_SEQUENCES_PER_VIDEO)\n",
    "    sequences_val = build_sequences(val_df, SEQUENCE_LENGTH, SEQUENCE_STRIDE, MAX_SEQUENCES_PER_VIDEO)\n",
    "    sequences_test = build_sequences(test_df, SEQUENCE_LENGTH, SEQUENCE_STRIDE, MAX_SEQUENCES_PER_VIDEO)\n",
    "    if not sequences_train:\n",
    "        print('No sequences for', dataset_name, 'skipping', model_name)\n",
    "        return None\n",
    "\n",
    "    run_dir = CKPT_DIR / dataset_name / model_name / str(int(time.time()))\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_path = run_dir / 'best.keras'\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(str(ckpt_path), save_best_only=True, monitor='val_accuracy', mode='max'),\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-6),\n",
    "        keras.callbacks.TensorBoard(log_dir=str(LOGS_DIR / f\"{dataset_name}_{model_name}_{int(time.time())}\")),\n",
    "        LivePlot(),\n",
    "    ]\n",
    "\n",
    "    def model_builder():\n",
    "        if model_name in ['lstm', 'gru']:\n",
    "            return build_temporal_model(model_name, LR)\n",
    "        return build_3d_cnn(LR)\n",
    "\n",
    "    def train_gen_fn(bs):\n",
    "        return SequenceGen(sequences_train, bs, augment=USE_ON_THE_FLY_AUGMENT, augment_multiplier=1)\n",
    "\n",
    "    def val_gen_fn(bs):\n",
    "        return SequenceGen(sequences_val, bs, augment=False, augment_multiplier=1)\n",
    "\n",
    "    model, used_bs = _fit_with_batch(model_builder, train_gen_fn, val_gen_fn, SEQ_BATCH, callbacks)\n",
    "\n",
    "    model_dir = MODELS_DIR / dataset_name\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    final_path = model_dir / f\"{model_name}_final.keras\"\n",
    "    model.save(final_path)\n",
    "\n",
    "    test_gen = SequenceGen(sequences_test, used_bs, augment=False, augment_multiplier=1)\n",
    "    loss, acc = model.evaluate(test_gen, verbose=1)\n",
    "    print(model_name, 'test acc', acc)\n",
    "\n",
    "    cleanup_old_checkpoints(CKPT_DIR / dataset_name / model_name, keep=3)\n",
    "    return final_path\n",
    "\n",
    "\n",
    "def cleanup_dataset_files(dataset_name: str, train_df: pd.DataFrame):\n",
    "    if CLEANUP_TRAIN:\n",
    "        for p in train_df['frame_path'].tolist():\n",
    "            try:\n",
    "                os.remove(p)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        train_csv = PROCESSED_DIR / dataset_name / 'train.csv'\n",
    "        train_aug = PROCESSED_DIR / dataset_name / 'train_aug.csv'\n",
    "        train_csv.unlink(missing_ok=True)\n",
    "        train_aug.unlink(missing_ok=True)\n",
    "    if CLEANUP_RAW:\n",
    "        shutil.rmtree(RAW_DIR / dataset_name, ignore_errors=True)\n",
    "\n",
    "\n",
    "def process_dataset(name: str):\n",
    "    ensure_dataset(name)\n",
    "    raw_dir = RAW_DIR / name\n",
    "    show_random_video(raw_dir)\n",
    "\n",
    "    out_dir = PROCESSED_DIR / name\n",
    "    train_csv = out_dir / 'train.csv'\n",
    "    val_csv = out_dir / 'val.csv'\n",
    "    test_csv = out_dir / 'test.csv'\n",
    "    if not (train_csv.exists() and val_csv.exists() and test_csv.exists()):\n",
    "        preprocess_dataset(name)\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    val_df = pd.read_csv(val_csv)\n",
    "    test_df = pd.read_csv(test_csv)\n",
    "\n",
    "    show_random_samples(train_df, title=f\"{name} samples\")\n",
    "    show_augmented_samples(train_df, n=12)\n",
    "\n",
    "    if USE_OFFLINE_AUGMENT:\n",
    "        aug_dir = out_dir / 'augmented'\n",
    "        train_df = offline_augment_train(train_df, aug_dir)\n",
    "        train_df.to_csv(out_dir / 'train_aug.csv', index=False)\n",
    "\n",
    "    for model_name in MODELS:\n",
    "        if model_name in ['mobilenetv2', 'resnet50', 'efficientnetb0']:\n",
    "            train_and_eval_frame(model_name, train_df, val_df, test_df, name)\n",
    "        elif model_name in ['lstm', 'gru', '3d_cnn']:\n",
    "            train_and_eval_sequence(model_name, train_df, val_df, test_df, name)\n",
    "        else:\n",
    "            print('Unknown model', model_name)\n",
    "\n",
    "    cleanup_dataset_files(name, train_df)\n",
    "\n",
    "\n",
    "for ds in DATASETS:\n",
    "    process_dataset(ds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}