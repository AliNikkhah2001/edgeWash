{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Handwashing Detection Training Pipeline\n",
    "\n",
    "**Complete training pipeline using modular Python modules**\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Dataset download (Kaggle WHO6)\n",
    "2. Data preprocessing (frame extraction)\n",
    "3. Model training (MobileNetV2)\n",
    "4. Evaluation and visualization\n",
    "5. Model comparison\n",
    "\n",
    "**Runtime**: GPU (recommended for training)\n",
    "\n",
    "**Expected Duration**: 2-3 hours for complete pipeline\n",
    "\n",
    "**Author**: Generated with AdaL (https://github.com/sylphai/adal-cli)\n",
    "\n",
    "**Date**: 2025-12-31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set working directory\n",
    "    import os\n",
    "    WORK_DIR = '/content/drive/MyDrive/handwash_training'\n",
    "    os.makedirs(WORK_DIR, exist_ok=True)\n",
    "    %cd {WORK_DIR}\n",
    "else:\n",
    "    WORK_DIR = '.'\n",
    "    print(f\"Working directory: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (TensorFlow preinstalled on Colab)\n",
    "!pip install -q scikit-learn pandas numpy opencv-python-headless\n",
    "!pip install -q matplotlib seaborn tqdm requests nbformat\n",
    "\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Training Modules\n",
    "\n",
    "Clone the modular Python training modules from your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not already cloned) and set repo root\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_URL = 'https://github.com/AliNikkhah2001/edgeWash.git'\n",
    "# Try common locations under Drive to avoid nested paths\n",
    "PREFERRED_DIR = Path('/content/drive/MyDrive/edgeWash')\n",
    "if 'WORK_DIR' in globals():\n",
    "    PREFERRED_DIR = Path(WORK_DIR) / 'edgeWash'\n",
    "PREFERRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(PREFERRED_DIR)\n",
    "\n",
    "REPO_DIR = Path('edgeWash')\n",
    "if not REPO_DIR.exists():\n",
    "    print(f'Cloning repository from {REPO_URL}...')\n",
    "    !git clone {REPO_URL}\n",
    "else:\n",
    "    print(f'Repository already exists: {REPO_DIR}')\n",
    "    print('Pulling latest changes...')\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# Enter repo root\n",
    "os.chdir(REPO_DIR)\n",
    "repo_root = Path.cwd()\n",
    "print('Repo root:', repo_root)\n",
    "\n",
    "# Add training modules to Python path\n",
    "training_dir = repo_root / 'training'\n",
    "if str(training_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(training_dir))\n",
    "print('Training modules path:', training_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training modules with robust path resolution\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_repo_root():\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path('/content/edgeWash'),\n",
    "        Path('/content/drive/MyDrive/edgeWash'),\n",
    "        Path('/content/drive/MyDrive/handwash_training'),\n",
    "        Path('/content/drive/MyDrive/handwash_training_colab/edgeWash'),\n",
    "        Path('/content/drive/MyDrive/handwash_training_colab/edgeWash/edgeWash')\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if (c / 'training' / 'config.py').exists():\n",
    "            os.chdir(c)\n",
    "            td = c / 'training'\n",
    "            if str(td) not in sys.path:\n",
    "                sys.path.insert(0, str(td))\n",
    "            print('Using repo root:', c)\n",
    "            print('Added to sys.path:', td)\n",
    "            return c, td\n",
    "    raise FileNotFoundError('Cannot locate training/config.py; check your repo path.')\n",
    "\n",
    "repo_root, training_dir = ensure_repo_root()\n",
    "\n",
    "import config\n",
    "import download_datasets\n",
    "import preprocess_data\n",
    "import data_generators\n",
    "import models\n",
    "import train as train_module\n",
    "import evaluate\n",
    "\n",
    "print('Training modules imported successfully!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration & Google Drive Paths\n",
    "\n",
    "View and customize training hyperparameters.\n",
    "\n",
    "**Important**: Checkpoints, logs, and models will be saved to Google Drive for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override config paths to save to Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    # Update paths to Google Drive\n",
    "    config.WORK_DIR = Path(WORK_DIR)\n",
    "    config.DATA_DIR = config.WORK_DIR / 'datasets'\n",
    "    config.RAW_DIR = config.DATA_DIR / 'raw'\n",
    "    config.PROCESSED_DIR = config.DATA_DIR / 'processed'\n",
    "    config.MODELS_DIR = config.WORK_DIR / 'models'\n",
    "    config.CHECKPOINTS_DIR = config.WORK_DIR / 'checkpoints'\n",
    "    config.LOGS_DIR = config.WORK_DIR / 'logs'\n",
    "    config.RESULTS_DIR = config.WORK_DIR / 'results'\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_path in [config.DATA_DIR, config.RAW_DIR, config.PROCESSED_DIR, config.MODELS_DIR, config.CHECKPOINTS_DIR, config.LOGS_DIR, config.RESULTS_DIR]:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\u2713 Paths configured to save to Google Drive:\")\n",
    "    print(f\"  Data: {config.DATA_DIR}\")\n",
    "    print(f\"  Models: {config.MODELS_DIR}\")\n",
    "    print(f\"  Checkpoints: {config.CHECKPOINTS_DIR}\")\n",
    "    print(f\"  Logs: {config.LOGS_DIR}\")\n",
    "    print(f\"  Results: {config.RESULTS_DIR}\")\n",
    "else:\n",
    "    print(\"Running locally - using default paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display configuration\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nImage size: {config.IMG_SIZE}\")\n",
    "print(f\"Sequence length: {config.SEQUENCE_LENGTH}\")\n",
    "print(f\"Number of classes: {config.NUM_CLASSES}\")\n",
    "print(f\"Class names: {config.CLASS_NAMES}\")\n",
    "\n",
    "print(f\"\\nBatch size: {config.BATCH_SIZE}\")\n",
    "print(f\"Epochs: {config.EPOCHS}\")\n",
    "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"Early stopping patience: {config.PATIENCE}\")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {config.TRAIN_RATIO*100:.0f}%\")\n",
    "print(f\"  Val:   {config.VAL_RATIO*100:.0f}%\")\n",
    "print(f\"  Test:  {config.TEST_RATIO*100:.0f}%\")\n",
    "\n",
    "print(f\"\\nAugmentation:\")\n",
    "for key, value in config.AUGMENTATION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nModel architectures available:\")\n",
    "for model_name, model_config in config.MODEL_CONFIGS.items():\n",
    "    print(f\"  - {model_name}: {model_config['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Download\n",
    "\n",
    "Download Kaggle WHO6 dataset (~1 GB, quick start).\n",
    "\n",
    "For full pipeline, also download PSKUS (18 GB) and METC (2 GB) - see commented code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle WHO6 dataset\n",
    "print(\"Downloading Kaggle WHO6 dataset...\")\n",
    "success = download_datasets.download_kaggle_dataset()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n\u2713 Kaggle dataset ready!\")\n",
    "else:\n",
    "    print(\"\\n\u2717 Kaggle dataset download failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download PSKUS and METC datasets (large, requires zenodo-get)\n",
    "# Uncomment to download:\n",
    "\n",
    "# # Install zenodo-get\n",
    "# !pip install zenodo-get\n",
    "\n",
    "# # Download PSKUS (18 GB, ~30-60 minutes)\n",
    "# print(\"Downloading PSKUS Hospital dataset (18 GB)...\")\n",
    "# download_datasets.download_pskus_dataset()\n",
    "\n",
    "# # Download METC (2 GB, ~5-10 minutes)\n",
    "# print(\"Downloading METC Lab dataset (2 GB)...\")\n",
    "# download_datasets.download_metc_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify datasets\n",
    "status = download_datasets.verify_datasets()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name, info in status.items():\n",
    "    status_icon = \"\u2713\" if info['exists'] else \"\u2717\"\n",
    "    print(f\"{status_icon} {info['name']}: {info['num_files']} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "Extract frames from videos and create train/val/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Kaggle dataset\n",
    "print(\"Preprocessing Kaggle dataset...\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "result = preprocess_data.preprocess_all_datasets(\n",
    "    use_kaggle=True,\n",
    "    use_pskus=False,  # Set True if PSKUS downloaded\n",
    "    use_metc=False    # Set True if METC downloaded\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(\"\\n\u2713 Preprocessing complete!\")\n",
    "    print(f\"\\nProcessed files:\")\n",
    "    for key, path in result.items():\n",
    "        print(f\"  {key}: {path}\")\n",
    "else:\n",
    "    print(\"\\n\u2717 Preprocessing failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Exploration\n",
    "\n",
    "Visualize dataset statistics and sample frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "train_df = pd.read_csv(config.PROCESSED_DIR / 'train.csv')\n",
    "val_df = pd.read_csv(config.PROCESSED_DIR / 'val.csv')\n",
    "test_df = pd.read_csv(config.PROCESSED_DIR / 'test.csv')\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_df)} frames ({len(train_df['video_id'].unique())} videos)\")\n",
    "print(f\"  Val:   {len(val_df)} frames ({len(val_df['video_id'].unique())} videos)\")\n",
    "print(f\"  Test:  {len(test_df)} frames ({len(test_df['video_id'].unique())} videos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df, split_name) in enumerate([(train_df, 'Train'), (val_df, 'Val'), (test_df, 'Test')]):\n",
    "    class_counts = df['class_name'].value_counts()\n",
    "    \n",
    "    axes[idx].bar(range(len(class_counts)), class_counts.values)\n",
    "    axes[idx].set_title(f'{split_name} Set - Class Distribution', fontsize=12)\n",
    "    axes[idx].set_xlabel('Class', fontsize=10)\n",
    "    axes[idx].set_ylabel('Number of Frames', fontsize=10)\n",
    "    axes[idx].set_xticks(range(len(class_counts)))\n",
    "    axes[idx].set_xticklabels([cn.split('_')[-1] for cn in class_counts.index], rotation=45, ha='right')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}