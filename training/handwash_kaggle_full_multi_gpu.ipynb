{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwash Full Pipeline (Kaggle)\n",
    "Self contained notebook for Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q --no-cache-dir scikit-learn pandas numpy opencv-python-headless matplotlib seaborn tqdm requests gdown zenodo-get ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from tensorflow import keras\n",
    "# =========================\n",
    "# Standard library\n",
    "# =========================\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# =========================\n",
    "# Third-party\n",
    "# =========================\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# TensorFlow / Keras  \u2190 THIS IS WHAT YOU ARE MISSING\n",
    "# =========================\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, sys, json, time, math, random, shutil, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "RUN_NAME = os.environ.get(\"RUN_NAME\", \"handwash_run\")\n",
    "WORK_DIR = Path(\"/kaggle/working/handwash_runs\") / RUN_NAME\n",
    "DATA_DIR = Path(\"/kaggle/working/handwash_data\")\n",
    "\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "MODELS_DIR = WORK_DIR / \"models\"\n",
    "CKPT_DIR = WORK_DIR / \"checkpoints\"\n",
    "LOGS_DIR = WORK_DIR / \"logs\"\n",
    "\n",
    "for p in [WORK_DIR, DATA_DIR, RAW_DIR, PROCESSED_DIR, MODELS_DIR, CKPT_DIR, LOGS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"Note: Enable internet in Kaggle if downloads fail.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "All options are user editable.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# User config (edit these)\n",
    "DATASETS = [\"kaggle\", \"pskus\", \"metc\", \"synthetic_blender_rozakar\"]\n",
    "\n",
    "AVAILABLE_FRAME_MODELS = [\n",
    "    \"mobilenetv2\",\n",
    "    \"resnet50\",\n",
    "    \"resnet101\",\n",
    "    \"resnet152\",\n",
    "    \"efficientnetb0\",\n",
    "    \"efficientnetb3\",\n",
    "    \"efficientnetv2b0\",\n",
    "    \"convnext_tiny\",\n",
    "    \"vit_b16\",\n",
    "]\n",
    "AVAILABLE_SEQUENCE_MODELS = [\"lstm\", \"gru\", \"3d_cnn\"]\n",
    "\n",
    "MODELS = [\"mobilenetv2\", \"resnet50\", \"efficientnetb0\", \"lstm\", \"gru\", \"3d_cnn\"]\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "NUM_CLASSES = 7\n",
    "CLASS_NAMES = [\n",
    "    \"Other\",\n",
    "    \"Step1_PalmToPalm\",\n",
    "    \"Step2_PalmOverDorsum\",\n",
    "    \"Step3_InterlacedFingers\",\n",
    "    \"Step4_BackOfFingers\",\n",
    "    \"Step5_ThumbRub\",\n",
    "    \"Step6_Fingertips\",\n",
    "]\n",
    "PSKUS_CODE_MAPPING = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 3,\n",
    "    4: 4,\n",
    "    5: 5,\n",
    "    6: 6,\n",
    "    7: 0,\n",
    "}\n",
    "\n",
    "\n",
    "FRAME_SKIP = 2\n",
    "SEQUENCE_LENGTH = 16\n",
    "SEQUENCE_STRIDE = 1\n",
    "MAX_SEQUENCES_PER_VIDEO = 200\n",
    "\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "BATCH_MOBILENET = 128\n",
    "BATCH_SEQUENCE = 64\n",
    "AUTO_TUNE_BATCH = True\n",
    "MIXED_PRECISION = True\n",
    "TB_PORT = 6008\n",
    "RESUME_MODEL_PATHS = {}  # e.g., {\"mobilenetv2\": \"/kaggle/input/your-model/mobilenetv2_final.keras\"}\n",
    "RECOMPILE_ON_RESUME = False  # set True to reset optimizer/LR on resume\n",
    "\n",
    "\n",
    "# Optimizer + loss\n",
    "OPTIMIZER_NAME = \"adamw\"\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# ResNet50 fine-tuning schedule\n",
    "RESNET50_SCHEDULE = True\n",
    "RESNET50_STAGE0_EPOCHS = 5\n",
    "RESNET50_STAGE1_EPOCHS = 10\n",
    "RESNET50_STAGE2_EPOCHS = 20\n",
    "RESNET50_STAGE0_LR = 3e-4\n",
    "RESNET50_STAGE1_LR = 1e-4\n",
    "RESNET50_STAGE2_LR = 3e-5\n",
    "RESNET50_STAGE0_WD = 1e-4\n",
    "RESNET50_STAGE1_WD = 1e-4\n",
    "RESNET50_STAGE2_WD = 5e-5\n",
    "\n",
    "# Augmentation\n",
    "USE_OFFLINE_AUGMENT = True  # generate augmented samples on disk\n",
    "USE_ON_THE_FLY_AUGMENT = False  # apply aug during loading\n",
    "AUGMENT_MULTIPLIER = 4  # how many total samples per original (1 = none)\n",
    "AUGMENT_MAX_PER_SAMPLE = 3  # cap for offline augment per original\n",
    "AUGMENT_PROB = None  # 0-1 ratio; None derives from AUGMENT_MULTIPLIER\n",
    "CONSISTENT_VIDEO_AUG = True  # keep the same aug per video\n",
    "AUGMENT_CONFIG = {\n",
    "    \"rotation\": 15,\n",
    "    \"zoom\": 0.15,\n",
    "    \"shift\": 0.1,\n",
    "    \"shear\": 0.1,\n",
    "    \"brightness\": (0.8, 1.2),\n",
    "    \"contrast\": (0.8, 1.2),\n",
    "    \"gamma\": (0.8, 1.2),\n",
    "    \"hflip\": True,\n",
    "    \"mid_flip\": True,\n",
    "    \"shadow\": True,\n",
    "    \"reverse_sequence\": True,\n",
    "}\n",
    "\n",
    "# Reporting\n",
    "SHOW_CONFUSION_MATRICES = True\n",
    "CONFUSION_NORMALIZE = False\n",
    "EVAL_TEST_EACH_EPOCH = True\n",
    "\n",
    "# Cleanup\n",
    "SKIP_DOWNLOAD_IF_PRESENT = True\n",
    "CLEANUP_RAW = True\n",
    "CLEANUP_TRAIN = True\n",
    "KEEP_VAL_TEST = True\n",
    "\n",
    "# Dataset sources\n",
    "KAGGLE_URL = \"https://github.com/atiselsts/data/raw/master/kaggle-dataset-6classes.tar\"\n",
    "PSKUS_ZENODO = \"4537209\"\n",
    "METC_ZENODO = \"5808789\"\n",
    "SYNTHETIC_LINKS = [\n",
    "    \"https://drive.google.com/uc?id=1EW3JQvElcuXzawxEMRkA8YXwK_Ipiv-p&export=download\",\n",
    "    \"https://drive.google.com/uc?id=163TsrDe4q5KTQGCv90JRYFkCs7AGxFip&export=download\",\n",
    "    \"https://drive.google.com/uc?id=1GxyTYfSodumH78NbjWdmbjm8JP8AOkAY&export=download\",\n",
    "    \"https://drive.google.com/uc?id=1IoRsgBBr8qoC3HO-vEr6E7K4UZ6ku6-1&export=download\",\n",
    "    \"https://drive.google.com/uc?id=1svCYnwDazy5FN1DYSgqbGscvDKL_YnID&export=download\",\n",
    "]\n",
    "\n",
    "print(\"Datasets:\", DATASETS)\n",
    "print(\"Models:\", MODELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Auto-tune and mixed precision\n",
    "if MIXED_PRECISION:\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    print(\"Mixed precision enabled\")\n",
    "\n",
    "\n",
    "def get_gpu_mem_mb():\n",
    "    try:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=memory.total\",\n",
    "            \"--format=csv,noheader,nounits\",\n",
    "        ])\n",
    "        return int(out.decode().strip().splitlines()[0])\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "if AUTO_TUNE_BATCH:\n",
    "    mem_mb = get_gpu_mem_mb()\n",
    "    if mem_mb > 0:\n",
    "        BATCH_MOBILENET = max(64, min(256, int(mem_mb / 120)))\n",
    "        BATCH_SEQUENCE = max(32, min(128, int(mem_mb / 240)))\n",
    "        print(\"Auto batch sizes:\", BATCH_MOBILENET, BATCH_SEQUENCE)\n",
    "    else:\n",
    "        print(\"GPU memory not detected; using configured batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Start TensorBoard (logs only; Kaggle does not expose ports)\n",
    "import subprocess\n",
    "\n",
    "tb_proc = subprocess.Popen([\n",
    "    \"tensorboard\",\n",
    "    \"--logdir\", str(LOGS_DIR),\n",
    "    \"--host\", \"0.0.0.0\",\n",
    "    \"--port\", str(TB_PORT),\n",
    "    \"--load_fast=false\",\n",
    "], stdout=open(LOGS_DIR / \"tensorboard.out\", \"w\"), stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"TensorBoard PID:\", tb_proc.pid)\n",
    "print(\"Logs:\", LOGS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /kaggle/working/handwash_runs --host 0.0.0.0 --port 6008\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import tarfile, zipfile\n",
    "from IPython.display import Video, display\n",
    "\n",
    "LABEL_TOKENS = {\n",
    "    \"step1\": 1,\n",
    "    \"step2\": 2,\n",
    "    \"step3\": 3,\n",
    "    \"step4\": 4,\n",
    "    \"step5\": 5,\n",
    "    \"step6\": 6,\n",
    "    \"other\": 0,\n",
    "}\n",
    "\n",
    "VIDEO_EXTS = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n",
    "IMAGE_EXTS = (\".jpg\", \".jpeg\", \".png\")\n",
    "\n",
    "\n",
    "def download_with_progress(url: str, dest: Path):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dest.exists():\n",
    "        print(\"skip\", dest)\n",
    "        return\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"content-length\", 0))\n",
    "        with open(dest, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dest.name) as pbar:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "\n",
    "\n",
    "def extract_tar(tar_path: Path, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with tarfile.open(tar_path) as tfp:\n",
    "        tfp.extractall(out_dir)\n",
    "    tar_path.unlink(missing_ok=True)\n",
    "\n",
    "\n",
    "def extract_zip(zip_path: Path, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        zf.extractall(out_dir)\n",
    "    zip_path.unlink(missing_ok=True)\n",
    "\n",
    "\n",
    "def download_kaggle():\n",
    "    out_dir = RAW_DIR / \"kaggle\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tar_path = out_dir / \"kaggle-dataset-6classes.tar\"\n",
    "    download_with_progress(KAGGLE_URL, tar_path)\n",
    "    print(\"Extracting kaggle...\")\n",
    "    extract_tar(tar_path, out_dir)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def download_zenodo(zenodo_id: str, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cmd = [\"zenodo_get\", \"-r\", zenodo_id, \"-o\", str(out_dir)]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "    zip_files = sorted(out_dir.glob(\"*.zip\"))\n",
    "    tar_files = sorted(out_dir.glob(\"*.tar*\"))\n",
    "    if zip_files or tar_files:\n",
    "        print(\"Extracting Zenodo archives...\")\n",
    "    for zip_file in zip_files:\n",
    "        extract_zip(zip_file, out_dir)\n",
    "    for tar_file in tar_files:\n",
    "        extract_tar(tar_file, out_dir)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def download_pskus():\n",
    "    return download_zenodo(PSKUS_ZENODO, RAW_DIR / \"pskus\")\n",
    "\n",
    "\n",
    "def download_metc():\n",
    "    return download_zenodo(METC_ZENODO, RAW_DIR / \"metc\")\n",
    "\n",
    "\n",
    "def download_synthetic():\n",
    "    out_dir = RAW_DIR / \"synthetic_blender_rozakar\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, link in enumerate(SYNTHETIC_LINKS, 1):\n",
    "        out_zip = out_dir / f\"synth_{i}.zip\"\n",
    "        if not out_zip.exists():\n",
    "            subprocess.check_call([\"gdown\", \"-q\", link, \"-O\", str(out_zip)])\n",
    "        extract_zip(out_zip, out_dir)\n",
    "    return out_dir\n",
    "\n",
    "def _extract_archives_if_needed(raw_dir: Path):\n",
    "    zip_files = sorted(raw_dir.glob(\"*.zip\"))\n",
    "    tar_files = sorted(raw_dir.glob(\"*.tar*\"))\n",
    "    if zip_files or tar_files:\n",
    "        print(\"Extracting existing archives in\", raw_dir)\n",
    "    for zip_file in zip_files:\n",
    "        extract_zip(zip_file, raw_dir)\n",
    "    for tar_file in tar_files:\n",
    "        extract_tar(tar_file, raw_dir)\n",
    "\n",
    "\n",
    "\n",
    "def ensure_dataset(name: str):\n",
    "    raw_dir = RAW_DIR / name\n",
    "    if raw_dir.exists():\n",
    "        _extract_archives_if_needed(raw_dir)\n",
    "        if SKIP_DOWNLOAD_IF_PRESENT:\n",
    "            print(\"skip download, exists\", raw_dir)\n",
    "            return raw_dir\n",
    "    if name == \"kaggle\":\n",
    "        return download_kaggle()\n",
    "    if name == \"pskus\":\n",
    "        return download_pskus()\n",
    "    if name == \"metc\":\n",
    "        return download_metc()\n",
    "    if name == \"synthetic_blender_rozakar\":\n",
    "        return download_synthetic()\n",
    "    raise ValueError(\"Unknown dataset \" + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Video, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def infer_label_from_path(p: Path) -> int:\n",
    "    parts = [part for part in Path(p).parts]\n",
    "    for part in reversed(parts):\n",
    "        if part.isdigit():\n",
    "            class_id = int(part)\n",
    "            if 0 <= class_id < len(CLASS_NAMES):\n",
    "                return class_id\n",
    "    text = str(p).lower()\n",
    "    for token, idx in LABEL_TOKENS.items():\n",
    "        if token in text:\n",
    "            return idx\n",
    "    return 0\n",
    "\n",
    "\n",
    "SYNTHETIC_GESTURE_TO_CLASS = {\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 2,\n",
    "    4: 3,\n",
    "    5: 4,\n",
    "    6: 5,\n",
    "    7: 5,\n",
    "    8: 6,\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_int_from_text(text: str) -> int | None:\n",
    "    match = re.search(r\"(\\d+)\", text)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "\n",
    "def infer_synthetic_class_id(path: Path) -> int | None:\n",
    "    for part in path.parts:\n",
    "        if \"gesture\" in part.lower():\n",
    "            num = _parse_int_from_text(part)\n",
    "            if num is None:\n",
    "                continue\n",
    "            return SYNTHETIC_GESTURE_TO_CLASS.get(num)\n",
    "    return None\n",
    "\n",
    "\n",
    "def synthetic_video_id(path: Path) -> str:\n",
    "    parts = list(path.parts)\n",
    "    gesture_idx = None\n",
    "    for i, part in enumerate(parts):\n",
    "        if part.lower().startswith(\"gesture\"):\n",
    "            gesture_idx = i\n",
    "    if gesture_idx is None or gesture_idx < 2:\n",
    "        return path.stem\n",
    "    character = parts[gesture_idx - 2]\n",
    "    environment = parts[gesture_idx - 1]\n",
    "    gesture = parts[gesture_idx]\n",
    "    return f\"{character}_{environment}_{gesture}\"\n",
    "\n",
    "\n",
    "def parse_frame_idx(path: Path) -> int:\n",
    "    num = _parse_int_from_text(path.stem)\n",
    "    return int(num) if num is not None else 0\n",
    "\n",
    "\n",
    "def _majority_vote(labels, total_movements):\n",
    "    counts = [0] * total_movements\n",
    "    for el in labels:\n",
    "        counts[int(el)] += 1\n",
    "    best = 0\n",
    "    for i in range(1, total_movements):\n",
    "        if counts[best] < counts[i]:\n",
    "            best = i\n",
    "    majority = (len(labels) + 2) // 2\n",
    "    if counts[best] < majority:\n",
    "        return -1\n",
    "    return best\n",
    "\n",
    "\n",
    "def _discount_reaction_indeterminacy(labels, reaction_frames):\n",
    "    new_labels = [u for u in labels]\n",
    "    n = len(labels) - 1\n",
    "    for i in range(n):\n",
    "        if i == 0 or labels[i] != labels[i + 1] or i == n - 1:\n",
    "            start = max(0, i - reaction_frames)\n",
    "            end = i\n",
    "            for j in range(start, end):\n",
    "                new_labels[j] = -1\n",
    "            start = i\n",
    "            end = min(n + 1, i + reaction_frames)\n",
    "            for j in range(start, end):\n",
    "                new_labels[j] = -1\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def _select_frames_to_save(is_washing, codes, movement0_prop=1.0):\n",
    "    old_code = -1\n",
    "    old_saved = False\n",
    "    num_snippets = 0\n",
    "    mapping = {}\n",
    "    current_snippet = {}\n",
    "    for i in range(len(is_washing)):\n",
    "        new_code = codes[i]\n",
    "        new_saved = (is_washing[i] == 2 and new_code != -1)\n",
    "        if new_saved != old_saved:\n",
    "            if new_saved:\n",
    "                num_snippets += 1\n",
    "                current_snippet = {}\n",
    "            else:\n",
    "                if old_code != 0 or np.random.rand() < movement0_prop:\n",
    "                    for key in current_snippet:\n",
    "                        mapping[key] = current_snippet[key]\n",
    "        if new_saved:\n",
    "            current_snippet_frame = len(current_snippet)\n",
    "            current_snippet[i] = (current_snippet_frame, num_snippets, new_code)\n",
    "        old_saved = new_saved\n",
    "        old_code = new_code\n",
    "    if old_saved:\n",
    "        if old_code != 0 or np.random.rand() < movement0_prop:\n",
    "            for key in current_snippet:\n",
    "                mapping[key] = current_snippet[key]\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def _find_annotations_dir(video_path: Path) -> Path | None:\n",
    "    for parent in video_path.parents:\n",
    "        ann_dir = parent / \"Annotations\"\n",
    "        if ann_dir.exists():\n",
    "            return ann_dir\n",
    "    return None\n",
    "\n",
    "\n",
    "def _load_frame_annotations(video_path: Path, annotator_prefix: str, total_annotators: int):\n",
    "    ann_dir = _find_annotations_dir(video_path)\n",
    "    if not ann_dir:\n",
    "        return [], 0\n",
    "    annotations = []\n",
    "    for a in range(1, total_annotators + 1):\n",
    "        annotator_dir = ann_dir / f\"{annotator_prefix}{a}\"\n",
    "        json_path = annotator_dir / f\"{video_path.stem}.json\"\n",
    "        if not json_path.exists():\n",
    "            continue\n",
    "        try:\n",
    "            with open(json_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            a_annotations = [(data['labels'][i]['is_washing'], data['labels'][i]['code']) for i in range(len(data['labels']))]\n",
    "            annotations.append(a_annotations)\n",
    "        except Exception as exc:\n",
    "            print(\"Failed to load\", json_path, exc)\n",
    "    return annotations, len(annotations)\n",
    "\n",
    "\n",
    "def _frame_labels_from_annotations(annotations, total_movements, reaction_frames):\n",
    "    num_annotators = len(annotations)\n",
    "    if num_annotators == 0:\n",
    "        return [], []\n",
    "    num_frames = len(annotations[0])\n",
    "    is_washing, codes = [], []\n",
    "    for frame_num in range(num_frames):\n",
    "        frame_annotations = [annotations[a][frame_num] for a in range(num_annotators)]\n",
    "        frame_is_washing_any = any(frame_annotations[a][0] for a in range(num_annotators))\n",
    "        frame_is_washing_all = all(frame_annotations[a][0] for a in range(num_annotators))\n",
    "        frame_codes = [frame_annotations[a][1] for a in range(num_annotators)]\n",
    "        frame_codes = [PSKUS_CODE_MAPPING.get(int(code), 0) for code in frame_codes]\n",
    "        if frame_is_washing_all:\n",
    "            frame_is_washing = 2\n",
    "        elif frame_is_washing_any:\n",
    "            frame_is_washing = 1\n",
    "        else:\n",
    "            frame_is_washing = 0\n",
    "        is_washing.append(frame_is_washing)\n",
    "        if frame_is_washing:\n",
    "            codes.append(_majority_vote(frame_codes, total_movements))\n",
    "        else:\n",
    "            codes.append(-1)\n",
    "    is_washing = _discount_reaction_indeterminacy(is_washing, reaction_frames)\n",
    "    codes = _discount_reaction_indeterminacy(codes, reaction_frames)\n",
    "    return is_washing, codes\n",
    "\n",
    "\n",
    "def _load_pskus_split(pskus_dir: Path):\n",
    "    csv_path = pskus_dir / \"statistics-with-locations.csv\"\n",
    "    if not csv_path.exists():\n",
    "        candidates = [\n",
    "            Path.cwd() / \"code/edgewash/dataset-pskus/statistics-with-locations.csv\",\n",
    "            Path.cwd() / \"edgeWash/code/edgewash/dataset-pskus/statistics-with-locations.csv\",\n",
    "        ]\n",
    "        for candidate in candidates:\n",
    "            if candidate.exists():\n",
    "                csv_path = candidate\n",
    "                print(\"Using fallback PSKUS split file:\", csv_path)\n",
    "                break\n",
    "    if not csv_path.exists():\n",
    "        print(\"PSKUS split CSV not found; will use random split later\")\n",
    "        return set(), set()\n",
    "    testfiles, trainvalfiles = set(), set()\n",
    "    try:\n",
    "        with open(csv_path, \"r\") as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            for row in reader:\n",
    "                if row and row[0] == \"filename\":\n",
    "                    continue\n",
    "                if not row:\n",
    "                    continue\n",
    "                filename = row[0]\n",
    "                location = row[1] if len(row) > 1 else \"\"\n",
    "                if location == \"Reanim\u0101cija\":\n",
    "                    testfiles.add(filename)\n",
    "                elif location != \"unknown\":\n",
    "                    trainvalfiles.add(filename)\n",
    "    except Exception as exc:\n",
    "        print(\"Failed to read PSKUS split CSV\", csv_path, exc)\n",
    "    return testfiles, trainvalfiles\n",
    "\n",
    "\n",
    "def extract_frames_from_video(video_path: Path, out_dir: Path, frame_skip: int) -> List[Dict]:\n",
    "    rows = []\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        return rows\n",
    "    base = video_path.stem\n",
    "    label = infer_label_from_path(video_path)\n",
    "    idx = 0\n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_idx % frame_skip == 0:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, IMG_SIZE)\n",
    "            out_path = out_dir / f\"{base}_{idx:06d}.jpg\"\n",
    "            cv2.imwrite(str(out_path), frame[:, :, ::-1])\n",
    "            rows.append({\"frame_path\": str(out_path), \"class_id\": label, \"video_id\": base, \"frame_idx\": idx})\n",
    "            idx += 1\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    return rows\n",
    "\n",
    "\n",
    "def preprocess_images(image_paths: List[Path], out_dir: Path) -> List[Dict]:\n",
    "    rows = []\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for img_path in tqdm(image_paths, desc=\"images\"):\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, IMG_SIZE)\n",
    "        label = infer_label_from_path(img_path)\n",
    "        out_path = out_dir / f\"{img_path.stem}.jpg\"\n",
    "        cv2.imwrite(str(out_path), img[:, :, ::-1])\n",
    "        rows.append({\"frame_path\": str(out_path), \"class_id\": label, \"video_id\": img_path.parent.name, \"frame_idx\": 0})\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _split_train_val_by_video(df, train_ratio=0.7, val_ratio=0.15):\n",
    "    unique_videos = df[\"video_id\"].unique()\n",
    "    video_to_class = df.groupby(\"video_id\")[\"class_id\"].first()\n",
    "    val_size = val_ratio / (train_ratio + val_ratio)\n",
    "    train_videos, val_videos = train_test_split(\n",
    "        unique_videos,\n",
    "        test_size=val_size,\n",
    "        random_state=42,\n",
    "        stratify=video_to_class[unique_videos],\n",
    "    )\n",
    "    train_df = df[df[\"video_id\"].isin(train_videos)].reset_index(drop=True)\n",
    "    val_df = df[df[\"video_id\"].isin(val_videos)].reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def split_and_save(df: pd.DataFrame, out_dir: Path) -> Dict[str, Path]:\n",
    "    if \"split\" in df.columns and df[\"split\"].notna().any():\n",
    "        test_df = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "        trainval_df = df[df[\"split\"] != \"test\"].reset_index(drop=True)\n",
    "        if not trainval_df.empty:\n",
    "            train_df, val_df = _split_train_val_by_video(trainval_df)\n",
    "        else:\n",
    "            train_df, val_df = df, df.iloc[0:0].copy()\n",
    "    else:\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        n = len(df)\n",
    "        train_end = int(0.7 * n)\n",
    "        val_end = int(0.85 * n)\n",
    "        train_df, val_df, test_df = df.iloc[:train_end], df.iloc[train_end:val_end], df.iloc[val_end:]\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_csv = out_dir / \"train.csv\"\n",
    "    val_csv = out_dir / \"val.csv\"\n",
    "    test_csv = out_dir / \"test.csv\"\n",
    "    train_df.to_csv(train_csv, index=False)\n",
    "    val_df.to_csv(val_csv, index=False)\n",
    "    test_df.to_csv(test_csv, index=False)\n",
    "    return {\"train\": train_csv, \"val\": val_csv, \"test\": test_csv}\n",
    "\n",
    "\n",
    "def preprocess_pskus_dataset(pskus_dir: Path, frames_root: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    testfiles, trainvalfiles = _load_pskus_split(pskus_dir)\n",
    "    has_split = bool(testfiles or trainvalfiles)\n",
    "    movement0_prop = 0.2\n",
    "    total_annotators = 8\n",
    "    total_movements = 8\n",
    "    fps = 30\n",
    "    reaction_frames = fps // 2\n",
    "\n",
    "    for video_path in pskus_dir.rglob(\"*.mp4\"):\n",
    "        filename = video_path.name\n",
    "        if has_split:\n",
    "            if filename in testfiles:\n",
    "                split = \"test\"\n",
    "            elif filename in trainvalfiles:\n",
    "                split = \"trainval\"\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            split = None\n",
    "\n",
    "        annotations, num_annotators = _load_frame_annotations(video_path, \"Annotator\", total_annotators)\n",
    "        if num_annotators <= 1:\n",
    "            continue\n",
    "        is_washing, codes = _frame_labels_from_annotations(annotations, total_movements, reaction_frames)\n",
    "        mapping = _select_frames_to_save(is_washing, codes, movement0_prop)\n",
    "        if not mapping:\n",
    "            continue\n",
    "        frames_dir = frames_root / (split or \"trainval\")\n",
    "        vidcap = cv2.VideoCapture(str(video_path))\n",
    "        is_success, image = vidcap.read()\n",
    "        frame_number = 0\n",
    "        while is_success:\n",
    "            if frame_number in mapping:\n",
    "                new_frame_num, snippet_num, code = mapping[frame_number]\n",
    "                out_sub = frames_dir / str(code)\n",
    "                out_sub.mkdir(parents=True, exist_ok=True)\n",
    "                filename_out = f\"frame_{new_frame_num}_snippet_{snippet_num}_{video_path.stem}.jpg\"\n",
    "                save_path = out_sub / filename_out\n",
    "                image_resized = cv2.resize(image, IMG_SIZE)\n",
    "                cv2.imwrite(str(save_path), image_resized)\n",
    "                row = {\n",
    "                    \"frame_path\": str(save_path),\n",
    "                    \"class_id\": int(code),\n",
    "                    \"video_id\": video_path.stem,\n",
    "                    \"frame_idx\": new_frame_num,\n",
    "                }\n",
    "                if split:\n",
    "                    row[\"split\"] = split\n",
    "                rows.append(row)\n",
    "            is_success, image = vidcap.read()\n",
    "            frame_number += 1\n",
    "        vidcap.release()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def preprocess_metc_dataset(metc_dir: Path, frames_root: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    total_annotators = 1\n",
    "    total_movements = 7\n",
    "    fps = 16\n",
    "    reaction_frames = fps // 2\n",
    "    test_proportion = 0.25\n",
    "    for video_path in metc_dir.rglob(\"*.mp4\"):\n",
    "        split = \"test\" if np.random.rand() < test_proportion else \"trainval\"\n",
    "        annotations, num_annotators = _load_frame_annotations(video_path, \"Annotator_\", total_annotators)\n",
    "        if num_annotators == 0:\n",
    "            continue\n",
    "        is_washing, codes = _frame_labels_from_annotations(annotations, total_movements, reaction_frames)\n",
    "        mapping = _select_frames_to_save(is_washing, codes, movement0_prop=1.0)\n",
    "        if not mapping:\n",
    "            continue\n",
    "        frames_dir = frames_root / split\n",
    "        vidcap = cv2.VideoCapture(str(video_path))\n",
    "        is_success, image = vidcap.read()\n",
    "        frame_number = 0\n",
    "        while is_success:\n",
    "            if frame_number in mapping:\n",
    "                new_frame_num, snippet_num, code = mapping[frame_number]\n",
    "                out_sub = frames_dir / str(code)\n",
    "                out_sub.mkdir(parents=True, exist_ok=True)\n",
    "                filename_out = f\"frame_{new_frame_num}_snippet_{snippet_num}_{video_path.stem}.jpg\"\n",
    "                save_path = out_sub / filename_out\n",
    "                image_resized = cv2.resize(image, IMG_SIZE)\n",
    "                cv2.imwrite(str(save_path), image_resized)\n",
    "                rows.append({\n",
    "                    \"frame_path\": str(save_path),\n",
    "                    \"class_id\": int(code),\n",
    "                    \"video_id\": video_path.stem,\n",
    "                    \"frame_idx\": new_frame_num,\n",
    "                    \"split\": split,\n",
    "                })\n",
    "            is_success, image = vidcap.read()\n",
    "            frame_number += 1\n",
    "        vidcap.release()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_synthetic_dataset(raw_dir: Path, frames_root: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    frames_root.mkdir(parents=True, exist_ok=True)\n",
    "    image_paths = [p for p in raw_dir.rglob(\"*.png\") if p.is_file()]\n",
    "    for img_path in tqdm(image_paths, desc=\"synthetic\"):\n",
    "        if \"rgb\" not in [part.lower() for part in img_path.parts]:\n",
    "            continue\n",
    "        class_id = infer_synthetic_class_id(img_path)\n",
    "        if class_id is None:\n",
    "            continue\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, IMG_SIZE)\n",
    "        video_id = synthetic_video_id(img_path)\n",
    "        frame_idx = parse_frame_idx(img_path)\n",
    "        out_sub = frames_root / str(class_id)\n",
    "        out_sub.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = out_sub / f\"{video_id}_{frame_idx:06d}.jpg\"\n",
    "        cv2.imwrite(str(out_path), img[:, :, ::-1])\n",
    "        rows.append({\n",
    "            \"frame_path\": str(out_path),\n",
    "            \"class_id\": int(class_id),\n",
    "            \"video_id\": video_id,\n",
    "            \"frame_idx\": int(frame_idx),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "\n",
    "def _is_archive(path: Path) -> bool:\n",
    "    name = path.name.lower()\n",
    "    return name.endswith((\".zip\", \".tar\", \".tar.gz\", \".tgz\", \".tar.bz2\", \".tar.xz\"))\n",
    "\n",
    "\n",
    "def _collect_archives(root: Path, max_hits: int = 5):\n",
    "    hits = []\n",
    "    if not root.exists():\n",
    "        return hits\n",
    "    for path in root.rglob(\"*\"):\n",
    "        if path.is_file() and _is_archive(path):\n",
    "            hits.append(path)\n",
    "            if len(hits) >= max_hits:\n",
    "                break\n",
    "    return hits\n",
    "\n",
    "\n",
    "def _iter_files(root: Path, exts, max_hits: int = 3):\n",
    "    hits = []\n",
    "    if not root.exists():\n",
    "        return hits\n",
    "    for path in root.rglob(\"*\"):\n",
    "        if path.is_file() and path.suffix.lower() in exts:\n",
    "            hits.append(path)\n",
    "            if len(hits) >= max_hits:\n",
    "                break\n",
    "    return hits\n",
    "\n",
    "\n",
    "def _find_pskus_split_csv(raw_dir: Path):\n",
    "    csv_path = raw_dir / \"statistics-with-locations.csv\"\n",
    "    if csv_path.exists():\n",
    "        return csv_path\n",
    "    candidates = [\n",
    "        Path.cwd() / \"code/edgewash/dataset-pskus/statistics-with-locations.csv\",\n",
    "        Path.cwd() / \"edgeWash/code/edgewash/dataset-pskus/statistics-with-locations.csv\",\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def validate_raw_dataset(name: str, raw_dir: Path, strict_archives: bool = True) -> None:\n",
    "    errors = []\n",
    "    if not raw_dir.exists():\n",
    "        errors.append(\"raw dir missing\")\n",
    "    elif name == \"kaggle\":\n",
    "        kaggle_root = raw_dir / \"kaggle-dataset-6classes\"\n",
    "        if not kaggle_root.exists():\n",
    "            errors.append(\"kaggle-dataset-6classes folder missing\")\n",
    "        if not _iter_files(kaggle_root, VIDEO_EXTS):\n",
    "            errors.append(\"no videos found under kaggle-dataset-6classes\")\n",
    "    elif name == \"pskus\":\n",
    "        dataset_dirs = [p for p in raw_dir.rglob(\"DataSet*\") if p.is_dir()]\n",
    "        if not dataset_dirs:\n",
    "            errors.append(\"no DataSet* folders found\")\n",
    "        if not _iter_files(raw_dir, VIDEO_EXTS):\n",
    "            errors.append(\"no videos found\")\n",
    "        if not _iter_files(raw_dir, (\".json\",), max_hits=1):\n",
    "            errors.append(\"no annotation JSON files found\")\n",
    "        if _find_pskus_split_csv(raw_dir) is None:\n",
    "            print(\"WARNING: statistics-with-locations.csv not found; will use random split.\")\n",
    "    elif name == \"metc\":\n",
    "        interface_dirs = [p for p in raw_dir.rglob(\"Interface_number_*\") if p.is_dir()]\n",
    "        if not interface_dirs:\n",
    "            errors.append(\"no Interface_number_* folders found\")\n",
    "        if not _iter_files(raw_dir, VIDEO_EXTS):\n",
    "            errors.append(\"no videos found\")\n",
    "        if not _iter_files(raw_dir, (\".json\",), max_hits=1):\n",
    "            errors.append(\"no annotation JSON files found\")\n",
    "    elif name == \"synthetic_blender_rozakar\":\n",
    "        pngs = _iter_files(raw_dir, (\".png\",), max_hits=3)\n",
    "        if not pngs:\n",
    "            errors.append(\"no PNG files found\")\n",
    "    else:\n",
    "        errors.append(\"unknown dataset name\")\n",
    "\n",
    "    archives = _collect_archives(raw_dir)\n",
    "    if archives:\n",
    "        msg = \"archive files still present: \" + \", \".join([p.name for p in archives])\n",
    "        if strict_archives:\n",
    "            errors.append(msg)\n",
    "        else:\n",
    "            print(\"WARN:\", msg)\n",
    "\n",
    "    if errors:\n",
    "        raise RuntimeError(f\"Raw dataset validation failed for {name}: \" + \"; \".join(errors))\n",
    "\n",
    "\n",
    "def validate_processed_dataset(out_dir: Path, max_rows: int = 20) -> None:\n",
    "    errors = []\n",
    "    for split in (\"train\", \"val\", \"test\"):\n",
    "        csv_path = out_dir / f\"{split}.csv\"\n",
    "        if not csv_path.exists():\n",
    "            errors.append(f\"missing {split}.csv\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path).head(max_rows)\n",
    "        if df.empty:\n",
    "            errors.append(f\"{split}.csv has no rows\")\n",
    "            continue\n",
    "        required = {\"frame_path\", \"class_id\", \"video_id\", \"frame_idx\"}\n",
    "        missing = required - set(df.columns)\n",
    "        if missing:\n",
    "            errors.append(f\"{split}.csv missing columns: {sorted(missing)}\")\n",
    "            continue\n",
    "        for row in df.itertuples():\n",
    "            frame_path = Path(row.frame_path)\n",
    "            if not frame_path.exists():\n",
    "                errors.append(f\"missing frame file: {frame_path}\")\n",
    "                break\n",
    "            if not (0 <= int(row.class_id) < NUM_CLASSES):\n",
    "                errors.append(f\"class_id out of range: {row.class_id}\")\n",
    "                break\n",
    "    if errors:\n",
    "        raise RuntimeError(\"Processed dataset validation failed: \" + \"; \".join(errors))\n",
    "\n",
    "def preprocess_dataset(name: str) -> Path:\n",
    "    raw_dir = RAW_DIR / name\n",
    "    out_dir = PROCESSED_DIR / name\n",
    "    frames_dir = out_dir / \"frames\"\n",
    "    frames_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if name == \"pskus\":\n",
    "        df = preprocess_pskus_dataset(raw_dir, frames_dir)\n",
    "    elif name == \"metc\":\n",
    "        df = preprocess_metc_dataset(raw_dir, frames_dir)\n",
    "    elif name == \"synthetic_blender_rozakar\":\n",
    "        df = preprocess_synthetic_dataset(raw_dir, frames_dir)\n",
    "    else:\n",
    "        video_files = [p for p in raw_dir.rglob(\"*\") if p.suffix.lower() in VIDEO_EXTS]\n",
    "        image_files = [p for p in raw_dir.rglob(\"*\") if p.suffix.lower() in IMAGE_EXTS]\n",
    "        rows = []\n",
    "        if video_files:\n",
    "            for vp in tqdm(video_files, desc=\"videos\"):\n",
    "                rows.extend(extract_frames_from_video(vp, frames_dir, FRAME_SKIP))\n",
    "        elif image_files:\n",
    "            rows.extend(preprocess_images(image_files, frames_dir))\n",
    "        else:\n",
    "            raise RuntimeError(\"No video or image files found in \" + str(raw_dir))\n",
    "        df = pd.DataFrame(rows)\n",
    "\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No frames extracted for \" + name)\n",
    "    split_and_save(df, out_dir)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def show_random_video(raw_dir: Path):\n",
    "    videos = [p for p in raw_dir.rglob(\"*\") if p.suffix.lower() in VIDEO_EXTS]\n",
    "    if not videos:\n",
    "        print(\"No videos found\")\n",
    "        return\n",
    "    print(\"Video sample:\", videos[0])\n",
    "    display(Video(str(videos[0]), embed=True))\n",
    "\n",
    "\n",
    "def show_random_samples(df: pd.DataFrame, title: str, n: int = 12):\n",
    "    import matplotlib.pyplot as plt\n",
    "    sample = df.sample(n, replace=True)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, row in enumerate(sample.itertuples(), 1):\n",
    "        img = cv2.imread(row.frame_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.subplot(3, 4, i)\n",
    "        plt.imshow(img)\n",
    "        label = CLASS_NAMES[int(row.class_id)] if int(row.class_id) < len(CLASS_NAMES) else str(row.class_id)\n",
    "        plt.title(label, fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_class_distribution(df: pd.DataFrame, label: str) -> None:\n",
    "    col = \"class_name\" if \"class_name\" in df.columns else \"class_id\"\n",
    "    counts = df[col].value_counts()\n",
    "    print(f\"{label} class distribution:\")\n",
    "    print(counts.to_string())\n",
    "    if len(counts) == 1:\n",
    "        print(\"WARNING: single-class dataset; check labeling.\")\n",
    "    if \"Other\" in counts.index:\n",
    "        if counts[\"Other\"] / len(df) > 0.95:\n",
    "            print(\"WARNING: 'Other' exceeds 95% of samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def random_shadow(img):\n",
    "    h, w = img.shape[:2]\n",
    "    x1, y1 = np.random.randint(0, w), 0\n",
    "    x2, y2 = np.random.randint(0, w), h\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [np.array([[x1, y1], [x2, y2], [0, h], [w, h]])], 255)\n",
    "    shadow = np.stack([mask] * 3, axis=-1)\n",
    "    alpha = np.random.uniform(0.5, 0.9)\n",
    "    return np.where(shadow > 0, (img * alpha).astype(np.uint8), img)\n",
    "\n",
    "\n",
    "def sample_aug_params():\n",
    "    hflip_enabled = any(\n",
    "        AUGMENT_CONFIG.get(k, False)\n",
    "        for k in (\"hflip\", \"mid_flip\", \"horizontal_flip\")\n",
    "    )\n",
    "    params = {\n",
    "        \"hflip\": hflip_enabled and random.random() < 0.5,\n",
    "        \"angle\": 0.0,\n",
    "        \"zoom\": 1.0,\n",
    "        \"shear\": 0.0,\n",
    "        \"tx\": 0,\n",
    "        \"ty\": 0,\n",
    "        \"brightness\": None,\n",
    "        \"contrast\": None,\n",
    "        \"gamma\": None,\n",
    "        \"shadow\": False,\n",
    "        \"reverse_sequence\": AUGMENT_CONFIG.get(\"reverse_sequence\", False) and random.random() < 0.5,\n",
    "    }\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"rotation\", 0) > 0:\n",
    "        params[\"angle\"] = random.uniform(-AUGMENT_CONFIG[\"rotation\"], AUGMENT_CONFIG[\"rotation\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"zoom\", 0) > 0:\n",
    "        params[\"zoom\"] = random.uniform(1 - AUGMENT_CONFIG[\"zoom\"], 1 + AUGMENT_CONFIG[\"zoom\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"shear\", 0) > 0:\n",
    "        params[\"shear\"] = random.uniform(-AUGMENT_CONFIG[\"shear\"], AUGMENT_CONFIG[\"shear\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"shift\", 0) > 0:\n",
    "        params[\"tx\"] = int(random.uniform(-AUGMENT_CONFIG[\"shift\"], AUGMENT_CONFIG[\"shift\"]) * IMG_SIZE[0])\n",
    "        params[\"ty\"] = int(random.uniform(-AUGMENT_CONFIG[\"shift\"], AUGMENT_CONFIG[\"shift\"]) * IMG_SIZE[1])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"brightness\"):\n",
    "        params[\"brightness\"] = random.uniform(*AUGMENT_CONFIG[\"brightness\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"contrast\"):\n",
    "        params[\"contrast\"] = random.uniform(*AUGMENT_CONFIG[\"contrast\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"gamma\"):\n",
    "        params[\"gamma\"] = random.uniform(*AUGMENT_CONFIG[\"gamma\"])\n",
    "\n",
    "    if AUGMENT_CONFIG.get(\"shadow\") and random.random() < 0.5:\n",
    "        params[\"shadow\"] = True\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def apply_aug(img, params):\n",
    "    if params.get(\"hflip\"):\n",
    "        img = cv2.flip(img, 1)\n",
    "\n",
    "    angle = params.get(\"angle\", 0.0)\n",
    "    if angle:\n",
    "        M = cv2.getRotationMatrix2D((IMG_SIZE[0] / 2, IMG_SIZE[1] / 2), angle, 1.0)\n",
    "        img = cv2.warpAffine(img, M, IMG_SIZE, borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    zoom = params.get(\"zoom\", 1.0)\n",
    "    if zoom != 1.0:\n",
    "        h, w = IMG_SIZE\n",
    "        img_resized = cv2.resize(img, (int(w * zoom), int(h * zoom)))\n",
    "        if zoom > 1:\n",
    "            startx = (img_resized.shape[1] - w) // 2\n",
    "            starty = (img_resized.shape[0] - h) // 2\n",
    "            img = img_resized[starty : starty + h, startx : startx + w]\n",
    "        else:\n",
    "            pad_x = (w - img_resized.shape[1]) // 2\n",
    "            pad_y = (h - img_resized.shape[0]) // 2\n",
    "            img = cv2.copyMakeBorder(\n",
    "                img_resized,\n",
    "                pad_y,\n",
    "                h - img_resized.shape[0] - pad_y,\n",
    "                pad_x,\n",
    "                w - img_resized.shape[1] - pad_x,\n",
    "                cv2.BORDER_REFLECT,\n",
    "            )\n",
    "\n",
    "    tx, ty = params.get(\"tx\", 0), params.get(\"ty\", 0)\n",
    "    if tx or ty:\n",
    "        M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        img = cv2.warpAffine(img, M, IMG_SIZE, borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    shear = params.get(\"shear\", 0.0)\n",
    "    if shear:\n",
    "        M = np.float32([[1, shear, 0], [0, 1, 0]])\n",
    "        img = cv2.warpAffine(img, M, IMG_SIZE, borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    brightness = params.get(\"brightness\")\n",
    "    if brightness is not None:\n",
    "        img = np.clip(img.astype(np.float32) * brightness, 0, 255).astype(np.uint8)\n",
    "\n",
    "    contrast = params.get(\"contrast\")\n",
    "    if contrast is not None:\n",
    "        img = np.clip(128 + contrast * (img.astype(np.float32) - 128), 0, 255).astype(np.uint8)\n",
    "\n",
    "    gamma = params.get(\"gamma\")\n",
    "    if gamma is not None:\n",
    "        img = np.clip(((img / 255.0) ** gamma) * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    if params.get(\"shadow\"):\n",
    "        img = random_shadow(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def show_augmented_samples(df: pd.DataFrame, n: int = 12):\n",
    "    import matplotlib.pyplot as plt\n",
    "    sample = df.sample(n, replace=True)\n",
    "    params_cache = {}\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, row in enumerate(sample.itertuples(), 1):\n",
    "        img = cv2.imread(row.frame_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        video_id = getattr(row, \"video_id\", None)\n",
    "        if CONSISTENT_VIDEO_AUG and video_id is not None:\n",
    "            params = params_cache.setdefault(video_id, sample_aug_params())\n",
    "        else:\n",
    "            params = sample_aug_params()\n",
    "        img = apply_aug(img, params)\n",
    "        plt.subplot(3, 4, i)\n",
    "        plt.imshow(img)\n",
    "        label = CLASS_NAMES[int(row.class_id)] if int(row.class_id) < len(CLASS_NAMES) else str(row.class_id)\n",
    "        plt.title(label, fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(\"Augmented samples\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class FrameGen(keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size, augment=False, augment_multiplier=1, shuffle=True, augment_prob=1.0):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augment\n",
    "        self.augment_multiplier = max(1, int(augment_multiplier))\n",
    "        self.shuffle = shuffle\n",
    "        self.augment_prob = max(0.0, min(1.0, float(augment_prob)))\n",
    "        self.consistent_video_aug = CONSISTENT_VIDEO_AUG and \"video_id\" in self.df.columns\n",
    "        self.video_aug_params = {}\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(math.floor(len(self.df) * self.augment_multiplier / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        if self.augment and self.consistent_video_aug:\n",
    "            self.video_aug_params = {\n",
    "                vid: sample_aug_params()\n",
    "                for vid in self.df[\"video_id\"].dropna().unique().tolist()\n",
    "            }\n",
    "\n",
    "    def _get_params(self, video_id=None):\n",
    "        if self.consistent_video_aug and video_id in self.video_aug_params:\n",
    "            return self.video_aug_params[video_id]\n",
    "        return sample_aug_params()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = np.random.choice(self.indices, size=self.batch_size, replace=True)\n",
    "        X = np.empty((self.batch_size, *IMG_SIZE, 3), np.float32)\n",
    "        y = np.empty((self.batch_size, NUM_CLASSES), np.float32)\n",
    "        for j, i in enumerate(ids):\n",
    "            row = self.df.iloc[i]\n",
    "            img = cv2.imread(row.frame_path)\n",
    "            if img is None:\n",
    "                img = np.zeros((*IMG_SIZE, 3), np.uint8)\n",
    "            else:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            do_aug = self.augment and (self.augment_prob >= 1.0 or random.random() < self.augment_prob)\n",
    "            if do_aug:\n",
    "                video_id = row.get(\"video_id\") if self.consistent_video_aug else None\n",
    "                params = self._get_params(video_id)\n",
    "                img = apply_aug(img, params)\n",
    "            X[j] = img.astype(np.float32) / 255.0\n",
    "            y[j] = keras.utils.to_categorical(int(row.class_id), NUM_CLASSES)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def build_sequences(df: pd.DataFrame, seq_len: int, stride: int, max_per_video: int):\n",
    "    sequences = []\n",
    "    for vid, group in df.groupby(\"video_id\"):\n",
    "        group = group.sort_values(\"frame_idx\")\n",
    "        frames = group[\"frame_path\"].tolist()\n",
    "        labels = group[\"class_id\"].tolist()\n",
    "        count = 0\n",
    "        for start in range(0, len(frames) - seq_len + 1, stride):\n",
    "            seq = frames[start : start + seq_len]\n",
    "            label = int(round(np.mean(labels[start : start + seq_len])))\n",
    "            sequences.append((seq, label, vid))\n",
    "            count += 1\n",
    "            if count >= max_per_video:\n",
    "                break\n",
    "    return sequences\n",
    "\n",
    "\n",
    "class SequenceGen(keras.utils.Sequence):\n",
    "    def __init__(self, sequences, batch_size, augment=False, augment_multiplier=1, shuffle=True, augment_prob=1.0):\n",
    "        self.sequences = sequences\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augment\n",
    "        self.augment_multiplier = max(1, int(augment_multiplier))\n",
    "        self.shuffle = shuffle\n",
    "        self.augment_prob = max(0.0, min(1.0, float(augment_prob)))\n",
    "        self.consistent_video_aug = CONSISTENT_VIDEO_AUG\n",
    "        self.video_aug_params = {}\n",
    "        self.indices = np.arange(len(self.sequences))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(math.floor(len(self.sequences) * self.augment_multiplier / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        if self.augment and self.consistent_video_aug:\n",
    "            video_ids = {seq[2] for seq in self.sequences}\n",
    "            self.video_aug_params = {vid: sample_aug_params() for vid in video_ids}\n",
    "\n",
    "    def _get_params(self, video_id=None):\n",
    "        if self.consistent_video_aug and video_id in self.video_aug_params:\n",
    "            return self.video_aug_params[video_id]\n",
    "        return sample_aug_params()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = np.random.choice(self.indices, size=self.batch_size, replace=True)\n",
    "        X = np.empty((self.batch_size, SEQUENCE_LENGTH, *IMG_SIZE, 3), np.float32)\n",
    "        y = np.empty((self.batch_size, NUM_CLASSES), np.float32)\n",
    "        for j, i in enumerate(ids):\n",
    "            seq_paths, label, video_id = self.sequences[i]\n",
    "            do_aug = self.augment and (self.augment_prob >= 1.0 or random.random() < self.augment_prob)\n",
    "            params = self._get_params(video_id) if do_aug else None\n",
    "            if do_aug and params.get(\"reverse_sequence\"):\n",
    "                seq_paths = list(reversed(seq_paths))\n",
    "            frames = []\n",
    "            for p in seq_paths:\n",
    "                img = cv2.imread(p)\n",
    "                if img is None:\n",
    "                    img = np.zeros((*IMG_SIZE, 3), np.uint8)\n",
    "                else:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                if do_aug:\n",
    "                    img = apply_aug(img, params)\n",
    "                frames.append(img.astype(np.float32) / 255.0)\n",
    "            X[j] = np.stack(frames, axis=0)\n",
    "            y[j] = keras.utils.to_categorical(label, NUM_CLASSES)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def offline_augment_train(train_df: pd.DataFrame, out_dir: Path) -> pd.DataFrame:\n",
    "    if AUGMENT_MULTIPLIER <= 1:\n",
    "        return train_df\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    rows = []\n",
    "    if CONSISTENT_VIDEO_AUG and \"video_id\" in train_df.columns:\n",
    "        groups = train_df.groupby(\"video_id\")\n",
    "    else:\n",
    "        groups = [(None, train_df)]\n",
    "    for video_id, group in tqdm(groups, desc=\"offline augment\"):\n",
    "        for row in group.itertuples():\n",
    "            rows.append({\n",
    "                \"frame_path\": row.frame_path,\n",
    "                \"class_id\": row.class_id,\n",
    "                \"video_id\": getattr(row, \"video_id\", None),\n",
    "                \"frame_idx\": getattr(row, \"frame_idx\", 0),\n",
    "            })\n",
    "        num_aug = min(AUGMENT_MULTIPLIER - 1, AUGMENT_MAX_PER_SAMPLE)\n",
    "        for k in range(num_aug):\n",
    "            params = sample_aug_params()\n",
    "            for row in group.itertuples():\n",
    "                img = cv2.imread(row.frame_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                aug = apply_aug(img, params)\n",
    "                vid_tag = str(video_id) if video_id is not None else \"sample\"\n",
    "                out_path = out_dir / f\"aug_{vid_tag}_{k}_{row.Index}.jpg\"\n",
    "                cv2.imwrite(str(out_path), aug[:, :, ::-1])\n",
    "                rows.append({\n",
    "                    \"frame_path\": str(out_path),\n",
    "                    \"class_id\": row.class_id,\n",
    "                    \"video_id\": getattr(row, \"video_id\", None),\n",
    "                    \"frame_idx\": getattr(row, \"frame_idx\", 0),\n",
    "                })\n",
    "    out_df = pd.DataFrame(rows).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "\n",
    "# Multi-GPU strategy\n",
    "_gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(_gpus) > 1:\n",
    "    STRATEGY = tf.distribute.MirroredStrategy()\n",
    "else:\n",
    "    STRATEGY = tf.distribute.get_strategy()\n",
    "NUM_REPLICAS = STRATEGY.num_replicas_in_sync\n",
    "print('Using strategy:', STRATEGY, 'replicas:', NUM_REPLICAS)\n",
    "\n",
    "# Auto batch sizing target\n",
    "TARGET_GPU_UTIL = 0.9\n",
    "FRAME_MB_ESTIMATE = 8.0\n",
    "SEQ_MB_ESTIMATE = 24.0\n",
    "MAX_BATCH_FRAME = 512\n",
    "MAX_BATCH_SEQ = 128\n",
    "\n",
    "\n",
    "def _gpu_total_mb():\n",
    "    try:\n",
    "        out = subprocess.check_output([\n",
    "            'nvidia-smi',\n",
    "            '--query-gpu=memory.total',\n",
    "            '--format=csv,noheader,nounits',\n",
    "        ])\n",
    "        return int(out.decode().strip().splitlines()[0])\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def _auto_batch(base, per_sample_mb, max_bs):\n",
    "    if not AUTO_TUNE_BATCH:\n",
    "        return base\n",
    "    total_mb = _gpu_total_mb()\n",
    "    if total_mb <= 0:\n",
    "        return base\n",
    "    target_mb = total_mb * TARGET_GPU_UTIL\n",
    "    bs = int(target_mb / per_sample_mb)\n",
    "    bs = max(base, min(max_bs, bs))\n",
    "    bs = bs * max(1, NUM_REPLICAS)\n",
    "    return bs\n",
    "\n",
    "\n",
    "FRAME_BATCH = _auto_batch(BATCH_MOBILENET, FRAME_MB_ESTIMATE, MAX_BATCH_FRAME)\n",
    "SEQ_BATCH = _auto_batch(BATCH_SEQUENCE, SEQ_MB_ESTIMATE, MAX_BATCH_SEQ)\n",
    "print('Auto batch sizes -> frame:', FRAME_BATCH, 'sequence:', SEQ_BATCH)\n",
    "\n",
    "\n",
    "def build_optimizer(lr: float, weight_decay: float):\n",
    "    name = OPTIMIZER_NAME.lower()\n",
    "    if name == \"adamw\":\n",
    "        return keras.optimizers.AdamW(learning_rate=lr, weight_decay=weight_decay)\n",
    "    if name == \"adam\":\n",
    "        return keras.optimizers.Adam(learning_rate=lr)\n",
    "    raise ValueError(f\"Unknown optimizer: {OPTIMIZER_NAME}\")\n",
    "\n",
    "\n",
    "def compile_model(model, lr: float, weight_decay: float):\n",
    "    loss = keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "    model.compile(\n",
    "        optimizer=build_optimizer(lr, weight_decay),\n",
    "        loss=loss,\n",
    "        metrics=[\"accuracy\", keras.metrics.TopKCategoricalAccuracy(k=2, name=\"top2_accuracy\")],\n",
    "    )\n",
    "\n",
    "\n",
    "def _apply_preprocess(inputs, preprocess_fn, name: str):\n",
    "    x = layers.Lambda(lambda t: t * 255.0, name=f\"{name}_rescale\")(inputs)\n",
    "    return layers.Lambda(preprocess_fn, name=f\"{name}_preprocess\")(x)\n",
    "\n",
    "\n",
    "def build_frame_model(backbone: str, lr: float, freeze_backbone: bool = True, return_backbone: bool = False):\n",
    "    inputs = keras.Input(shape=(*IMG_SIZE, 3), name=\"image_input\")\n",
    "    base = None\n",
    "\n",
    "    if backbone == \"mobilenetv2\":\n",
    "        x = _apply_preprocess(inputs, keras.applications.mobilenet_v2.preprocess_input, \"mobilenetv2\")\n",
    "        base = keras.applications.MobileNetV2(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    elif backbone == \"resnet50\":\n",
    "        x = _apply_preprocess(inputs, keras.applications.resnet.preprocess_input, \"resnet50\")\n",
    "        base = keras.applications.ResNet50(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    elif backbone == \"resnet101\":\n",
    "        x = _apply_preprocess(inputs, keras.applications.resnet.preprocess_input, \"resnet101\")\n",
    "        base = keras.applications.ResNet101(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    elif backbone == \"resnet152\":\n",
    "        x = _apply_preprocess(inputs, keras.applications.resnet.preprocess_input, \"resnet152\")\n",
    "        base = keras.applications.ResNet152(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    elif backbone == \"efficientnetb0\":\n",
    "        x = _apply_preprocess(inputs, keras.applications.efficientnet.preprocess_input, \"efficientnetb0\")\n",
    "        base = keras.applications.EfficientNetB0(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    elif backbone == \"efficientnetb3\":\n",
    "        x = _apply_preprocess(inputs, keras.applications.efficientnet.preprocess_input, \"efficientnetb3\")\n",
    "        base = keras.applications.EfficientNetB3(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    elif backbone == \"efficientnetv2b0\":\n",
    "        x = _apply_preprocess(inputs, keras.applications.efficientnet_v2.preprocess_input, \"efficientnetv2b0\")\n",
    "        base = keras.applications.EfficientNetV2B0(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    elif backbone == \"convnext_tiny\":\n",
    "        x = _apply_preprocess(inputs, keras.applications.convnext.preprocess_input, \"convnext_tiny\")\n",
    "        base = keras.applications.ConvNeXtTiny(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    elif backbone == \"vit_b16\":\n",
    "        try:\n",
    "            import keras_cv\n",
    "        except Exception as exc:\n",
    "            raise RuntimeError(\"vit_b16 requires keras-cv (pip install keras-cv)\") from exc\n",
    "        model = keras_cv.models.ViTClassifier(\n",
    "            input_shape=(*IMG_SIZE, 3),\n",
    "            num_classes=NUM_CLASSES,\n",
    "            activation=\"softmax\",\n",
    "            include_rescaling=True,\n",
    "            pretrained=\"imagenet21k+imagenet2012\",\n",
    "        )\n",
    "        compile_model(model, lr, WEIGHT_DECAY)\n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "\n",
    "    base.trainable = not freeze_backbone\n",
    "    x = base(x, training=False)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, out)\n",
    "    compile_model(model, lr, WEIGHT_DECAY)\n",
    "\n",
    "    if return_backbone:\n",
    "        return model, base\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_temporal_model(kind: str, lr: float, freeze_backbone: bool = True):\n",
    "    frame_in = keras.Input(shape=(*IMG_SIZE, 3))\n",
    "    frame_pre = _apply_preprocess(frame_in, keras.applications.mobilenet_v2.preprocess_input, \"mobilenetv2\")\n",
    "    base = keras.applications.MobileNetV2(include_top=False, input_shape=(*IMG_SIZE, 3), pooling=\"avg\")\n",
    "    base.trainable = not freeze_backbone\n",
    "    feat = base(frame_pre, training=False)\n",
    "    encoder = keras.Model(frame_in, feat)\n",
    "\n",
    "    inp = keras.Input(shape=(SEQUENCE_LENGTH, *IMG_SIZE, 3))\n",
    "    x = layers.TimeDistributed(encoder)(inp)\n",
    "    if kind == \"lstm\":\n",
    "        x = layers.LSTM(128)(x)\n",
    "    else:\n",
    "        x = layers.GRU(128)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    compile_model(model, lr, WEIGHT_DECAY)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_3d_cnn(lr: float):\n",
    "    inp = keras.Input(shape=(SEQUENCE_LENGTH, *IMG_SIZE, 3))\n",
    "    x = layers.Conv3D(32, (3, 3, 3), padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.MaxPool3D((1, 2, 2))(x)\n",
    "    x = layers.Conv3D(64, (3, 3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool3D((2, 2, 2))(x)\n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    compile_model(model, lr, WEIGHT_DECAY)\n",
    "    return model\n",
    "\n",
    "\n",
    "class LivePlot(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.history = {\"loss\": [], \"val_loss\": [], \"accuracy\": [], \"val_accuracy\": []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        for k in self.history:\n",
    "            if k in logs:\n",
    "                self.history[k].append(logs[k])\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        ax[0].plot(self.history[\"loss\"], label=\"loss\")\n",
    "        ax[0].plot(self.history[\"val_loss\"], label=\"val_loss\")\n",
    "        ax[0].legend(); ax[0].set_title(\"Loss\")\n",
    "        ax[1].plot(self.history[\"accuracy\"], label=\"acc\")\n",
    "        ax[1].plot(self.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "        ax[1].legend(); ax[1].set_title(\"Accuracy\")\n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "\n",
    "\n",
    "def cleanup_old_checkpoints(base_dir: Path, keep: int = 3):\n",
    "    if not base_dir.exists():\n",
    "        return\n",
    "    dirs = sorted([p for p in base_dir.iterdir() if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    for d in dirs[keep:]:\n",
    "        shutil.rmtree(d, ignore_errors=True)\n",
    "\n",
    "\n",
    "def _confusion_matrix_plot(y_true, y_pred, title: str):\n",
    "    if not SHOW_CONFUSION_MATRICES:\n",
    "        return\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(NUM_CLASSES)))\n",
    "    if CONFUSION_NORMALIZE:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=False, cmap=\"Blues\", xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Pred\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _eval_and_confusion(model, test_gen, title: str):\n",
    "    preds = model.predict(test_gen, verbose=0)\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "    y_true = []\n",
    "    for i in range(len(test_gen)):\n",
    "        _, batch_y = test_gen[i]\n",
    "        y_true.extend(np.argmax(batch_y, axis=1))\n",
    "    y_true = np.array(y_true[: len(y_pred)])\n",
    "    _confusion_matrix_plot(y_true, y_pred, title)\n",
    "\n",
    "\n",
    "def _get_aug_prob():\n",
    "    if AUGMENT_PROB is None:\n",
    "        if AUGMENT_MULTIPLIER <= 1:\n",
    "            return 0.0\n",
    "        return (AUGMENT_MULTIPLIER - 1) / float(AUGMENT_MULTIPLIER)\n",
    "    try:\n",
    "        return max(0.0, min(1.0, float(AUGMENT_PROB)))\n",
    "    except (TypeError, ValueError):\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def _get_aug_settings():\n",
    "    train_mult = AUGMENT_MULTIPLIER if USE_ON_THE_FLY_AUGMENT else 1\n",
    "    train_prob = _get_aug_prob() if USE_ON_THE_FLY_AUGMENT else 0.0\n",
    "    mix_prob = _get_aug_prob() if (USE_ON_THE_FLY_AUGMENT or USE_OFFLINE_AUGMENT) else 0.0\n",
    "    return train_mult, train_prob, mix_prob\n",
    "\n",
    "\n",
    "def _resolve_resume_path(model_name: str):\n",
    "    if not RESUME_MODEL_PATHS:\n",
    "        return None\n",
    "    return RESUME_MODEL_PATHS.get(model_name)\n",
    "\n",
    "class TestEvalCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_gen, test_mix_gen, label):\n",
    "        super().__init__()\n",
    "        self.test_gen = test_gen\n",
    "        self.test_mix_gen = test_mix_gen\n",
    "        self.label = label\n",
    "\n",
    "    def _run_eval(self, gen, title, epoch):\n",
    "        results = self.model.evaluate(gen, verbose=0)\n",
    "        metrics = dict(zip(self.model.metrics_names, results))\n",
    "        print(f\"[Epoch {epoch + 1}] {title} metrics: {metrics}\")\n",
    "        _eval_and_confusion(self.model, gen, title)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if not EVAL_TEST_EACH_EPOCH:\n",
    "            return\n",
    "        if self.test_gen is not None:\n",
    "            self._run_eval(self.test_gen, f\"{self.label} test\", epoch)\n",
    "        if self.test_mix_gen is not None:\n",
    "            self._run_eval(self.test_mix_gen, f\"{self.label} test+aug\", epoch)\n",
    "\n",
    "\n",
    "def _set_resnet_trainable(base_model, train_conv4: bool, train_conv5: bool):\n",
    "    for layer in base_model.layers:\n",
    "        if layer.name.startswith(\"conv5_\"):\n",
    "            layer.trainable = train_conv5\n",
    "        elif layer.name.startswith(\"conv4_\"):\n",
    "            layer.trainable = train_conv4\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "\n",
    "def _fit_with_batch(model_builder, train_gen_fn, val_gen_fn, batch_size, callbacks, test_gen_fn=None, test_mix_gen_fn=None, eval_label=None):\n",
    "    bs = batch_size\n",
    "    while bs >= 8:\n",
    "        try:\n",
    "            tf.keras.backend.clear_session()\n",
    "            with STRATEGY.scope():\n",
    "                model = model_builder()\n",
    "            train_gen = train_gen_fn(bs)\n",
    "            val_gen = val_gen_fn(bs)\n",
    "            callbacks_run = list(callbacks)\n",
    "            if EVAL_TEST_EACH_EPOCH and test_gen_fn is not None:\n",
    "                test_gen = test_gen_fn(bs)\n",
    "                test_mix_gen = test_mix_gen_fn(bs) if test_mix_gen_fn else None\n",
    "                callbacks_run.append(TestEvalCallback(test_gen, test_mix_gen, eval_label or \"model\"))\n",
    "            model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, callbacks=callbacks_run, verbose=1)\n",
    "            return model, bs\n",
    "        except tf.errors.ResourceExhaustedError:\n",
    "            print('OOM at batch', bs, 'retrying with smaller batch')\n",
    "            bs = bs // 2\n",
    "    raise RuntimeError('No viable batch size')\n",
    "\n",
    "def _train_resnet50_schedule_with_bs(train_df, val_df, test_df, bs, label):\n",
    "    train_mult, train_prob, mix_prob = _get_aug_settings()\n",
    "    test_gen = FrameGen(test_df, bs, augment=False, augment_multiplier=1)\n",
    "    test_mix_gen = None\n",
    "    if mix_prob > 0:\n",
    "        test_mix_gen = FrameGen(test_df, bs, augment=True, augment_multiplier=1, augment_prob=mix_prob)\n",
    "\n",
    "    with STRATEGY.scope():\n",
    "        model, backbone = build_frame_model(\"resnet50\", RESNET50_STAGE0_LR, freeze_backbone=True, return_backbone=True)\n",
    "\n",
    "    _set_resnet_trainable(backbone, train_conv4=False, train_conv5=False)\n",
    "    compile_model(model, RESNET50_STAGE0_LR, RESNET50_STAGE0_WD)\n",
    "    run_dir = CKPT_DIR / \"resnet50\" / str(int(time.time()))\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(str(run_dir / \"best.keras\"), save_best_only=True, monitor=\"val_accuracy\", mode=\"max\"),\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True),\n",
    "        keras.callbacks.TensorBoard(log_dir=str(LOGS_DIR / f\"resnet50_stage0_{int(time.time())}\")),\n",
    "        LivePlot(),\n",
    "    ]\n",
    "    if EVAL_TEST_EACH_EPOCH:\n",
    "        callbacks.append(TestEvalCallback(test_gen, test_mix_gen, f\"{label} stage0\"))\n",
    "    train_gen = FrameGen(train_df, bs, augment=USE_ON_THE_FLY_AUGMENT, augment_multiplier=train_mult, augment_prob=train_prob)\n",
    "    val_gen = FrameGen(val_df, bs, augment=False, augment_multiplier=1)\n",
    "    model.fit(train_gen, validation_data=val_gen, epochs=RESNET50_STAGE0_EPOCHS, callbacks=callbacks, verbose=1)\n",
    "\n",
    "    _set_resnet_trainable(backbone, train_conv4=True, train_conv5=True)\n",
    "    steps = max(1, len(train_gen) * RESNET50_STAGE1_EPOCHS)\n",
    "    schedule = keras.optimizers.schedules.CosineDecay(RESNET50_STAGE1_LR, decay_steps=steps)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.AdamW(learning_rate=schedule, weight_decay=RESNET50_STAGE1_WD),\n",
    "        loss=keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),\n",
    "        metrics=[\"accuracy\", keras.metrics.TopKCategoricalAccuracy(k=2, name=\"top2_accuracy\")],\n",
    "    )\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(str(run_dir / \"best_stage1.keras\"), save_best_only=True, monitor=\"val_accuracy\", mode=\"max\"),\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True),\n",
    "        keras.callbacks.TensorBoard(log_dir=str(LOGS_DIR / f\"resnet50_stage1_{int(time.time())}\")),\n",
    "        LivePlot(),\n",
    "    ]\n",
    "    if EVAL_TEST_EACH_EPOCH:\n",
    "        callbacks.append(TestEvalCallback(test_gen, test_mix_gen, f\"{label} stage1\"))\n",
    "    model.fit(train_gen, validation_data=val_gen, epochs=RESNET50_STAGE1_EPOCHS, callbacks=callbacks, verbose=1)\n",
    "\n",
    "    for layer in backbone.layers:\n",
    "        layer.trainable = True\n",
    "    compile_model(model, RESNET50_STAGE2_LR, RESNET50_STAGE2_WD)\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(str(run_dir / \"best_stage2.keras\"), save_best_only=True, monitor=\"val_accuracy\", mode=\"max\"),\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.5, min_lr=1e-6),\n",
    "        keras.callbacks.TensorBoard(log_dir=str(LOGS_DIR / f\"resnet50_stage2_{int(time.time())}\")),\n",
    "        LivePlot(),\n",
    "    ]\n",
    "    if EVAL_TEST_EACH_EPOCH:\n",
    "        callbacks.append(TestEvalCallback(test_gen, test_mix_gen, f\"{label} stage2\"))\n",
    "    model.fit(train_gen, validation_data=val_gen, epochs=RESNET50_STAGE2_EPOCHS, callbacks=callbacks, verbose=1)\n",
    "    return model\n",
    "\n",
    "def train_resnet50_schedule(train_df, val_df, test_df, label):\n",
    "    bs = FRAME_BATCH\n",
    "    while bs >= 8:\n",
    "        try:\n",
    "            model = _train_resnet50_schedule_with_bs(train_df, val_df, test_df, bs, label)\n",
    "            return model, bs\n",
    "        except tf.errors.ResourceExhaustedError:\n",
    "            print('OOM during schedule at batch', bs, 'retrying with smaller batch')\n",
    "            bs = bs // 2\n",
    "    raise RuntimeError('No viable batch size for resnet50 schedule')\n",
    "\n",
    "def train_and_eval_frame(model_name, train_df, val_df, test_df, dataset_name):\n",
    "    train_mult, train_prob, mix_prob = _get_aug_settings()\n",
    "    resume_path = _resolve_resume_path(model_name)\n",
    "    if resume_path is not None and not Path(resume_path).exists():\n",
    "        print(\"Resume path not found:\", resume_path)\n",
    "        resume_path = None\n",
    "    if resume_path:\n",
    "        print(f\"Resuming {model_name} from {resume_path}\")\n",
    "\n",
    "    if model_name == \"resnet50\" and RESNET50_SCHEDULE and not resume_path:\n",
    "        model, used_bs = train_resnet50_schedule(train_df, val_df, test_df, f\"{dataset_name} {model_name}\")\n",
    "    else:\n",
    "        run_dir = CKPT_DIR / dataset_name / model_name / str(int(time.time()))\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        ckpt_path = run_dir / 'best.keras'\n",
    "        callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(str(ckpt_path), save_best_only=True, monitor='val_accuracy', mode='max'),\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-6),\n",
    "            keras.callbacks.TensorBoard(log_dir=str(LOGS_DIR / f\"{dataset_name}_{model_name}_{int(time.time())}\")),\n",
    "            LivePlot(),\n",
    "        ]\n",
    "        def model_builder():\n",
    "            if resume_path:\n",
    "                model = keras.models.load_model(resume_path)\n",
    "                if RECOMPILE_ON_RESUME:\n",
    "                    compile_model(model, LR, WEIGHT_DECAY)\n",
    "                return model\n",
    "            return build_frame_model(model_name, LR)\n",
    "        def train_gen_fn(bs):\n",
    "            return FrameGen(train_df, bs, augment=USE_ON_THE_FLY_AUGMENT, augment_multiplier=train_mult, augment_prob=train_prob)\n",
    "        def val_gen_fn(bs):\n",
    "            return FrameGen(val_df, bs, augment=False, augment_multiplier=1)\n",
    "        def test_gen_fn(bs):\n",
    "            return FrameGen(test_df, bs, augment=False, augment_multiplier=1)\n",
    "        def test_mix_gen_fn(bs):\n",
    "            if mix_prob <= 0:\n",
    "                return None\n",
    "            return FrameGen(test_df, bs, augment=True, augment_multiplier=1, augment_prob=mix_prob)\n",
    "\n",
    "        model, used_bs = _fit_with_batch(\n",
    "            model_builder,\n",
    "            train_gen_fn,\n",
    "            val_gen_fn,\n",
    "            FRAME_BATCH,\n",
    "            callbacks,\n",
    "            test_gen_fn=test_gen_fn,\n",
    "            test_mix_gen_fn=test_mix_gen_fn,\n",
    "            eval_label=f\"{dataset_name} {model_name}\",\n",
    "        )\n",
    "\n",
    "    model_dir = MODELS_DIR / dataset_name\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    final_path = model_dir / f\"{model_name}_final.keras\"\n",
    "    model.save(final_path)\n",
    "\n",
    "    test_gen = FrameGen(test_df, used_bs, augment=False, augment_multiplier=1)\n",
    "    loss, acc = model.evaluate(test_gen, verbose=1)\n",
    "    print(model_name, 'test acc', acc)\n",
    "    _eval_and_confusion(model, test_gen, f\"{dataset_name} {model_name} (frame)\")\n",
    "    if mix_prob > 0:\n",
    "        test_mix_gen = FrameGen(test_df, used_bs, augment=True, augment_multiplier=1, augment_prob=mix_prob)\n",
    "        loss, acc = model.evaluate(test_mix_gen, verbose=1)\n",
    "        print(model_name, 'test+aug acc', acc)\n",
    "        _eval_and_confusion(model, test_mix_gen, f\"{dataset_name} {model_name} (frame+aug)\")\n",
    "\n",
    "    cleanup_old_checkpoints(CKPT_DIR / dataset_name / model_name, keep=3)\n",
    "    return final_path\n",
    "\n",
    "def train_and_eval_sequence(model_name, train_df, val_df, test_df, dataset_name):\n",
    "    sequences_train = build_sequences(train_df, SEQUENCE_LENGTH, SEQUENCE_STRIDE, MAX_SEQUENCES_PER_VIDEO)\n",
    "    sequences_val = build_sequences(val_df, SEQUENCE_LENGTH, SEQUENCE_STRIDE, MAX_SEQUENCES_PER_VIDEO)\n",
    "    sequences_test = build_sequences(test_df, SEQUENCE_LENGTH, SEQUENCE_STRIDE, MAX_SEQUENCES_PER_VIDEO)\n",
    "    if not sequences_train:\n",
    "        print('No sequences for', dataset_name, 'skipping', model_name)\n",
    "        return None\n",
    "\n",
    "    train_mult, train_prob, mix_prob = _get_aug_settings()\n",
    "\n",
    "    resume_path = _resolve_resume_path(model_name)\n",
    "    if resume_path is not None and not Path(resume_path).exists():\n",
    "        print(\"Resume path not found:\", resume_path)\n",
    "        resume_path = None\n",
    "    if resume_path:\n",
    "        print(f\"Resuming {model_name} from {resume_path}\")\n",
    "\n",
    "    run_dir = CKPT_DIR / dataset_name / model_name / str(int(time.time()))\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_path = run_dir / 'best.keras'\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(str(ckpt_path), save_best_only=True, monitor='val_accuracy', mode='max'),\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-6),\n",
    "        keras.callbacks.TensorBoard(log_dir=str(LOGS_DIR / f\"{dataset_name}_{model_name}_{int(time.time())}\")),\n",
    "        LivePlot(),\n",
    "    ]\n",
    "\n",
    "    def model_builder():\n",
    "        if resume_path:\n",
    "            model = keras.models.load_model(resume_path)\n",
    "            if RECOMPILE_ON_RESUME:\n",
    "                compile_model(model, LR, WEIGHT_DECAY)\n",
    "            return model\n",
    "        if model_name in ['lstm', 'gru']:\n",
    "            return build_temporal_model(model_name, LR)\n",
    "        return build_3d_cnn(LR)\n",
    "\n",
    "    def train_gen_fn(bs):\n",
    "        return SequenceGen(sequences_train, bs, augment=USE_ON_THE_FLY_AUGMENT, augment_multiplier=train_mult, augment_prob=train_prob)\n",
    "\n",
    "    def val_gen_fn(bs):\n",
    "        return SequenceGen(sequences_val, bs, augment=False, augment_multiplier=1)\n",
    "\n",
    "    def test_gen_fn(bs):\n",
    "        return SequenceGen(sequences_test, bs, augment=False, augment_multiplier=1)\n",
    "\n",
    "    def test_mix_gen_fn(bs):\n",
    "        if mix_prob <= 0:\n",
    "            return None\n",
    "        return SequenceGen(sequences_test, bs, augment=True, augment_multiplier=1, augment_prob=mix_prob)\n",
    "\n",
    "    model, used_bs = _fit_with_batch(\n",
    "        model_builder,\n",
    "        train_gen_fn,\n",
    "        val_gen_fn,\n",
    "        SEQ_BATCH,\n",
    "        callbacks,\n",
    "        test_gen_fn=test_gen_fn,\n",
    "        test_mix_gen_fn=test_mix_gen_fn,\n",
    "        eval_label=f\"{dataset_name} {model_name}\",\n",
    "    )\n",
    "\n",
    "    model_dir = MODELS_DIR / dataset_name\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    final_path = model_dir / f\"{model_name}_final.keras\"\n",
    "    model.save(final_path)\n",
    "\n",
    "    test_gen = SequenceGen(sequences_test, used_bs, augment=False, augment_multiplier=1)\n",
    "    loss, acc = model.evaluate(test_gen, verbose=1)\n",
    "    print(model_name, 'test acc', acc)\n",
    "    _eval_and_confusion(model, test_gen, f\"{dataset_name} {model_name} (sequence)\")\n",
    "    if mix_prob > 0:\n",
    "        test_mix_gen = SequenceGen(sequences_test, used_bs, augment=True, augment_multiplier=1, augment_prob=mix_prob)\n",
    "        loss, acc = model.evaluate(test_mix_gen, verbose=1)\n",
    "        print(model_name, 'test+aug acc', acc)\n",
    "        _eval_and_confusion(model, test_mix_gen, f\"{dataset_name} {model_name} (sequence+aug)\")\n",
    "\n",
    "    cleanup_old_checkpoints(CKPT_DIR / dataset_name / model_name, keep=3)\n",
    "    return final_path\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def cleanup_dataset_files(dataset_name: str, train_df: pd.DataFrame):\n",
    "    if CLEANUP_TRAIN:\n",
    "        for p in train_df['frame_path'].tolist():\n",
    "            try:\n",
    "                os.remove(p)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        train_csv = PROCESSED_DIR / dataset_name / 'train.csv'\n",
    "        train_aug = PROCESSED_DIR / dataset_name / 'train_aug.csv'\n",
    "        train_csv.unlink(missing_ok=True)\n",
    "        train_aug.unlink(missing_ok=True)\n",
    "    if CLEANUP_RAW:\n",
    "        shutil.rmtree(RAW_DIR / dataset_name, ignore_errors=True)\n",
    "\n",
    "\n",
    "def process_dataset(name: str):\n",
    "    ensure_dataset(name)\n",
    "    raw_dir = RAW_DIR / name\n",
    "    validate_raw_dataset(name, raw_dir)\n",
    "    show_random_video(raw_dir)\n",
    "\n",
    "    out_dir = PROCESSED_DIR / name\n",
    "    train_csv = out_dir / 'train.csv'\n",
    "    val_csv = out_dir / 'val.csv'\n",
    "    test_csv = out_dir / 'test.csv'\n",
    "    if not (train_csv.exists() and val_csv.exists() and test_csv.exists()):\n",
    "        preprocess_dataset(name)\n",
    "    validate_processed_dataset(out_dir)\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    val_df = pd.read_csv(val_csv)\n",
    "    test_df = pd.read_csv(test_csv)\n",
    "\n",
    "    show_random_samples(train_df, title=f\"{name} samples\")\n",
    "    show_augmented_samples(train_df, n=12)\n",
    "\n",
    "    if USE_OFFLINE_AUGMENT:\n",
    "        aug_dir = out_dir / 'augmented'\n",
    "        train_df = offline_augment_train(train_df, aug_dir)\n",
    "        train_df.to_csv(out_dir / 'train_aug.csv', index=False)\n",
    "\n",
    "    for model_name in MODELS:\n",
    "        if model_name in AVAILABLE_FRAME_MODELS:\n",
    "            train_and_eval_frame(model_name, train_df, val_df, test_df, name)\n",
    "        elif model_name in AVAILABLE_SEQUENCE_MODELS:\n",
    "            train_and_eval_sequence(model_name, train_df, val_df, test_df, name)\n",
    "        else:\n",
    "            print('Unknown model', model_name)\n",
    "\n",
    "    cleanup_dataset_files(name, train_df)\n",
    "\n",
    "\n",
    "print('Starting training loop...')\n",
    "for ds in DATASETS:\n",
    "    process_dataset(ds)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}