{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwash Training (Colab-friendly)\n",
    "\n",
    "End-to-end pipeline in notebook form. Mirrors `scripts/run_colab_pipeline.sh` but lets you inspect and tweak each stage, preview augmentations, and monitor TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Dependencies (TensorFlow is preinstalled on Colab)\n",
    "!pip install -q --no-cache-dir scikit-learn pandas numpy opencv-python-headless matplotlib seaborn tqdm requests nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Paths, Drive, and imports\n",
    "import os, sys, pathlib, json, shutil, time\n",
    "from typing import List\n",
    "\n",
    "from google.colab import drive, output\n",
    "\n",
    "# Adjust these if you cloned elsewhere\n",
    "PROJECT_ROOT = pathlib.Path('/content/edgeWash').resolve()\n",
    "DATA_DIR = pathlib.Path(os.environ.get('DATA_DIR', '/content/handwash_data')).resolve()\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "CHECKPOINTS_DIR = PROJECT_ROOT / 'checkpoints'\n",
    "LOGS_DIR = PROJECT_ROOT / 'logs'\n",
    "\n",
    "# Mount Drive to save checkpoints/logs there (optional)\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Make training modules importable\n",
    "sys.path.insert(0, str(PROJECT_ROOT / 'training'))\n",
    "os.environ['PYTHONPATH'] = str(PROJECT_ROOT / 'training') + ':' + os.environ.get('PYTHONPATH', '')\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import download_datasets\n",
    "import preprocess_data\n",
    "import train\n",
    "import evaluate\n",
    "from config import AUGMENT_MULTIPLIER, SEQUENCE_LENGTH, IMG_SIZE\n",
    "from data_generators import create_frame_generators, create_sequence_generators\n",
    "\n",
    "PROJECT_ROOT, DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Run configuration\n",
    "Tweak batches to better load the GPU; increase until you approach GPU RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['kaggle', 'pskus', 'metc', 'synthetic_blender_rozakar']\n",
    "MODELS = ['mobilenetv2', 'resnet50', 'efficientnetb0', 'lstm', 'gru', '3d_cnn']\n",
    "EPOCHS = 20\n",
    "BATCH_MOBILENET = 128\n",
    "BATCH_SEQUENCE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "AUGMENT_MULT = 5  # override if needed\n",
    "USE_EXISTING_PROCESSED = False\n",
    "\n",
    "# Ensure dirs\n",
    "for p in [DATA_DIR, RAW_DIR, PROCESSED_DIR, MODELS_DIR, CHECKPOINTS_DIR, LOGS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    'datasets': DATASETS,\n",
    "    'models': MODELS,\n",
    "    'epochs': EPOCHS,\n",
    "    'batch_mobilenet': BATCH_MOBILENET,\n",
    "    'batch_sequence': BATCH_SEQUENCE,\n",
    "    'augment_multiplier': AUGMENT_MULT,\n",
    "    'lr': LEARNING_RATE,\n",
    "    'data_dir': str(DATA_DIR)\n",
    "}\n",
    "json.dumps(config, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) TensorBoard (start first)\n",
    "Run the cell, then open the window below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, signal\n",
    "tb_proc = subprocess.Popen([\n",
    "    'tensorboard', '--logdir', str(LOGS_DIR), '--host', '0.0.0.0', '--port', '6006', '--load_fast=false'\n",
    "], stdout=open(LOGS_DIR / 'tensorboard.out', 'w'), stderr=subprocess.STDOUT)\n",
    "output.serve_kernel_port_as_window(6006)\n",
    "print('TensorBoard PID', tb_proc.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Download datasets (skips if already present)\n",
    "Progress bars and warnings appear in the cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaders = {\n",
    "    'kaggle': download_datasets.download_kaggle_dataset,\n",
    "    'pskus': download_datasets.download_pskus_dataset,\n",
    "    'metc': download_datasets.download_metc_dataset,\n",
    "    'synthetic_blender_rozakar': download_datasets.download_synthetic_blender_rozakar,\n",
    "}\n",
    "\n",
    "for name in DATASETS:\n",
    "    print(f\"\\n=== {name}: download ===\")\n",
    "    ok = downloaders[name]()\n",
    "    if not ok:\n",
    "        print(f\"WARNING: download failed for {name}; continue or fix before training.\")\n",
    "\n",
    "print('\\nVerification:')\n",
    "print(json.dumps(download_datasets.verify_datasets(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Preprocess (per dataset)\n",
    "This extracts frames/sequences and writes train/val/test CSVs. Set `USE_EXISTING_PROCESSED=True` to skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_EXISTING_PROCESSED:\n",
    "    for name in DATASETS:\n",
    "        print(f\"\\n=== {name}: preprocess ===\")\n",
    "        use_kaggle = name == 'kaggle'\n",
    "        use_pskus = name == 'pskus'\n",
    "        use_metc = name == 'metc'\n",
    "        use_synth = name == 'synthetic_blender_rozakar'\n",
    "        res = preprocess_data.preprocess_all_datasets(\n",
    "            use_kaggle=use_kaggle,\n",
    "            use_pskus=use_pskus,\n",
    "            use_metc=use_metc,\n",
    "            use_synthetic_blender_rozakar=use_synth\n",
    "        )\n",
    "        print(json.dumps({k: str(v) for k, v in res.items()}, indent=2))\n",
    "else:\n",
    "    print('Skipping preprocess: USE_EXISTING_PROCESSED=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Augmentation preview\n",
    "Shows original vs augmented frames using the on-the-fly augmentations (flip/rotate/zoom/shift/brightness/shadow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = PROCESSED_DIR / 'train.csv'\n",
    "if not train_csv.exists():\n",
    "    raise FileNotFoundError('train.csv missing; run preprocessing first')\n",
    "\n",
    "train_df = pd.read_csv(train_csv).sample(16, replace=True, random_state=42)\n",
    "gen, _, _ = create_frame_generators(train_df, train_df, train_df, batch_size=8, augment_multiplier=2)\n",
    "batch_imgs, _ = gen[0]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(batch_imgs[i])\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Augmented samples (frame models)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Train per dataset and model\n",
    "Batches are chosen by model type; checkpoints/logs go to `models/` and `checkpoints/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_for(model):\n",
    "    m = model.lower()\n",
    "    if m in ['mobilenetv2', 'resnet50', 'efficientnetb0']:\n",
    "        return BATCH_MOBILENET\n",
    "    if m in ['lstm', 'gru']:\n",
    "        return BATCH_SEQUENCE\n",
    "    if m == '3d_cnn':\n",
    "        return 12\n",
    "    raise ValueError(m)\n",
    "\n",
    "for name in DATASETS:\n",
    "    print(f\"\\n=== {name}: training ===\")\n",
    "    for model in MODELS:\n",
    "        try:\n",
    "            res = train.train_model(\n",
    "                model_type=model,\n",
    "                train_csv=PROCESSED_DIR / 'train.csv',\n",
    "                val_csv=PROCESSED_DIR / 'val.csv',\n",
    "                batch_size=batch_for(model),\n",
    "                epochs=EPOCHS,\n",
    "                learning_rate=LEARNING_RATE\n",
    "            )\n",
    "            print(json.dumps({\n",
    "                'model': model,\n",
    "                'best_epoch': int(res['best_epoch']) + 1,\n",
    "                'best_val_acc': float(res['history']['val_accuracy'][res['best_epoch']]),\n",
    "                'final_model': str(res['final_model_path'])\n",
    "            }, indent=2))\n",
    "        except Exception as exc:\n",
    "            print(f\"{model}: FAILED -> {exc}\")\n",
    "    # Optional cleanup to save space after each dataset\n",
    "    if name != DATASETS[-1]:\n",
    "        shutil.rmtree(RAW_DIR / name, ignore_errors=True)\n",
    "        for f in ['train.csv', 'val.csv', 'frames.csv']:\n",
    "            try:\n",
    "                os.remove(PROCESSED_DIR / f)\n",
    "            except FileNotFoundError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Evaluate on test set (if still present)\n",
    "Runs evaluation for any final models that were saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = PROCESSED_DIR / 'test.csv'\n",
    "if test_csv.exists():\n",
    "    for model in MODELS:\n",
    "        model_path = PROJECT_ROOT / 'models' / f'{model.lower()}_final.keras'\n",
    "        if not model_path.exists():\n",
    "            continue\n",
    "        try:\n",
    "            res = evaluate.evaluate_model(\n",
    "                model_path=model_path,\n",
    "                test_csv=test_csv,\n",
    "                model_type=model,\n",
    "                batch_size=batch_for(model),\n",
    "                save_results=True\n",
    "            )\n",
    "            print(model, json.dumps({k: v for k, v in res.items() if isinstance(v, (float, int, str))}, indent=2))\n",
    "        except Exception as exc:\n",
    "            print(f\"eval {model}: FAILED -> {exc}\")\n",
    "else:\n",
    "    print('test.csv not found; skipping evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Stop TensorBoard when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.kill(tb_proc.pid, signal.SIGTERM)\n",
    "    print('TensorBoard stopped')\n",
    "except Exception as exc:\n",
    "    print('TensorBoard stop error', exc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
