{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Kaggle GPU MobileNetV2 training + INT8 export\n\nThis notebook is self-contained and Kaggle-friendly:\n- detects Kaggle input datasets if running on Kaggle\n- trains MobileNetV2 on Kaggle WHO6 videos using random-frame sampling\n- evaluates on a held-out test split\n- exports an INT8 TFLite model for Axis DLPU compatibility\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "%pip install -q numpy pandas opencv-python tensorflow tqdm requests scikit-learn\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os\nimport time\nimport random\nimport tarfile\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport requests\n\nRANDOM_SEED = 42\nIMG_SIZE = (224, 224)\nNUM_CLASSES = 7\nCLASS_NAMES = ['Other', 'Step1_PalmToPalm', 'Step2_PalmOverDorsum', 'Step3_InterlacedFingers', 'Step4_BackOfFingers', 'Step5_ThumbRub', 'Step6_Fingertips']\nKAGGLE_URL = 'https://github.com/atiselsts/data/raw/master/kaggle-dataset-6classes.tar'\nKAGGLE_CLASS_MAPPING = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, 'step1': 1, 'step2': 2, 'step3': 3, 'step4': 4, 'step5': 5, 'step6': 6, 'other': 0}\nTRAIN_RATIO = 0.7\nVAL_RATIO = 0.15\nTEST_RATIO = 0.15\n\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\ntry:\n    tf.random.set_seed(RANDOM_SEED)\nexcept Exception:\n    pass\n\n\ndef find_repo_root(start=None):\n    start = Path.cwd() if start is None else Path(start)\n    for parent in [start] + list(start.parents):\n        if (parent / \"training\").exists():\n            return parent\n    return start\n\n\nREPO_ROOT = find_repo_root()\nTRAINING_DIR = REPO_ROOT / \"training\"\nRAW_DIR = REPO_ROOT / \"datasets\" / \"raw\"\nOUTPUT_DIR = TRAINING_DIR / \"outputs\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"Repo root:\", REPO_ROOT)\nprint(\"Output dir:\", OUTPUT_DIR)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## GPU check\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "print(\"TF version:\", tf.__version__)\nprint(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\nfor gpu in tf.config.list_physical_devices(\"GPU\"):\n    try:\n        tf.config.experimental.set_memory_growth(gpu, True)\n    except Exception:\n        pass\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Locate Kaggle dataset or download fallback\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "KAGGLE_DIR = RAW_DIR / \"kaggle\"\nKAGGLE_TAR = KAGGLE_DIR / \"kaggle-dataset-6classes.tar\"\nKAGGLE_EXTRACTED = KAGGLE_DIR / \"kaggle-dataset-6classes\"\n\n\ndef download_with_progress(url, dest: Path):\n    dest.parent.mkdir(parents=True, exist_ok=True)\n    if dest.exists():\n        print(\"skip\", dest)\n        return\n    with requests.get(url, stream=True, timeout=30) as r:\n        r.raise_for_status()\n        total = int(r.headers.get(\"content-length\", 0))\n        with open(dest, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dest.name) as pbar:\n            for chunk in r.iter_content(chunk_size=1024 * 1024):\n                if not chunk:\n                    continue\n                f.write(chunk)\n                pbar.update(len(chunk))\n\n\ndef extract_tar(tar_path: Path, extract_root: Path):\n    extract_root.mkdir(parents=True, exist_ok=True)\n    with tarfile.open(tar_path, \"r\") as tar:\n        tar.extractall(path=extract_root)\n\n\ndef find_kaggle_input_dataset():\n    root = Path(\"/kaggle/input\")\n    if not root.exists():\n        return None\n    for item in root.iterdir():\n        if not item.is_dir():\n            continue\n        candidate = item / \"kaggle-dataset-6classes\"\n        if candidate.exists():\n            return candidate\n        if all((item / str(i)).exists() for i in range(NUM_CLASSES)):\n            return item\n    return None\n\n\nDATA_ROOT = find_kaggle_input_dataset()\nif DATA_ROOT is not None:\n    print(\"Using Kaggle input dataset:\", DATA_ROOT)\nelse:\n    print(\"Kaggle input dataset not found. Downloading...\")\n    if not KAGGLE_EXTRACTED.exists():\n        download_with_progress(KAGGLE_URL, KAGGLE_TAR)\n        extract_tar(KAGGLE_TAR, KAGGLE_DIR)\n    DATA_ROOT = KAGGLE_EXTRACTED\n\nprint(\"Dataset root:\", DATA_ROOT)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Index videos and create splits\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "VIDEO_EXTS = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n\n\ndef kaggle_class_id_from_folder(name: str) -> int:\n    name_lower = name.lower()\n    if name_lower in KAGGLE_CLASS_MAPPING:\n        return int(KAGGLE_CLASS_MAPPING[name_lower])\n    digits = \"\".join(ch for ch in name_lower if ch.isdigit())\n    if digits:\n        class_id = int(digits)\n        if 0 <= class_id < len(CLASS_NAMES):\n            return class_id\n    raise ValueError(f\"Unknown Kaggle class folder: {name}\")\n\n\ndef collect_videos(dataset_root: Path) -> pd.DataFrame:\n    rows = []\n    for class_dir in sorted(dataset_root.iterdir()):\n        if not class_dir.is_dir():\n            continue\n        class_id = kaggle_class_id_from_folder(class_dir.name)\n        for vid in class_dir.iterdir():\n            if vid.suffix.lower() not in VIDEO_EXTS:\n                continue\n            rows.append({\"video_path\": str(vid), \"class_id\": class_id})\n    df = pd.DataFrame(rows)\n    if df.empty:\n        raise RuntimeError(f\"No videos found in {dataset_root}\")\n    return df\n\n\nvideos_df = collect_videos(DATA_ROOT)\nprint(\"Total videos:\", len(videos_df))\n\ntrain_df, temp_df = train_test_split(\n    videos_df,\n    test_size=(VAL_RATIO + TEST_RATIO),\n    stratify=videos_df[\"class_id\"],\n    random_state=RANDOM_SEED,\n)\nval_size = VAL_RATIO / (VAL_RATIO + TEST_RATIO)\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=(1.0 - val_size),\n    stratify=temp_df[\"class_id\"],\n    random_state=RANDOM_SEED,\n)\n\nprint(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## tf.data pipeline (random frames per video)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_v2_preprocess\n\n\ndef _load_random_frame_py(video_path_bytes, label):\n    if hasattr(video_path_bytes, \"numpy\"):\n        video_path_bytes = video_path_bytes.numpy()\n    video_path = video_path_bytes.decode(\"utf-8\")\n    cap = cv2.VideoCapture(video_path)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if frame_count <= 0:\n        cap.release()\n        raise RuntimeError(f\"No frames in {video_path}\")\n    target_idx = np.random.randint(0, frame_count)\n    cap.set(cv2.CAP_PROP_POS_FRAMES, target_idx)\n    ok, frame = cap.read()\n    cap.release()\n    if not ok:\n        raise RuntimeError(f\"Failed reading frame from {video_path}\")\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame = cv2.resize(frame, IMG_SIZE)\n    frame = frame.astype(np.float32)\n    frame = mobilenet_v2_preprocess(frame)\n    return frame, np.int32(label)\n\n\ndef _load_random_frame(video_path, label):\n    frame, label = tf.py_function(\n        _load_random_frame_py,\n        inp=[video_path, label],\n        Tout=[tf.float32, tf.int32],\n    )\n    frame.set_shape((*IMG_SIZE, 3))\n    label.set_shape(())\n    return frame, label\n\n\ndef make_dataset(df, batch_size=32, shuffle=True):\n    ds = tf.data.Dataset.from_tensor_slices((df[\"video_path\"].values, df[\"class_id\"].values))\n    if shuffle:\n        ds = ds.shuffle(len(df), seed=RANDOM_SEED, reshuffle_each_iteration=True)\n    ds = ds.map(_load_random_frame, num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds\n\n\nBATCH_SIZE = 32\ntrain_ds = make_dataset(train_df, batch_size=BATCH_SIZE, shuffle=True)\nval_ds = make_dataset(val_df, batch_size=BATCH_SIZE, shuffle=False)\ntest_ds = make_dataset(test_df, batch_size=BATCH_SIZE, shuffle=False)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Build and train MobileNetV2 (GPU)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from tensorflow.keras import layers, models\n\n\ndef build_model():\n    base = tf.keras.applications.MobileNetV2(\n        input_shape=(*IMG_SIZE, 3),\n        include_top=False,\n        weights=\"imagenet\",\n    )\n    base.trainable = False\n    inputs = layers.Input(shape=(*IMG_SIZE, 3))\n    x = base(inputs, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n    model = models.Model(inputs, outputs)\n    return model, base\n\n\nmodel, base = build_model()\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[\"accuracy\"],\n)\n\nmodel.summary()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "EPOCHS_STAGE1 = 3\nEPOCHS_STAGE2 = 2\n\nhistory_stage1 = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS_STAGE1,\n)\n\nbase.trainable = True\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=[\"accuracy\"],\n)\n\nhistory_stage2 = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS_STAGE2,\n)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Evaluate and save\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "metrics = model.evaluate(test_ds, verbose=1)\nprint(\"Test metrics:\", dict(zip(model.metrics_names, metrics)))\n\nmodel_path = OUTPUT_DIR / \"mobilenetv2_kaggle_gpu.keras\"\nmodel.save(model_path)\nprint(\"Saved model:\", model_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Export INT8 TFLite (Axis DLPU compatible)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from typing import Iterable\n\nDISABLE_PER_CHANNEL = False\n\n\ndef representative_dataset_from_ds(ds, max_batches=10) -> Iterable[list[np.ndarray]]:\n    count = 0\n    for batch, _ in ds:\n        batch_np = batch.numpy().astype(np.float32)\n        for i in range(batch_np.shape[0]):\n            yield [batch_np[i : i + 1]]\n        count += 1\n        if count >= max_batches:\n            break\n\n\ntflite_path = OUTPUT_DIR / \"mobilenetv2_kaggle_gpu_int8.tflite\"\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = lambda: representative_dataset_from_ds(train_ds)\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.int8\nconverter.inference_output_type = tf.int8\nif DISABLE_PER_CHANNEL:\n    converter._experimental_disable_per_channel = True\n\ntry:\n    tflite_model = converter.convert()\n    tflite_path.write_bytes(tflite_model)\n    print(\"Saved TFLite:\", tflite_path)\nexcept Exception as exc:\n    print(\"INT8 conversion failed:\", exc)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "if tflite_path.exists():\n    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n    interpreter.allocate_tensors()\n    input_detail = interpreter.get_input_details()[0]\n    output_detail = interpreter.get_output_details()[0]\n    print(\"Input dtype:\", input_detail[\"dtype\"], \"quant:\", input_detail.get(\"quantization\"))\n    print(\"Output dtype:\", output_detail[\"dtype\"], \"quant:\", output_detail.get(\"quantization\"))\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}