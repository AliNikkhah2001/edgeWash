{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation Notebook\n",
    "\n",
    "Use this after datasets are prepared. Runs training, evaluation, TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Detect Colab\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "    print('Running on Google Colab')\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print('Running locally')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mount Drive and set WORK_DIR (adjust if you used a different folder)\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    WORK_DIR = '/content/drive/MyDrive/handwash_training'\n",
    "else:\n",
    "    WORK_DIR = '.'\n",
    "\n",
    "print('WORK_DIR:', WORK_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ensure repo root and import training modules\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_repo_root():\n",
    "    candidates = [\n",
    "        Path(WORK_DIR) / 'edgeWash',\n",
    "        Path('/content/edgeWash'),\n",
    "        Path('/content/drive/MyDrive/edgeWash'),\n",
    "        Path('/content/drive/MyDrive/handwash_training/edgeWash'),\n",
    "        Path('/content/drive/MyDrive/handwash_training_colab/edgeWash'),\n",
    "        Path.cwd()\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if (c / 'training' / 'config.py').exists():\n",
    "            os.chdir(c)\n",
    "            td = c / 'training'\n",
    "            if str(td) not in sys.path:\n",
    "                sys.path.insert(0, str(td))\n",
    "            print('Using repo root:', c)\n",
    "            print('Added to sys.path:', td)\n",
    "            return c, td\n",
    "    raise FileNotFoundError('Cannot locate training/config.py; update WORK_DIR or clone repo.')\n",
    "\n",
    "repo_root, training_dir = ensure_repo_root()\n",
    "\n",
    "import config\n",
    "import download_datasets\n",
    "import preprocess_data\n",
    "import data_generators\n",
    "import models\n",
    "import train as train_module\n",
    "import evaluate\n",
    "\n",
    "print('Training modules imported successfully!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Options (2D, 3D, and temporal)\n",
    "\n",
    "Pick a backbone: frame-based (MobileNetV2/ResNet50/EfficientNetB0) or temporal (LSTM/GRU/3D CNN).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "AVAILABLE_MODELS = ['mobilenetv2', 'resnet50', 'efficientnetb0', 'lstm', 'gru', '3d_cnn']\n",
    "FRAME_BACKBONES = ['mobilenetv2', 'resnet50', 'efficientnetb0']\n",
    "TEMPORAL_BACKBONES = ['lstm', 'gru', '3d_cnn']\n",
    "print('Available models:', ', '.join(AVAILABLE_MODELS))\n",
    "print('Frame models:', ', '.join(FRAME_BACKBONES))\n",
    "print('Temporal models:', ', '.join(TEMPORAL_BACKBONES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling frequency (frame skip)\n",
    "\n",
    "Set how many frames to skip when extracting. If you change this, rerun preprocessing so splits update.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "FRAME_SKIP_TO_USE = 2  # e.g., 1=all frames, 2=every other, 4=every 4th\n",
    "config.FRAME_SKIP = FRAME_SKIP_TO_USE\n",
    "print('Frame skip set to', config.FRAME_SKIP)\n",
    "print('Available presets:', getattr(config, 'FRAME_SKIP_OPTIONS', [1,2,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live TensorBoard (start before training)\n",
    "\n",
    "Starts TensorBoard so you can watch training live in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {config.LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Selected Models\n",
    "\n",
    "Train any combination of models (2D frame, temporal, 3D CNN). Use the config cell below to pick them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 10  # adjust as needed\n",
    "MODELS_TO_TRAIN = ['mobilenetv2', 'resnet50', 'efficientnetb0', 'lstm', 'gru', '3d_cnn']\n",
    "FRAME_SKIP_USED = config.FRAME_SKIP\n",
    "\n",
    "print('=' * 80)\n",
    "print('TRAINING PIPELINE')\n",
    "print('=' * 80)\n",
    "print('\nModels: ' + ', '.join([m.upper() for m in MODELS_TO_TRAIN]))\n",
    "print(f'Epochs: {EPOCHS}')\n",
    "print(f'Frame skip: {FRAME_SKIP_USED}')\n",
    "print(f'Checkpoints will be saved to: {config.CHECKPOINTS_DIR}')\n",
    "print(f'Final models will be saved to: {config.MODELS_DIR}')\n",
    "print('\n' + '=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "training_results = {}\n",
    "total_models = len(MODELS_TO_TRAIN)\n",
    "\n",
    "for idx, model_type in enumerate(MODELS_TO_TRAIN, start=1):\n",
    "    print('\n' + '='*80)\n",
    "    print(f'TRAINING MODEL {idx}/{total_models}: {model_type.upper()}')\n",
    "    print('='*80)\n",
    "    \n",
    "    if model_type in FRAME_BACKBONES:\n",
    "        batch_size = 32\n",
    "    elif model_type == \"3d_cnn\":\n",
    "        batch_size = 12\n",
    "    else:\n",
    "        batch_size = 16\n",
    "    \n",
    "    result = train_module.train_model(\n",
    "        model_type=model_type,\n",
    "        train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "        val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "        batch_size=batch_size,\n",
    "        epochs=EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE\n",
    "    )\n",
    "    training_results[model_type] = result\n",
    "    \n",
    "    best_epoch = result[\"best_epoch\"]\n",
    "    best_val_acc = result[\"history\"][\"val_accuracy\"][best_epoch]\n",
    "    best_val_loss = result[\"history\"][\"val_loss\"][best_epoch]\n",
    "    \n",
    "    print(f'\\n\u2713 {model_type.upper()} training complete!')\n",
    "    print(f'  Best epoch: {best_epoch + 1}')\n",
    "    print(f'  Best val accuracy: {best_val_acc:.4f}')\n",
    "    print(f'  Best val loss: {best_val_loss:.4f}')\n",
    "    print('  Final model saved: {}'.format(result['final_model_path']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Visualization\n",
    "\n",
    "Compare training curves across all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate All Models\n",
    "\n",
    "Evaluate all trained models on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "evaluation_results = {}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING ALL MODELS ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    print(f\"\\nEvaluating {model_type.upper()}...\")\n",
    "    \n",
    "    batch_size = 32 if model_type == 'mobilenetv2' else 16\n",
    "    \n",
    "    eval_results = evaluate.evaluate_model(\n",
    "        model_path=training_results[model_type]['final_model_path'],\n",
    "        test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "        model_type=model_type,\n",
    "        batch_size=batch_size,\n",
    "        save_results=True\n",
    "    )\n",
    "    \n",
    "    evaluation_results[model_type] = eval_results\n",
    "    \n",
    "    print(f\"\u2713 {model_type.upper()} evaluation complete!\")\n",
    "    print(f\"  Accuracy: {eval_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score: {eval_results['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL EVALUATIONS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed metrics for each model\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    eval_results = evaluation_results[model_type]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{model_type.upper()} - TEST SET METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"  Accuracy:       {eval_results['accuracy']:.4f}\")\n",
    "    print(f\"  Top-2 Accuracy: {eval_results['top2_accuracy']:.4f}\")\n",
    "    print(f\"  Precision:      {eval_results['precision']:.4f}\")\n",
    "    print(f\"  Recall:         {eval_results['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:       {eval_results['f1_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class F1-Scores:\")\n",
    "    for class_name in config.CLASS_NAMES:\n",
    "        metrics = eval_results['per_class_metrics'][class_name]\n",
    "        print(f\"  {class_name}: {metrics['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorBoard\n",
    "\n",
    "Launch TensorBoard to view training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension (Jupyter/Colab)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%tensorboard --logdir {config.LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Comparison\n",
    "\n",
    "Compare all 3 models with comprehensive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison visualization\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING MODEL COMPARISON PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Call compare_models from evaluate module\n",
    "comparison_path = config.RESULTS_DIR / 'model_comparison.png'\n",
    "evaluate.compare_models(\n",
    "    evaluation_results,\n",
    "    save_path=comparison_path\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Comparison plot saved: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison plot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "if comparison_path.exists():\n",
    "    display(Image(filename=str(comparison_path)))\n",
    "else:\n",
    "    print(\"Comparison plot not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    eval_results = evaluation_results[model_type]\n",
    "    summary_data.append({\n",
    "        'Model': model_type.upper(),\n",
    "        'Accuracy': f\"{eval_results['accuracy']:.4f}\",\n",
    "        'Top-2 Acc': f\"{eval_results['top2_accuracy']:.4f}\",\n",
    "        'Precision': f\"{eval_results['precision']:.4f}\",\n",
    "        'Recall': f\"{eval_results['recall']:.4f}\",\n",
    "        'F1-Score': f\"{eval_results['f1_score']:.4f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_path = config.RESULTS_DIR / 'model_comparison_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\n\u2713 Summary saved: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model\n",
    "best_model = max(evaluation_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "best_model_name = best_model[0]\n",
    "best_f1 = best_model[1]['f1_score']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n\ud83c\udfc6 {best_model_name.upper()} achieved the highest F1-Score: {best_f1:.4f}\")\n",
    "print(f\"\\nAll metrics for {best_model_name.upper()}:\")\n",
    "for metric, value in best_model[1].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Saved Models & Checkpoints\n",
    "\n",
    "Summary of all saved model weights and checkpoints on Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display saved model paths\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVED MODEL WEIGHTS (Google Drive)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nModels directory: {config.MODELS_DIR}\")\n",
    "print(f\"Checkpoints directory: {config.CHECKPOINTS_DIR}\")\n",
    "\n",
    "print(\"\\nFinal Model Weights:\")\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    model_path = training_results[model_type]['final_model_path']\n",
    "    checkpoint_path = training_results[model_type]['best_checkpoint_path']\n",
    "    \n",
    "    print(f\"\\n{model_type.upper()}:\")\n",
    "    print(f\"  Final model: {model_path}\")\n",
    "    print(f\"  Best checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Check file size\n",
    "    if Path(model_path).exists():\n",
    "        size_mb = Path(model_path).stat().st_size / (1024 * 1024)\n",
    "        print(f\"  Model size: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All model weights are saved to Google Drive!\")\n",
    "print(\"They will persist even if Colab runtime disconnects.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary & Next Steps\n",
    "\n",
    "Complete training pipeline finished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 10  # adjust as needed\n",
    "MODELS_TO_TRAIN = ['mobilenetv2', 'resnet50', 'efficientnetb0', 'lstm', 'gru', '3d_cnn']\n",
    "FRAME_SKIP_USED = config.FRAME_SKIP\n",
    "\n",
    "print('=' * 80)\n",
    "print('TRAINING PIPELINE')\n",
    "print('=' * 80)\n",
    "print('\nModels: ' + ', '.join([m.upper() for m in MODELS_TO_TRAIN]))\n",
    "print(f'Epochs: {EPOCHS}')\n",
    "print(f'Frame skip: {FRAME_SKIP_USED}')\n",
    "print(f'Checkpoints will be saved to: {config.CHECKPOINTS_DIR}')\n",
    "print(f'Final models will be saved to: {config.MODELS_DIR}')\n",
    "print('\n' + '=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%tensorboard --logdir {config.LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optional: Train Additional Models\n",
    "\n",
    "Train LSTM or GRU models for temporal modeling (requires sequence data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train LSTM model\n",
    "\n",
    "# lstm_result = train_module.train_model(\n",
    "#     model_type='lstm',\n",
    "#     train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "#     val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "#     batch_size=16,  # Reduce batch size for sequence models\n",
    "#     epochs=20,\n",
    "#     learning_rate=config.LEARNING_RATE\n",
    "# )\n",
    "\n",
    "# print(\"\\n\u2713 LSTM training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train GRU model\n",
    "\n",
    "# gru_result = train_module.train_model(\n",
    "#     model_type='gru',\n",
    "#     train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "#     val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "#     batch_size=16,\n",
    "#     epochs=20,\n",
    "#     learning_rate=config.LEARNING_RATE\n",
    "# )\n",
    "\n",
    "# print(\"\\n\u2713 GRU training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Comparison\n",
    "\n",
    "Compare multiple models (if trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare MobileNetV2, LSTM, GRU\n",
    "# Uncomment if you trained multiple models\n",
    "\n",
    "# model_results = {\n",
    "#     'MobileNetV2': eval_results,\n",
    "#     'LSTM': evaluate.evaluate_model(\n",
    "#         model_path=str(config.MODELS_DIR / 'lstm_final.keras'),\n",
    "#         test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "#         model_type='lstm',\n",
    "#         batch_size=16,\n",
    "#         save_results=True\n",
    "#     ),\n",
    "#     'GRU': evaluate.evaluate_model(\n",
    "#         model_path=str(config.MODELS_DIR / 'gru_final.keras'),\n",
    "#         test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "#         model_type='gru',\n",
    "#         batch_size=16,\n",
    "#         save_results=True\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# # Create comparison plot\n",
    "# evaluate.compare_models(\n",
    "#     model_results,\n",
    "#     save_path=config.RESULTS_DIR / 'model_comparison.png'\n",
    "# )\n",
    "\n",
    "# display(Image(filename=str(config.RESULTS_DIR / 'model_comparison.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary & Next Steps\n",
    "\n",
    "Training pipeline complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 10  # adjust as needed\n",
    "MODELS_TO_TRAIN = ['mobilenetv2', 'resnet50', 'efficientnetb0', 'lstm', 'gru', '3d_cnn']\n",
    "FRAME_SKIP_USED = config.FRAME_SKIP\n",
    "\n",
    "print('=' * 80)\n",
    "print('TRAINING PIPELINE')\n",
    "print('=' * 80)\n",
    "print('\nModels: ' + ', '.join([m.upper() for m in MODELS_TO_TRAIN]))\n",
    "print(f'Epochs: {EPOCHS}')\n",
    "print(f'Frame skip: {FRAME_SKIP_USED}')\n",
    "print(f'Checkpoints will be saved to: {config.CHECKPOINTS_DIR}')\n",
    "print(f'Final models will be saved to: {config.MODELS_DIR}')\n",
    "print('\n' + '=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space-Saving Sequential Pipeline\n",
    "\n",
    "Download one dataset at a time, train/evaluate, then delete raw/extracted frames to conserve Drive space. Models/logs/results are kept."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configure per-dataset run\n",
    "DATASETS = ['kaggle', 'pskus', 'metc']  # edit list/order\n",
    "MODELS = ['mobilenetv2']  # add resnet50, efficientnetb0, lstm, gru, 3d_cnn\n",
    "EPOCHS = 5\n",
    "BATCH_FRAME = 32\n",
    "BATCH_SEQ = 16\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import train as train_module\n",
    "import evaluate\n",
    "import download_datasets\n",
    "import preprocess_data\n",
    "import config\n",
    "\n",
    "def cleanup_dataset(name):\n",
    "    raw_path = Path('datasets/raw') / name\n",
    "    if raw_path.exists():\n",
    "        shutil.rmtree(raw_path, ignore_errors=True)\n",
    "        print(f'Removed raw: {raw_path}')\n",
    "    for jpg in Path('datasets/processed').rglob('*.jpg'):\n",
    "        try:\n",
    "            jpg.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "    for d in sorted(Path('datasets/processed').rglob('*'), key=lambda x: len(str(x)), reverse=True):\n",
    "        if d.is_dir():\n",
    "            try:\n",
    "                next(d.iterdir())\n",
    "            except StopIteration:\n",
    "                d.rmdir()\n",
    "\n",
    "for ds in DATASETS:\n",
    "    print(f'=== DATASET {ds.upper()} ===')\n",
    "    if ds == 'kaggle':\n",
    "        ok = download_datasets.download_kaggle_dataset()\n",
    "    elif ds == 'pskus':\n",
    "        ok = download_datasets.download_pskus_dataset()\n",
    "    elif ds == 'metc':\n",
    "        ok = download_datasets.download_metc_dataset()\n",
    "    else:\n",
    "        raise ValueError(f'Unknown dataset {ds}')\n",
    "    if not ok:\n",
    "        print(f'Skipping {ds}, download failed.')\n",
    "        continue\n",
    "    result = preprocess_data.preprocess_all_datasets(\n",
    "        use_kaggle=ds=='kaggle', use_pskus=ds=='pskus', use_metc=ds=='metc')\n",
    "    if not result:\n",
    "        print(f'Skipping {ds}, preprocess failed.')\n",
    "        cleanup_dataset(ds)\n",
    "        continue\n",
    "    for model in MODELS:\n",
    "        batch = BATCH_FRAME if model in ['mobilenetv2','resnet50','efficientnetb0'] else (12 if model=='3d_cnn' else BATCH_SEQ)\n",
    "        res = train_module.train_model(\n",
    "            model_type=model,\n",
    "            train_csv=Path('datasets/processed/train.csv'),\n",
    "            val_csv=Path('datasets/processed/val.csv'),\n",
    "            batch_size=batch,\n",
    "            epochs=EPOCHS,\n",
    "            learning_rate=config.LEARNING_RATE\n",
    "        )\n",
    "        eval_res = evaluate.evaluate_model(\n",
    "            model_path=res['final_model_path'],\n",
    "            test_csv=Path('datasets/processed/test.csv'),\n",
    "            model_type=model,\n",
    "            batch_size=batch,\n",
    "            save_results=True\n",
    "        )\n",
    "        val_acc = res['history']['val_accuracy'][res['best_epoch']]\n",
    "        print('{} {}: val_acc={:.4f} test_acc={:.4f}'.format(ds, model, val_acc, eval_res['accuracy']))\n",
    "    cleanup_dataset(ds)\n",
    "    print(f'=== DONE {ds.upper()} ===\n')\n",
    "print('Sequential pipeline complete.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}