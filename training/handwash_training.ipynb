{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Handwashing Detection Training Pipeline\n",
    "\n",
    "**Complete training pipeline using modular Python modules**\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Dataset download (Kaggle WHO6)\n",
    "2. Data preprocessing (frame extraction)\n",
    "3. Model training (MobileNetV2)\n",
    "4. Evaluation and visualization\n",
    "5. Model comparison\n",
    "\n",
    "**Runtime**: GPU (recommended for training)\n",
    "\n",
    "**Expected Duration**: 2-3 hours for complete pipeline\n",
    "\n",
    "**Author**: Generated with AdaL (https://github.com/sylphai/adal-cli)\n",
    "\n",
    "**Date**: 2025-12-31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set working directory\n",
    "    import os\n",
    "    WORK_DIR = '/content/drive/MyDrive/handwash_training'\n",
    "    os.makedirs(WORK_DIR, exist_ok=True)\n",
    "    %cd {WORK_DIR}\n",
    "else:\n",
    "    WORK_DIR = '.'\n",
    "    print(f\"Working directory: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tensorflow==2.15.0\n",
    "!pip install -q scikit-learn pandas numpy opencv-python-headless\n",
    "!pip install -q matplotlib seaborn tqdm requests\n",
    "\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Training Modules\n",
    "\n",
    "Clone the modular Python training modules from your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not already cloned)\n",
    "REPO_URL = \"https://github.com/AliNikkhah2001/edgeWash.git\"\n",
    "REPO_DIR = Path(\"edgeWash\")\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    print(f\"Cloning repository from {REPO_URL}...\")\n",
    "    !git clone {REPO_URL}\n",
    "else:\n",
    "    print(f\"Repository already exists: {REPO_DIR}\")\n",
    "    print(\"Pulling latest changes...\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# Add training modules to Python path\n",
    "training_dir = REPO_DIR / \"training\"\n",
    "if str(training_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(training_dir))\n",
    "\n",
    "print(f\"Training modules path: {training_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "import config\n",
    "import download_datasets\n",
    "import preprocess_data\n",
    "import data_generators\n",
    "import models\n",
    "import train as train_module\n",
    "import evaluate\n",
    "\n",
    "print(\"Training modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "View and customize training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display configuration\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nImage size: {config.IMG_SIZE}\")\n",
    "print(f\"Sequence length: {config.SEQUENCE_LENGTH}\")\n",
    "print(f\"Number of classes: {config.NUM_CLASSES}\")\n",
    "print(f\"Class names: {config.CLASS_NAMES}\")\n",
    "\n",
    "print(f\"\\nBatch size: {config.BATCH_SIZE}\")\n",
    "print(f\"Epochs: {config.EPOCHS}\")\n",
    "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"Early stopping patience: {config.PATIENCE}\")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {config.TRAIN_RATIO*100:.0f}%\")\n",
    "print(f\"  Val:   {config.VAL_RATIO*100:.0f}%\")\n",
    "print(f\"  Test:  {config.TEST_RATIO*100:.0f}%\")\n",
    "\n",
    "print(f\"\\nAugmentation:\")\n",
    "for key, value in config.AUGMENTATION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nModel architectures available:\")\n",
    "for model_name, model_config in config.MODEL_CONFIGS.items():\n",
    "    print(f\"  - {model_name}: {model_config['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Download\n",
    "\n",
    "Download Kaggle WHO6 dataset (~1 GB, quick start).\n",
    "\n",
    "For full pipeline, also download PSKUS (18 GB) and METC (2 GB) - see commented code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle WHO6 dataset\n",
    "print(\"Downloading Kaggle WHO6 dataset...\")\n",
    "success = download_datasets.download_kaggle_dataset()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n✓ Kaggle dataset ready!\")\n",
    "else:\n",
    "    print(\"\\n✗ Kaggle dataset download failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download PSKUS and METC datasets (large, requires zenodo-get)\n",
    "# Uncomment to download:\n",
    "\n",
    "# # Install zenodo-get\n",
    "# !pip install zenodo-get\n",
    "\n",
    "# # Download PSKUS (18 GB, ~30-60 minutes)\n",
    "# print(\"Downloading PSKUS Hospital dataset (18 GB)...\")\n",
    "# download_datasets.download_pskus_dataset()\n",
    "\n",
    "# # Download METC (2 GB, ~5-10 minutes)\n",
    "# print(\"Downloading METC Lab dataset (2 GB)...\")\n",
    "# download_datasets.download_metc_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify datasets\n",
    "status = download_datasets.verify_datasets()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name, info in status.items():\n",
    "    status_icon = \"✓\" if info['exists'] else \"✗\"\n",
    "    print(f\"{status_icon} {info['name']}: {info['num_files']} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "Extract frames from videos and create train/val/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Kaggle dataset\n",
    "print(\"Preprocessing Kaggle dataset...\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "result = preprocess_data.preprocess_all_datasets(\n",
    "    use_kaggle=True,\n",
    "    use_pskus=False,  # Set True if PSKUS downloaded\n",
    "    use_metc=False    # Set True if METC downloaded\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(\"\\n✓ Preprocessing complete!\")\n",
    "    print(f\"\\nProcessed files:\")\n",
    "    for key, path in result.items():\n",
    "        print(f\"  {key}: {path}\")\n",
    "else:\n",
    "    print(\"\\n✗ Preprocessing failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Exploration\n",
    "\n",
    "Visualize dataset statistics and sample frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "train_df = pd.read_csv(config.PROCESSED_DIR / 'train.csv')\n",
    "val_df = pd.read_csv(config.PROCESSED_DIR / 'val.csv')\n",
    "test_df = pd.read_csv(config.PROCESSED_DIR / 'test.csv')\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_df)} frames ({len(train_df['video_id'].unique())} videos)\")\n",
    "print(f\"  Val:   {len(val_df)} frames ({len(val_df['video_id'].unique())} videos)\")\n",
    "print(f\"  Test:  {len(test_df)} frames ({len(test_df['video_id'].unique())} videos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df, split_name) in enumerate([(train_df, 'Train'), (val_df, 'Val'), (test_df, 'Test')]):\n",
    "    class_counts = df['class_name'].value_counts()\n",
    "    \n",
    "    axes[idx].bar(range(len(class_counts)), class_counts.values)\n",
    "    axes[idx].set_title(f'{split_name} Set - Class Distribution', fontsize=12)\n",
    "    axes[idx].set_xlabel('Class', fontsize=10)\n",
    "    axes[idx].set_ylabel('Number of Frames', fontsize=10)\n",
    "    axes[idx].set_xticks(range(len(class_counts)))\n",
    "    axes[idx].set_xticklabels([cn.split('_')[-1] for cn in class_counts.index], rotation=45, ha='right')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample frames\n",
    "import cv2\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "combined_df = pd.concat([train_df, val_df, test_df])\n",
    "\n",
    "for class_id in range(config.NUM_CLASSES):\n",
    "    # Get sample frame for this class\n",
    "    sample_row = combined_df[combined_df['class_id'] == class_id].sample(1).iloc[0]\n",
    "    frame_path = sample_row['frame_path']\n",
    "    \n",
    "    # Load and display frame\n",
    "    img = cv2.imread(frame_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    axes[class_id].imshow(img_rgb)\n",
    "    axes[class_id].set_title(config.CLASS_NAMES[class_id], fontsize=10)\n",
    "    axes[class_id].axis('off')\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[7])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_frames.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training\n",
    "\n",
    "Train MobileNetV2 model (frame-based classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "MODEL_TYPE = 'mobilenetv2'\n",
    "EPOCHS = 20  # Reduce for quick testing (use 50 for full training)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Training {MODEL_TYPE} for {EPOCHS} epochs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "result = train_module.train_model(\n",
    "    model_type=MODEL_TYPE,\n",
    "    train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "    val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=config.LEARNING_RATE\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "print(f\"Final model saved: {result['final_model_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Visualization\n",
    "\n",
    "Plot training curves (loss, accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "history = result['history']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_title('Model Loss', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['accuracy'], label='Train Accuracy')\n",
    "axes[1].plot(history['val_accuracy'], label='Val Accuracy')\n",
    "axes[1].set_title('Model Accuracy', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Best epoch\n",
    "best_epoch = result['best_epoch']\n",
    "print(f\"\\nBest epoch: {best_epoch + 1}\")\n",
    "print(f\"Best val_accuracy: {history['val_accuracy'][best_epoch]:.4f}\")\n",
    "print(f\"Best val_loss: {history['val_loss'][best_epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "\n",
    "Evaluate trained model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "eval_results = evaluate.evaluate_model(\n",
    "    model_path=result['final_model_path'],\n",
    "    test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "    model_type=MODEL_TYPE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"  Accuracy:      {eval_results['accuracy']:.4f}\")\n",
    "print(f\"  Top-2 Accuracy: {eval_results['top2_accuracy']:.4f}\")\n",
    "print(f\"  Precision:     {eval_results['precision']:.4f}\")\n",
    "print(f\"  Recall:        {eval_results['recall']:.4f}\")\n",
    "print(f\"  F1-Score:      {eval_results['f1_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nPer-Class Metrics:\")\n",
    "for class_name in config.CLASS_NAMES:\n",
    "    metrics = eval_results['per_class_metrics'][class_name]\n",
    "    print(f\"  {class_name}:\")\n",
    "    print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"    Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"    F1-Score:  {metrics['f1-score']:.4f}\")\n",
    "    print(f\"    Support:   {int(metrics['support'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "from IPython.display import Image, display\n",
    "\n",
    "cm_path = config.RESULTS_DIR / MODEL_TYPE / 'confusion_matrix.png'\n",
    "if cm_path.exists():\n",
    "    display(Image(filename=str(cm_path)))\n",
    "else:\n",
    "    # Plot confusion matrix inline\n",
    "    evaluate.plot_confusion_matrix(\n",
    "        eval_results['confusion_matrix'],\n",
    "        config.CLASS_NAMES,\n",
    "        save_path=None,\n",
    "        normalize=True\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorBoard\n",
    "\n",
    "Launch TensorBoard to view training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension (Jupyter/Colab)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%tensorboard --logdir {config.LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optional: Train Additional Models\n",
    "\n",
    "Train LSTM or GRU models for temporal modeling (requires sequence data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train LSTM model\n",
    "\n",
    "# lstm_result = train_module.train_model(\n",
    "#     model_type='lstm',\n",
    "#     train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "#     val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "#     batch_size=16,  # Reduce batch size for sequence models\n",
    "#     epochs=20,\n",
    "#     learning_rate=config.LEARNING_RATE\n",
    "# )\n",
    "\n",
    "# print(\"\\n✓ LSTM training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train GRU model\n",
    "\n",
    "# gru_result = train_module.train_model(\n",
    "#     model_type='gru',\n",
    "#     train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "#     val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "#     batch_size=16,\n",
    "#     epochs=20,\n",
    "#     learning_rate=config.LEARNING_RATE\n",
    "# )\n",
    "\n",
    "# print(\"\\n✓ GRU training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Comparison\n",
    "\n",
    "Compare multiple models (if trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare MobileNetV2, LSTM, GRU\n",
    "# Uncomment if you trained multiple models\n",
    "\n",
    "# model_results = {\n",
    "#     'MobileNetV2': eval_results,\n",
    "#     'LSTM': evaluate.evaluate_model(\n",
    "#         model_path=str(config.MODELS_DIR / 'lstm_final.keras'),\n",
    "#         test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "#         model_type='lstm',\n",
    "#         batch_size=16,\n",
    "#         save_results=True\n",
    "#     ),\n",
    "#     'GRU': evaluate.evaluate_model(\n",
    "#         model_path=str(config.MODELS_DIR / 'gru_final.keras'),\n",
    "#         test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "#         model_type='gru',\n",
    "#         batch_size=16,\n",
    "#         save_results=True\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# # Create comparison plot\n",
    "# evaluate.compare_models(\n",
    "#     model_results,\n",
    "#     save_path=config.RESULTS_DIR / 'model_comparison.png'\n",
    "# )\n",
    "\n",
    "# display(Image(filename=str(config.RESULTS_DIR / 'model_comparison.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary & Next Steps\n",
    "\n",
    "Training pipeline complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TRAINING PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTrained model: {MODEL_TYPE}\")\n",
    "print(f\"Model saved: {result['final_model_path']}\")\n",
    "print(f\"\\nTest Accuracy: {eval_results['accuracy']:.4f}\")\n",
    "print(f\"Test F1-Score: {eval_results['f1_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nResults saved to:\")\n",
    "print(f\"  - Confusion matrix: {config.RESULTS_DIR / MODEL_TYPE / 'confusion_matrix.png'}\")\n",
    "print(f\"  - Classification report: {config.RESULTS_DIR / MODEL_TYPE / 'classification_report.txt'}\")\n",
    "print(f\"  - Metrics CSV: {config.RESULTS_DIR / MODEL_TYPE / 'metrics.csv'}\")\n",
    "\n",
    "print(f\"\\nTensorBoard logs: {config.LOGS_DIR}\")\n",
    "print(f\"Checkpoints: {config.CHECKPOINTS_DIR}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Fine-tune model with more epochs (50+)\")\n",
    "print(\"  2. Train temporal models (LSTM/GRU) for sequence modeling\")\n",
    "print(\"  3. Download larger datasets (PSKUS, METC) for better accuracy\")\n",
    "print(\"  4. Experiment with different augmentation strategies\")\n",
    "print(\"  5. Export model to TFLite for mobile deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
