{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Handwashing Detection Training Pipeline\n",
    "\n",
    "**Complete training pipeline using modular Python modules**\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Dataset download (Kaggle WHO6)\n",
    "2. Data preprocessing (frame extraction)\n",
    "3. Model training (MobileNetV2)\n",
    "4. Evaluation and visualization\n",
    "5. Model comparison\n",
    "\n",
    "**Runtime**: GPU (recommended for training)\n",
    "\n",
    "**Expected Duration**: 2-3 hours for complete pipeline\n",
    "\n",
    "**Author**: Generated with AdaL (https://github.com/sylphai/adal-cli)\n",
    "\n",
    "**Date**: 2025-12-31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set working directory\n",
    "    import os\n",
    "    WORK_DIR = '/content/drive/MyDrive/handwash_training'\n",
    "    os.makedirs(WORK_DIR, exist_ok=True)\n",
    "    %cd {WORK_DIR}\n",
    "else:\n",
    "    WORK_DIR = '.'\n",
    "    print(f\"Working directory: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tensorflow==2.15.0\n",
    "!pip install -q scikit-learn pandas numpy opencv-python-headless\n",
    "!pip install -q matplotlib seaborn tqdm requests\n",
    "\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Training Modules\n",
    "\n",
    "Clone the modular Python training modules from your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not already cloned)\n",
    "REPO_URL = \"https://github.com/AliNikkhah2001/edgeWash.git\"\n",
    "REPO_DIR = Path(\"edgeWash\")\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    print(f\"Cloning repository from {REPO_URL}...\")\n",
    "    !git clone {REPO_URL}\n",
    "else:\n",
    "    print(f\"Repository already exists: {REPO_DIR}\")\n",
    "    print(\"Pulling latest changes...\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# Add training modules to Python path\n",
    "training_dir = REPO_DIR / \"training\"\n",
    "if str(training_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(training_dir))\n",
    "\n",
    "print(f\"Training modules path: {training_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "import config\n",
    "import download_datasets\n",
    "import preprocess_data\n",
    "import data_generators\n",
    "import models\n",
    "import train as train_module\n",
    "import evaluate\n",
    "\n",
    "print(\"Training modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration & Google Drive Paths\n",
    "\n",
    "View and customize training hyperparameters.\n",
    "\n",
    "**Important**: Checkpoints, logs, and models will be saved to Google Drive for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override config paths to save to Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    # Update paths to Google Drive\n",
    "    config.MODELS_DIR = Path(WORK_DIR) / 'models'\n",
    "    config.CHECKPOINTS_DIR = Path(WORK_DIR) / 'checkpoints'\n",
    "    config.LOGS_DIR = Path(WORK_DIR) / 'logs'\n",
    "    config.RESULTS_DIR = Path(WORK_DIR) / 'results'\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_path in [config.MODELS_DIR, config.CHECKPOINTS_DIR, config.LOGS_DIR, config.RESULTS_DIR]:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"‚úì Paths configured to save to Google Drive:\")\n",
    "    print(f\"  Models: {config.MODELS_DIR}\")\n",
    "    print(f\"  Checkpoints: {config.CHECKPOINTS_DIR}\")\n",
    "    print(f\"  Logs: {config.LOGS_DIR}\")\n",
    "    print(f\"  Results: {config.RESULTS_DIR}\")\n",
    "else:\n",
    "    print(\"Running locally - using default paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display configuration\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nImage size: {config.IMG_SIZE}\")\n",
    "print(f\"Sequence length: {config.SEQUENCE_LENGTH}\")\n",
    "print(f\"Number of classes: {config.NUM_CLASSES}\")\n",
    "print(f\"Class names: {config.CLASS_NAMES}\")\n",
    "\n",
    "print(f\"\\nBatch size: {config.BATCH_SIZE}\")\n",
    "print(f\"Epochs: {config.EPOCHS}\")\n",
    "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"Early stopping patience: {config.PATIENCE}\")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {config.TRAIN_RATIO*100:.0f}%\")\n",
    "print(f\"  Val:   {config.VAL_RATIO*100:.0f}%\")\n",
    "print(f\"  Test:  {config.TEST_RATIO*100:.0f}%\")\n",
    "\n",
    "print(f\"\\nAugmentation:\")\n",
    "for key, value in config.AUGMENTATION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nModel architectures available:\")\n",
    "for model_name, model_config in config.MODEL_CONFIGS.items():\n",
    "    print(f\"  - {model_name}: {model_config['name']}\")"
  ]
 },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Download\n",
    "\n",
    "Download Kaggle WHO6 dataset (~1 GB, quick start).\n",
    "\n",
    "For full pipeline, also download PSKUS (18 GB) and METC (2 GB) - see commented code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle WHO6 dataset\n",
    "print(\"Downloading Kaggle WHO6 dataset...\")\n",
    "success = download_datasets.download_kaggle_dataset()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n‚úì Kaggle dataset ready!\")\n",
    "else:\n",
    "    print(\"\\n‚úó Kaggle dataset download failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download PSKUS and METC datasets (large, requires zenodo-get)\n",
    "# Uncomment to download:\n",
    "\n",
    "# # Install zenodo-get\n",
    "# !pip install zenodo-get\n",
    "\n",
    "# # Download PSKUS (18 GB, ~30-60 minutes)\n",
    "# print(\"Downloading PSKUS Hospital dataset (18 GB)...\")\n",
    "# download_datasets.download_pskus_dataset()\n",
    "\n",
    "# # Download METC (2 GB, ~5-10 minutes)\n",
    "# print(\"Downloading METC Lab dataset (2 GB)...\")\n",
    "# download_datasets.download_metc_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify datasets\n",
    "status = download_datasets.verify_datasets()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name, info in status.items():\n",
    "    status_icon = \"‚úì\" if info['exists'] else \"‚úó\"\n",
    "    print(f\"{status_icon} {info['name']}: {info['num_files']} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "Extract frames from videos and create train/val/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Kaggle dataset\n",
    "print(\"Preprocessing Kaggle dataset...\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "result = preprocess_data.preprocess_all_datasets(\n",
    "    use_kaggle=True,\n",
    "    use_pskus=False,  # Set True if PSKUS downloaded\n",
    "    use_metc=False    # Set True if METC downloaded\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(\"\\n‚úì Preprocessing complete!\")\n",
    "    print(f\"\\nProcessed files:\")\n",
    "    for key, path in result.items():\n",
    "        print(f\"  {key}: {path}\")\n",
    "else:\n",
    "    print(\"\\n‚úó Preprocessing failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Exploration\n",
    "\n",
    "Visualize dataset statistics and sample frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "train_df = pd.read_csv(config.PROCESSED_DIR / 'train.csv')\n",
    "val_df = pd.read_csv(config.PROCESSED_DIR / 'val.csv')\n",
    "test_df = pd.read_csv(config.PROCESSED_DIR / 'test.csv')\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_df)} frames ({len(train_df['video_id'].unique())} videos)\")\n",
    "print(f\"  Val:   {len(val_df)} frames ({len(val_df['video_id'].unique())} videos)\")\n",
    "print(f\"  Test:  {len(test_df)} frames ({len(test_df['video_id'].unique())} videos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df, split_name) in enumerate([(train_df, 'Train'), (val_df, 'Val'), (test_df, 'Test')]):\n",
    "    class_counts = df['class_name'].value_counts()\n",
    "    \n",
    "    axes[idx].bar(range(len(class_counts)), class_counts.values)\n",
    "    axes[idx].set_title(f'{split_name} Set - Class Distribution', fontsize=12)\n",
    "    axes[idx].set_xlabel('Class', fontsize=10)\n",
    "    axes[idx].set_ylabel('Number of Frames', fontsize=10)\n",
    "    axes[idx].set_xticks(range(len(class_counts)))\n",
    "    axes[idx].set_xticklabels([cn.split('_')[-1] for cn in class_counts.index], rotation=45, ha='right')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train All Models\n",
    "\n",
    "Train all 3 model architectures sequentially:\n",
    "1. **MobileNetV2**: Frame-based classifier (fast inference)\n",
    "2. **LSTM**: Temporal sequence model (context-aware)\n",
    "3. **GRU**: Alternative temporal model (faster than LSTM)\n",
    "\n",
    "Best weights for each model will be saved to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 20  # Increase to 50 for production\n",
    "MODELS_TO_TRAIN = ['mobilenetv2', 'lstm', 'gru']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING PIPELINE: All 3 Models\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nModels: {', '.join([m.upper() for m in MODELS_TO_TRAIN])}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Checkpoints will be saved to: {config.CHECKPOINTS_DIR}\")\n",
    "print(f\"Final models will be saved to: {config.MODELS_DIR}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "training_results = {}\n",
    "\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(f\"TRAINING MODEL {MODELS_TO_TRAIN.index(model_type) + 1}/3: {model_type.upper()}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    # Adjust batch size for sequence models\n",
    "    batch_size = 32 if model_type == 'mobilenetv2' else 16\n",
    "    \n",
    "    # Train model\n",
    "    result = train_module.train_model(\n",
    "        model_type=model_type,\n",
    "        train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "        val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "        batch_size=batch_size,\n",
    "        epochs=EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    training_results[model_type] = result\n",
    "    \n",
    "    # Display summary\n",
    "    best_epoch = result['best_epoch']\n",
    "    best_val_acc = result['history']['val_accuracy'][best_epoch]\n",
    "    best_val_loss = result['history']['val_loss'][best_epoch]\n",
    "    \n",
    "    print(f\"\\n‚úì {model_type.upper()} training complete!\")\n",
    "    print(f\"  Best epoch: {best_epoch + 1}\")\n",
    "    print(f\"  Best val accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "    print(f\"  Final model saved: {result['final_model_path']}\")\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"ALL MODELS TRAINED SUCCESSFULLY\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Visualization\n",
    "\n",
    "Compare training curves across all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate All Models\n",
    "\n",
    "Evaluate all trained models on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "evaluation_results = {}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING ALL MODELS ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    print(f\"\\nEvaluating {model_type.upper()}...\")\n",
    "    \n",
    "    batch_size = 32 if model_type == 'mobilenetv2' else 16\n",
    "    \n",
    "    eval_results = evaluate.evaluate_model(\n",
    "        model_path=training_results[model_type]['final_model_path'],\n",
    "        test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "        model_type=model_type,\n",
    "        batch_size=batch_size,\n",
    "        save_results=True\n",
    "    )\n",
    "    \n",
    "    evaluation_results[model_type] = eval_results\n",
    "    \n",
    "    print(f\"‚úì {model_type.upper()} evaluation complete!\")\n",
    "    print(f\"  Accuracy: {eval_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score: {eval_results['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL EVALUATIONS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed metrics for each model\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    eval_results = evaluation_results[model_type]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{model_type.upper()} - TEST SET METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"  Accuracy:       {eval_results['accuracy']:.4f}\")\n",
    "    print(f\"  Top-2 Accuracy: {eval_results['top2_accuracy']:.4f}\")\n",
    "    print(f\"  Precision:      {eval_results['precision']:.4f}\")\n",
    "    print(f\"  Recall:         {eval_results['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:       {eval_results['f1_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class F1-Scores:\")\n",
    "    for class_name in config.CLASS_NAMES:\n",
    "        metrics = eval_results['per_class_metrics'][class_name]\n",
    "        print(f\"  {class_name}: {metrics['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorBoard\n",
    "\n",
    "Launch TensorBoard to view training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension (Jupyter/Colab)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%tensorboard --logdir {config.LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Comparison\n",
    "\n",
    "Compare all 3 models with comprehensive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison visualization\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING MODEL COMPARISON PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Call compare_models from evaluate module\n",
    "comparison_path = config.RESULTS_DIR / 'model_comparison.png'\n",
    "evaluate.compare_models(\n",
    "    evaluation_results,\n",
    "    save_path=comparison_path\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Comparison plot saved: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison plot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "if comparison_path.exists():\n",
    "    display(Image(filename=str(comparison_path)))\n",
    "else:\n",
    "    print(\"Comparison plot not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    eval_results = evaluation_results[model_type]\n",
    "    summary_data.append({\n",
    "        'Model': model_type.upper(),\n",
    "        'Accuracy': f\"{eval_results['accuracy']:.4f}\",\n",
    "        'Top-2 Acc': f\"{eval_results['top2_accuracy']:.4f}\",\n",
    "        'Precision': f\"{eval_results['precision']:.4f}\",\n",
    "        'Recall': f\"{eval_results['recall']:.4f}\",\n",
    "        'F1-Score': f\"{eval_results['f1_score']:.4f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_path = config.RESULTS_DIR / 'model_comparison_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\n‚úì Summary saved: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model\n",
    "best_model = max(evaluation_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "best_model_name = best_model[0]\n",
    "best_f1 = best_model[1]['f1_score']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüèÜ {best_model_name.upper()} achieved the highest F1-Score: {best_f1:.4f}\")\n",
    "print(f\"\\nAll metrics for {best_model_name.upper()}:\")\n",
    "for metric, value in best_model[1].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Saved Models & Checkpoints\n",
    "\n",
    "Summary of all saved model weights and checkpoints on Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display saved model paths\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVED MODEL WEIGHTS (Google Drive)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nModels directory: {config.MODELS_DIR}\")\n",
    "print(f\"Checkpoints directory: {config.CHECKPOINTS_DIR}\")\n",
    "\n",
    "print(\"\\nFinal Model Weights:\")\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    model_path = training_results[model_type]['final_model_path']\n",
    "    checkpoint_path = training_results[model_type]['best_checkpoint_path']\n",
    "    \n",
    "    print(f\"\\n{model_type.upper()}:\")\n",
    "    print(f\"  Final model: {model_path}\")\n",
    "    print(f\"  Best checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Check file size\n",
    "    if Path(model_path).exists():\n",
    "        size_mb = Path(model_path).stat().st_size / (1024 * 1024)\n",
    "        print(f\"  Model size: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All model weights are saved to Google Drive!\")\n",
    "print(\"They will persist even if Colab runtime disconnects.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary & Next Steps\n",
    "\n",
    "Complete training pipeline finished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TRAINING PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n‚úì Trained {len(MODELS_TO_TRAIN)} models: {', '.join([m.upper() for m in MODELS_TO_TRAIN])}\")\n",
    "print(f\"‚úì All models evaluated on test set\")\n",
    "print(f\"‚úì Best model: {best_model_name.upper()} (F1: {best_f1:.4f})\")\n",
    "\n",
    "print(f\"\\nResults saved to Google Drive:\")\n",
    "print(f\"  - Models: {config.MODELS_DIR}\")\n",
    "print(f\"  - Checkpoints: {config.CHECKPOINTS_DIR}\")\n",
    "print(f\"  - Logs: {config.LOGS_DIR}\")\n",
    "print(f\"  - Evaluation results: {config.RESULTS_DIR}\")\n",
    "print(f\"  - Comparison plot: {comparison_path}\")\n",
    "print(f\"  - Summary CSV: {summary_path}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Fine-tune best model with more epochs (50+)\")\n",
    "print(\"  2. Download larger datasets (PSKUS, METC) for better accuracy\")\n",
    "print(\"  3. Experiment with different augmentation strategies\")\n",
    "print(\"  4. Analyze per-class performance and address weak classes\")\n",
    "print(\"  5. Export best model to TFLite for mobile deployment\")\n",
    "print(\"  6. Create inference demo with real-time video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%tensorboard --logdir {config.LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optional: Train Additional Models\n",
    "\n",
    "Train LSTM or GRU models for temporal modeling (requires sequence data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train LSTM model\n",
    "\n",
    "# lstm_result = train_module.train_model(\n",
    "#     model_type='lstm',\n",
    "#     train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "#     val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "#     batch_size=16,  # Reduce batch size for sequence models\n",
    "#     epochs=20,\n",
    "#     learning_rate=config.LEARNING_RATE\n",
    "# )\n",
    "\n",
    "# print(\"\\n‚úì LSTM training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train GRU model\n",
    "\n",
    "# gru_result = train_module.train_model(\n",
    "#     model_type='gru',\n",
    "#     train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "#     val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "#     batch_size=16,\n",
    "#     epochs=20,\n",
    "#     learning_rate=config.LEARNING_RATE\n",
    "# )\n",
    "\n",
    "# print(\"\\n‚úì GRU training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Comparison\n",
    "\n",
    "Compare multiple models (if trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare MobileNetV2, LSTM, GRU\n",
    "# Uncomment if you trained multiple models\n",
    "\n",
    "# model_results = {\n",
    "#     'MobileNetV2': eval_results,\n",
    "#     'LSTM': evaluate.evaluate_model(\n",
    "#         model_path=str(config.MODELS_DIR / 'lstm_final.keras'),\n",
    "#         test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "#         model_type='lstm',\n",
    "#         batch_size=16,\n",
    "#         save_results=True\n",
    "#     ),\n",
    "#     'GRU': evaluate.evaluate_model(\n",
    "#         model_path=str(config.MODELS_DIR / 'gru_final.keras'),\n",
    "#         test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "#         model_type='gru',\n",
    "#         batch_size=16,\n",
    "#         save_results=True\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# # Create comparison plot\n",
    "# evaluate.compare_models(\n",
    "#     model_results,\n",
    "#     save_path=config.RESULTS_DIR / 'model_comparison.png'\n",
    "# )\n",
    "\n",
    "# display(Image(filename=str(config.RESULTS_DIR / 'model_comparison.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary & Next Steps\n",
    "\n",
    "Training pipeline complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TRAINING PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTrained model: {MODEL_TYPE}\")\n",
    "print(f\"Model saved: {result['final_model_path']}\")\n",
    "print(f\"\\nTest Accuracy: {eval_results['accuracy']:.4f}\")\n",
    "print(f\"Test F1-Score: {eval_results['f1_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nResults saved to:\")\n",
    "print(f\"  - Confusion matrix: {config.RESULTS_DIR / MODEL_TYPE / 'confusion_matrix.png'}\")\n",
    "print(f\"  - Classification report: {config.RESULTS_DIR / MODEL_TYPE / 'classification_report.txt'}\")\n",
    "print(f\"  - Metrics CSV: {config.RESULTS_DIR / MODEL_TYPE / 'metrics.csv'}\")\n",
    "\n",
    "print(f\"\\nTensorBoard logs: {config.LOGS_DIR}\")\n",
    "print(f\"Checkpoints: {config.CHECKPOINTS_DIR}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Fine-tune model with more epochs (50+)\")\n",
    "print(\"  2. Train temporal models (LSTM/GRU) for sequence modeling\")\n",
    "print(\"  3. Download larger datasets (PSKUS, METC) for better accuracy\")\n",
    "print(\"  4. Experiment with different augmentation strategies\")\n",
    "print(\"  5. Export model to TFLite for mobile deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
