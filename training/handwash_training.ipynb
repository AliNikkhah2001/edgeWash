{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Handwashing Detection Training Pipeline\n",
    "\n",
    "**Complete training pipeline using modular Python modules**\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Dataset download (Kaggle WHO6)\n",
    "2. Data preprocessing (frame extraction)\n",
    "3. Model training (MobileNetV2)\n",
    "4. Evaluation and visualization\n",
    "5. Model comparison\n",
    "\n",
    "**Runtime**: GPU (recommended for training)\n",
    "\n",
    "**Expected Duration**: 2-3 hours for complete pipeline\n",
    "\n",
    "**Author**: Generated with AdaL (https://github.com/sylphai/adal-cli)\n",
    "\n",
    "**Date**: 2025-12-31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set working directory\n",
    "    import os\n",
    "    WORK_DIR = '/content/drive/MyDrive/handwash_training'\n",
    "    os.makedirs(WORK_DIR, exist_ok=True)\n",
    "    %cd {WORK_DIR}\n",
    "else:\n",
    "    WORK_DIR = '.'\n",
    "    print(f\"Working directory: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (TensorFlow preinstalled on Colab)\n",
    "!pip install -q scikit-learn pandas numpy opencv-python-headless\n",
    "!pip install -q matplotlib seaborn tqdm requests nbformat\n",
    "\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU availability\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Training Modules\n",
    "\n",
    "Clone the modular Python training modules from your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not already cloned)\n",
    "REPO_URL = \"https://github.com/AliNikkhah2001/edgeWash.git\"\n",
    "REPO_DIR = Path(\"edgeWash\")\n",
    "\n",
    "if not REPO_DIR.exists():\n",
    "    print(f\"Cloning repository from {REPO_URL}...\")\n",
    "    !git clone {REPO_URL}\n",
    "else:\n",
    "    print(f\"Repository already exists: {REPO_DIR}\")\n",
    "    print(\"Pulling latest changes...\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# Add training modules to Python path\n",
    "training_dir = REPO_DIR / \"training\"\n",
    "if str(training_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(training_dir))\n",
    "\n",
    "print(f\"Training modules path: {training_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "import config\n",
    "import download_datasets\n",
    "import preprocess_data\n",
    "import data_generators\n",
    "import models\n",
    "import train as train_module\n",
    "import evaluate\n",
    "\n",
    "print(\"Training modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration & Google Drive Paths\n",
    "\n",
    "View and customize training hyperparameters.\n",
    "\n",
    "**Important**: Checkpoints, logs, and models will be saved to Google Drive for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override config paths to save to Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    # Update paths to Google Drive\n",
    "    config.WORK_DIR = Path(WORK_DIR)\n",
    "    config.DATA_DIR = config.WORK_DIR / 'datasets'\n",
    "    config.RAW_DIR = config.DATA_DIR / 'raw'\n",
    "    config.PROCESSED_DIR = config.DATA_DIR / 'processed'\n",
    "    config.MODELS_DIR = config.WORK_DIR / 'models'\n",
    "    config.CHECKPOINTS_DIR = config.WORK_DIR / 'checkpoints'\n",
    "    config.LOGS_DIR = config.WORK_DIR / 'logs'\n",
    "    config.RESULTS_DIR = config.WORK_DIR / 'results'\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_path in [config.DATA_DIR, config.RAW_DIR, config.PROCESSED_DIR, config.MODELS_DIR, config.CHECKPOINTS_DIR, config.LOGS_DIR, config.RESULTS_DIR]:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\u2713 Paths configured to save to Google Drive:\")\n",
    "    print(f\"  Data: {config.DATA_DIR}\")\n",
    "    print(f\"  Models: {config.MODELS_DIR}\")\n",
    "    print(f\"  Checkpoints: {config.CHECKPOINTS_DIR}\")\n",
    "    print(f\"  Logs: {config.LOGS_DIR}\")\n",
    "    print(f\"  Results: {config.RESULTS_DIR}\")\n",
    "else:\n",
    "    print(\"Running locally - using default paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display configuration\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nImage size: {config.IMG_SIZE}\")\n",
    "print(f\"Sequence length: {config.SEQUENCE_LENGTH}\")\n",
    "print(f\"Number of classes: {config.NUM_CLASSES}\")\n",
    "print(f\"Class names: {config.CLASS_NAMES}\")\n",
    "\n",
    "print(f\"\\nBatch size: {config.BATCH_SIZE}\")\n",
    "print(f\"Epochs: {config.EPOCHS}\")\n",
    "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"Early stopping patience: {config.PATIENCE}\")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {config.TRAIN_RATIO*100:.0f}%\")\n",
    "print(f\"  Val:   {config.VAL_RATIO*100:.0f}%\")\n",
    "print(f\"  Test:  {config.TEST_RATIO*100:.0f}%\")\n",
    "\n",
    "print(f\"\\nAugmentation:\")\n",
    "for key, value in config.AUGMENTATION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nModel architectures available:\")\n",
    "for model_name, model_config in config.MODEL_CONFIGS.items():\n",
    "    print(f\"  - {model_name}: {model_config['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Download\n",
    "\n",
    "Download Kaggle WHO6 dataset (~1 GB, quick start).\n",
    "\n",
    "For full pipeline, also download PSKUS (18 GB) and METC (2 GB) - see commented code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle WHO6 dataset\n",
    "print(\"Downloading Kaggle WHO6 dataset...\")\n",
    "success = download_datasets.download_kaggle_dataset()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n\u2713 Kaggle dataset ready!\")\n",
    "else:\n",
    "    print(\"\\n\u2717 Kaggle dataset download failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download PSKUS and METC datasets (large, requires zenodo-get)\n",
    "# Uncomment to download:\n",
    "\n",
    "# # Install zenodo-get\n",
    "# !pip install zenodo-get\n",
    "\n",
    "# # Download PSKUS (18 GB, ~30-60 minutes)\n",
    "# print(\"Downloading PSKUS Hospital dataset (18 GB)...\")\n",
    "# download_datasets.download_pskus_dataset()\n",
    "\n",
    "# # Download METC (2 GB, ~5-10 minutes)\n",
    "# print(\"Downloading METC Lab dataset (2 GB)...\")\n",
    "# download_datasets.download_metc_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify datasets\n",
    "status = download_datasets.verify_datasets()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name, info in status.items():\n",
    "    status_icon = \"\u2713\" if info['exists'] else \"\u2717\"\n",
    "    print(f\"{status_icon} {info['name']}: {info['num_files']} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "Extract frames from videos and create train/val/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Kaggle dataset\n",
    "print(\"Preprocessing Kaggle dataset...\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "result = preprocess_data.preprocess_all_datasets(\n",
    "    use_kaggle=True,\n",
    "    use_pskus=False,  # Set True if PSKUS downloaded\n",
    "    use_metc=False    # Set True if METC downloaded\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(\"\\n\u2713 Preprocessing complete!\")\n",
    "    print(f\"\\nProcessed files:\")\n",
    "    for key, path in result.items():\n",
    "        print(f\"  {key}: {path}\")\n",
    "else:\n",
    "    print(\"\\n\u2717 Preprocessing failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Exploration\n",
    "\n",
    "Visualize dataset statistics and sample frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "train_df = pd.read_csv(config.PROCESSED_DIR / 'train.csv')\n",
    "val_df = pd.read_csv(config.PROCESSED_DIR / 'val.csv')\n",
    "test_df = pd.read_csv(config.PROCESSED_DIR / 'test.csv')\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_df)} frames ({len(train_df['video_id'].unique())} videos)\")\n",
    "print(f\"  Val:   {len(val_df)} frames ({len(val_df['video_id'].unique())} videos)\")\n",
    "print(f\"  Test:  {len(test_df)} frames ({len(test_df['video_id'].unique())} videos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (df, split_name) in enumerate([(train_df, 'Train'), (val_df, 'Val'), (test_df, 'Test')]):\n",
    "    class_counts = df['class_name'].value_counts()\n",
    "    \n",
    "    axes[idx].bar(range(len(class_counts)), class_counts.values)\n",
    "    axes[idx].set_title(f'{split_name} Set - Class Distribution', fontsize=12)\n",
    "    axes[idx].set_xlabel('Class', fontsize=10)\n",
    "    axes[idx].set_ylabel('Number of Frames', fontsize=10)\n",
    "    axes[idx].set_xticks(range(len(class_counts)))\n",
    "    axes[idx].set_xticklabels([cn.split('_')[-1] for cn in class_counts.index], rotation=45, ha='right')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Options (2D, 3D, and temporal)\n",
    "\n",
    "Pick a backbone: frame-based (MobileNetV2/ResNet50/EfficientNetB0) or temporal (LSTM/GRU/3D CNN).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "AVAILABLE_MODELS = ['mobilenetv2', 'resnet50', 'efficientnetb0', 'lstm', 'gru', '3d_cnn']\n",
    "FRAME_BACKBONES = ['mobilenetv2', 'resnet50', 'efficientnetb0']\n",
    "TEMPORAL_BACKBONES = ['lstm', 'gru', '3d_cnn']\n",
    "print('Available models:', ', '.join(AVAILABLE_MODELS))\n",
    "print('Frame models:', ', '.join(FRAME_BACKBONES))\n",
    "print('Temporal models:', ', '.join(TEMPORAL_BACKBONES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling frequency (frame skip)\n",
    "\n",
    "Set how many frames to skip when extracting. If you change this, rerun preprocessing so splits update.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "FRAME_SKIP_TO_USE = 2  # e.g., 1=all frames, 2=every other, 4=every 4th\n",
    "config.FRAME_SKIP = FRAME_SKIP_TO_USE\n",
    "print('Frame skip set to', config.FRAME_SKIP)\n",
    "print('Available presets:', getattr(config, 'FRAME_SKIP_OPTIONS', [1,2,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live TensorBoard (start before training)\n",
    "\n",
    "Starts TensorBoard so you can watch training live in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {config.LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Selected Models\n",
    "\n",
    "Train any combination of models (2D frame, temporal, 3D CNN). Use the config cell below to pick them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 10  # adjust as needed\n",
    "MODELS_TO_TRAIN = ['mobilenetv2', 'resnet50', 'efficientnetb0', 'lstm', 'gru', '3d_cnn']\n",
    "FRAME_SKIP_USED = config.FRAME_SKIP\n",
    "\n",
    "print('=' * 80)\n",
    "print('TRAINING PIPELINE')\n",
    "print('=' * 80)\n",
    "print('\nModels: ' + ', '.join([m.upper() for m in MODELS_TO_TRAIN]))\n",
    "print(f'Epochs: {EPOCHS}')\n",
    "print(f'Frame skip: {FRAME_SKIP_USED}')\n",
    "print(f'Checkpoints will be saved to: {config.CHECKPOINTS_DIR}')\n",
    "print(f'Final models will be saved to: {config.MODELS_DIR}')\n",
    "print('\n' + '=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "training_results = {}\n",
    "total_models = len(MODELS_TO_TRAIN)\n",
    "\n",
    "for idx, model_type in enumerate(MODELS_TO_TRAIN, start=1):\n",
    "    print('\n' + '='*80)\n",
    "    print(f'TRAINING MODEL {idx}/{total_models}: {model_type.upper()}')\n",
    "    print('='*80)\n",
    "    \n",
    "    if model_type in FRAME_BACKBONES:\n",
    "        batch_size = 32\n",
    "    elif model_type == \"3d_cnn\":\n",
    "        batch_size = 12\n",
    "    else:\n",
    "        batch_size = 16\n",
    "    \n",
    "    result = train_module.train_model(\n",
    "        model_type=model_type,\n",
    "        train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "        val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "        batch_size=batch_size,\n",
    "        epochs=EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE\n",
    "    )\n",
    "    training_results[model_type] = result\n",
    "    \n",
    "    best_epoch = result[\"best_epoch\"]\n",
    "    best_val_acc = result[\"history\"][\"val_accuracy\"][best_epoch]\n",
    "    best_val_loss = result[\"history\"][\"val_loss\"][best_epoch]\n",
    "    \n",
    "    print(f'\\n\u2713 {model_type.upper()} training complete!')\n",
    "    print(f'  Best epoch: {best_epoch + 1}')\n",
    "    print(f'  Best val accuracy: {best_val_acc:.4f}')\n",
    "    print(f'  Best val loss: {best_val_loss:.4f}')\n",
    "    print('  Final model saved: {}'.format(result['final_model_path']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Visualization\n",
    "\n",
    "Compare training curves across all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate All Models\n",
    "\n",
    "Evaluate all trained models on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "evaluation_results = {}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING ALL MODELS ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    print(f\"\\nEvaluating {model_type.upper()}...\")\n",
    "    \n",
    "    batch_size = 32 if model_type == 'mobilenetv2' else 16\n",
    "    \n",
    "    eval_results = evaluate.evaluate_model(\n",
    "        model_path=training_results[model_type]['final_model_path'],\n",
    "        test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "        model_type=model_type,\n",
    "        batch_size=batch_size,\n",
    "        save_results=True\n",
    "    )\n",
    "    \n",
    "    evaluation_results[model_type] = eval_results\n",
    "    \n",
    "    print(f\"\u2713 {model_type.upper()} evaluation complete!\")\n",
    "    print(f\"  Accuracy: {eval_results['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score: {eval_results['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL EVALUATIONS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed metrics for each model\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    eval_results = evaluation_results[model_type]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{model_type.upper()} - TEST SET METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"  Accuracy:       {eval_results['accuracy']:.4f}\")\n",
    "    print(f\"  Top-2 Accuracy: {eval_results['top2_accuracy']:.4f}\")\n",
    "    print(f\"  Precision:      {eval_results['precision']:.4f}\")\n",
    "    print(f\"  Recall:         {eval_results['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:       {eval_results['f1_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class F1-Scores:\")\n",
    "    for class_name in config.CLASS_NAMES:\n",
    "        metrics = eval_results['per_class_metrics'][class_name]\n",
    "        print(f\"  {class_name}: {metrics['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TensorBoard\n",
    "\n",
    "Launch TensorBoard to view training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension (Jupyter/Colab)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%tensorboard --logdir {config.LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Comparison\n",
    "\n",
    "Compare all 3 models with comprehensive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison visualization\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING MODEL COMPARISON PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Call compare_models from evaluate module\n",
    "comparison_path = config.RESULTS_DIR / 'model_comparison.png'\n",
    "evaluate.compare_models(\n",
    "    evaluation_results,\n",
    "    save_path=comparison_path\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Comparison plot saved: {comparison_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison plot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "if comparison_path.exists():\n",
    "    display(Image(filename=str(comparison_path)))\n",
    "else:\n",
    "    print(\"Comparison plot not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    eval_results = evaluation_results[model_type]\n",
    "    summary_data.append({\n",
    "        'Model': model_type.upper(),\n",
    "        'Accuracy': f\"{eval_results['accuracy']:.4f}\",\n",
    "        'Top-2 Acc': f\"{eval_results['top2_accuracy']:.4f}\",\n",
    "        'Precision': f\"{eval_results['precision']:.4f}\",\n",
    "        'Recall': f\"{eval_results['recall']:.4f}\",\n",
    "        'F1-Score': f\"{eval_results['f1_score']:.4f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_path = config.RESULTS_DIR / 'model_comparison_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"\\n\u2713 Summary saved: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model\n",
    "best_model = max(evaluation_results.items(), key=lambda x: x[1]['f1_score'])\n",
    "best_model_name = best_model[0]\n",
    "best_f1 = best_model[1]['f1_score']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n\ud83c\udfc6 {best_model_name.upper()} achieved the highest F1-Score: {best_f1:.4f}\")\n",
    "print(f\"\\nAll metrics for {best_model_name.upper()}:\")\n",
    "for metric, value in best_model[1].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Saved Models & Checkpoints\n",
    "\n",
    "Summary of all saved model weights and checkpoints on Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display saved model paths\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVED MODEL WEIGHTS (Google Drive)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nModels directory: {config.MODELS_DIR}\")\n",
    "print(f\"Checkpoints directory: {config.CHECKPOINTS_DIR}\")\n",
    "\n",
    "print(\"\\nFinal Model Weights:\")\n",
    "for model_type in MODELS_TO_TRAIN:\n",
    "    model_path = training_results[model_type]['final_model_path']\n",
    "    checkpoint_path = training_results[model_type]['best_checkpoint_path']\n",
    "    \n",
    "    print(f\"\\n{model_type.upper()}:\")\n",
    "    print(f\"  Final model: {model_path}\")\n",
    "    print(f\"  Best checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Check file size\n",
    "    if Path(model_path).exists():\n",
    "        size_mb = Path(model_path).stat().st_size / (1024 * 1024)\n",
    "        print(f\"  Model size: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All model weights are saved to Google Drive!\")\n",
    "print(\"They will persist even if Colab runtime disconnects.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary & Next Steps\n",
    "\n",
    "Complete training pipeline finished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 10  # adjust as needed\n",
    "MODELS_TO_TRAIN = ['mobilenetv2', 'resnet50', 'efficientnetb0', 'lstm', 'gru', '3d_cnn']\n",
    "FRAME_SKIP_USED = config.FRAME_SKIP\n",
    "\n",
    "print('=' * 80)\n",
    "print('TRAINING PIPELINE')\n",
    "print('=' * 80)\n",
    "print('\nModels: ' + ', '.join([m.upper() for m in MODELS_TO_TRAIN]))\n",
    "print(f'Epochs: {EPOCHS}')\n",
    "print(f'Frame skip: {FRAME_SKIP_USED}')\n",
    "print(f'Checkpoints will be saved to: {config.CHECKPOINTS_DIR}')\n",
    "print(f'Final models will be saved to: {config.MODELS_DIR}')\n",
    "print('\n' + '=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%tensorboard --logdir {config.LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optional: Train Additional Models\n",
    "\n",
    "Train LSTM or GRU models for temporal modeling (requires sequence data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train LSTM model\n",
    "\n",
    "# lstm_result = train_module.train_model(\n",
    "#     model_type='lstm',\n",
    "#     train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "#     val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "#     batch_size=16,  # Reduce batch size for sequence models\n",
    "#     epochs=20,\n",
    "#     learning_rate=config.LEARNING_RATE\n",
    "# )\n",
    "\n",
    "# print(\"\\n\u2713 LSTM training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train GRU model\n",
    "\n",
    "# gru_result = train_module.train_model(\n",
    "#     model_type='gru',\n",
    "#     train_csv=config.PROCESSED_DIR / 'train.csv',\n",
    "#     val_csv=config.PROCESSED_DIR / 'val.csv',\n",
    "#     batch_size=16,\n",
    "#     epochs=20,\n",
    "#     learning_rate=config.LEARNING_RATE\n",
    "# )\n",
    "\n",
    "# print(\"\\n\u2713 GRU training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Comparison\n",
    "\n",
    "Compare multiple models (if trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare MobileNetV2, LSTM, GRU\n",
    "# Uncomment if you trained multiple models\n",
    "\n",
    "# model_results = {\n",
    "#     'MobileNetV2': eval_results,\n",
    "#     'LSTM': evaluate.evaluate_model(\n",
    "#         model_path=str(config.MODELS_DIR / 'lstm_final.keras'),\n",
    "#         test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "#         model_type='lstm',\n",
    "#         batch_size=16,\n",
    "#         save_results=True\n",
    "#     ),\n",
    "#     'GRU': evaluate.evaluate_model(\n",
    "#         model_path=str(config.MODELS_DIR / 'gru_final.keras'),\n",
    "#         test_csv=config.PROCESSED_DIR / 'test.csv',\n",
    "#         model_type='gru',\n",
    "#         batch_size=16,\n",
    "#         save_results=True\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# # Create comparison plot\n",
    "# evaluate.compare_models(\n",
    "#     model_results,\n",
    "#     save_path=config.RESULTS_DIR / 'model_comparison.png'\n",
    "# )\n",
    "\n",
    "# display(Image(filename=str(config.RESULTS_DIR / 'model_comparison.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary & Next Steps\n",
    "\n",
    "Training pipeline complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 10  # adjust as needed\n",
    "MODELS_TO_TRAIN = ['mobilenetv2', 'resnet50', 'efficientnetb0', 'lstm', 'gru', '3d_cnn']\n",
    "FRAME_SKIP_USED = config.FRAME_SKIP\n",
    "\n",
    "print('=' * 80)\n",
    "print('TRAINING PIPELINE')\n",
    "print('=' * 80)\n",
    "print('\nModels: ' + ', '.join([m.upper() for m in MODELS_TO_TRAIN]))\n",
    "print(f'Epochs: {EPOCHS}')\n",
    "print(f'Frame skip: {FRAME_SKIP_USED}')\n",
    "print(f'Checkpoints will be saved to: {config.CHECKPOINTS_DIR}')\n",
    "print(f'Final models will be saved to: {config.MODELS_DIR}')\n",
    "print('\n' + '=' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}