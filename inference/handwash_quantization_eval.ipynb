{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Handwash quantization + evaluation (MobileNetV2 + LSTM)\n\nThis notebook:\n- loads trained MobileNetV2 and LSTM/GRU handwash models\n- exports post-training quantized TFLite models\n- evaluates float vs quantized models on Kaggle WHO6\n- saves TFLite outputs\n- summarizes Axis Model Zoo compatibility notes\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "%pip install -q numpy pandas opencv-python tensorflow matplotlib scikit-learn tqdm requests seaborn\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os\nimport sys\nimport math\nimport time\nimport random\nimport tarfile\nfrom pathlib import Path\nimport importlib.util\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport requests\n\n\ndef find_repo_root(start=None):\n    start = Path.cwd() if start is None else Path(start)\n    for parent in [start] + list(start.parents):\n        if (parent / \"inference\" / \"config.py\").exists() or (parent / \"training\" / \"config.py\").exists():\n            return parent\n    return start\n\n\ndef _load_module(path: Path, name: str):\n    spec = importlib.util.spec_from_file_location(name, path)\n    if spec is None or spec.loader is None:\n        raise ImportError(f\"Cannot load module from {path}\")\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    return module\n\n\nREPO_ROOT = find_repo_root()\nINFERENCE_DIR = REPO_ROOT / \"inference\"\nTRAINING_DIR = REPO_ROOT / \"training\"\n\nif (INFERENCE_DIR / \"config.py\").exists():\n    cfg = _load_module(INFERENCE_DIR / \"config.py\", \"cfg\")\nelif (TRAINING_DIR / \"config.py\").exists():\n    cfg = _load_module(TRAINING_DIR / \"config.py\", \"cfg\")\nelse:\n    raise FileNotFoundError(\"config.py not found in inference/ or training/\")\n\nnp.random.seed(cfg.RANDOM_SEED)\nrandom.seed(cfg.RANDOM_SEED)\n\ndef _seed_tf(seed):\n    try:\n        tf.random.set_seed(seed)\n    except Exception:\n        pass\n\n_seed_tf(cfg.RANDOM_SEED)\n\nRAW_DIR = REPO_ROOT / \"datasets\" / \"raw\"\nPROCESSED_DIR = REPO_ROOT / \"datasets\" / \"processed\"\nOUTPUT_DIR = INFERENCE_DIR / \"outputs\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"Repo root:\", REPO_ROOT)\nprint(\"Output dir:\", OUTPUT_DIR)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Download Kaggle WHO6 dataset (if needed)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "KAGGLE_URL = cfg.DATASETS[\"kaggle\"][\"url\"]\nKAGGLE_DIR = RAW_DIR / \"kaggle\"\nKAGGLE_TAR = KAGGLE_DIR / \"kaggle-dataset-6classes.tar\"\nKAGGLE_EXTRACTED = KAGGLE_DIR / \"kaggle-dataset-6classes\"\n\n\ndef download_with_progress(url, dest: Path):\n    dest.parent.mkdir(parents=True, exist_ok=True)\n    if dest.exists():\n        print(\"skip\", dest)\n        return\n    with requests.get(url, stream=True, timeout=30) as r:\n        r.raise_for_status()\n        total = int(r.headers.get(\"content-length\", 0))\n        with open(dest, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dest.name) as pbar:\n            for chunk in r.iter_content(chunk_size=1024 * 1024):\n                if not chunk:\n                    continue\n                f.write(chunk)\n                pbar.update(len(chunk))\n\n\ndef extract_tar(tar_path: Path, extract_root: Path):\n    extract_root.mkdir(parents=True, exist_ok=True)\n    with tarfile.open(tar_path, \"r\") as tar:\n        tar.extractall(path=extract_root)\n\n\nif not KAGGLE_EXTRACTED.exists():\n    print(\"Downloading Kaggle WHO6...\")\n    download_with_progress(KAGGLE_URL, KAGGLE_TAR)\n    print(\"Extracting...\")\n    extract_tar(KAGGLE_TAR, KAGGLE_DIR)\nelse:\n    print(\"Kaggle dataset already extracted:\", KAGGLE_EXTRACTED)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Index videos and create splits\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "VIDEO_EXTS = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n\n\ndef kaggle_class_id_from_folder(name: str) -> int:\n    name_lower = name.lower()\n    if name_lower in cfg.KAGGLE_CLASS_MAPPING:\n        return int(cfg.KAGGLE_CLASS_MAPPING[name_lower])\n    digits = \"\".join(ch for ch in name_lower if ch.isdigit())\n    if digits:\n        class_id = int(digits)\n        if 0 <= class_id < len(cfg.CLASS_NAMES):\n            return class_id\n    raise ValueError(f\"Unknown Kaggle class folder: {name}\")\n\n\ndef collect_videos(dataset_root: Path) -> pd.DataFrame:\n    rows = []\n    for class_dir in sorted(dataset_root.iterdir()):\n        if not class_dir.is_dir():\n            continue\n        class_id = kaggle_class_id_from_folder(class_dir.name)\n        for vid in class_dir.iterdir():\n            if vid.suffix.lower() not in VIDEO_EXTS:\n                continue\n            rows.append({\"video_path\": str(vid), \"class_id\": class_id})\n    df = pd.DataFrame(rows)\n    if df.empty:\n        raise RuntimeError(f\"No videos found in {dataset_root}\")\n    return df\n\n\nvideos_df = collect_videos(KAGGLE_EXTRACTED)\nprint(\"Total videos:\", len(videos_df))\n\ntrain_df, temp_df = train_test_split(\n    videos_df,\n    test_size=(cfg.VAL_RATIO + cfg.TEST_RATIO),\n    stratify=videos_df[\"class_id\"],\n    random_state=cfg.RANDOM_SEED,\n)\nval_size = cfg.VAL_RATIO / (cfg.VAL_RATIO + cfg.TEST_RATIO)\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=(1.0 - val_size),\n    stratify=temp_df[\"class_id\"],\n    random_state=cfg.RANDOM_SEED,\n)\n\nprint(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Frame and sequence sampling helpers\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_v2_preprocess\n\n\nclass MobileNetPreprocessingLayer(tf.keras.layers.Layer):\n    def call(self, x):\n        return (x / 127.5) - 1.0\n\n\nCUSTOM_OBJECTS = {\n    \"MobileNetPreprocessingLayer\": MobileNetPreprocessingLayer,\n    \"preprocess_input\": mobilenet_v2_preprocess,\n}\n\n\ndef iter_video_frames(video_path: str, stride: int = 2, max_frames=None):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    idx = 0\n    while True:\n        ok, frame = cap.read()\n        if not ok:\n            break\n        if idx % stride == 0:\n            frames.append(frame)\n            if max_frames is not None and len(frames) >= max_frames:\n                break\n        idx += 1\n    cap.release()\n    return frames\n\n\ndef resize_and_preprocess(frame_bgr, size):\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    frame_rgb = cv2.resize(frame_rgb, size)\n    frame_rgb = frame_rgb.astype(np.float32)\n    return mobilenet_v2_preprocess(frame_rgb)\n\n\ndef build_frame_dataset(df, size, frame_stride=2, max_frames_per_video=8, max_videos=None):\n    xs, ys = [], []\n    for i, row in enumerate(df.itertuples(index=False)):\n        if max_videos is not None and i >= max_videos:\n            break\n        frames = iter_video_frames(row.video_path, stride=frame_stride, max_frames=max_frames_per_video)\n        for frame in frames:\n            xs.append(resize_and_preprocess(frame, size))\n            ys.append(row.class_id)\n    if not xs:\n        raise RuntimeError(\"No frames collected for evaluation\")\n    return np.stack(xs), np.array(ys, dtype=np.int64)\n\n\ndef build_sequence_dataset(df, size, sequence_length=16, frame_stride=2, max_sequences_per_video=4, max_videos=None):\n    xs, ys = [], []\n    for i, row in enumerate(df.itertuples(index=False)):\n        if max_videos is not None and i >= max_videos:\n            break\n        max_frames = sequence_length * max_sequences_per_video\n        frames = iter_video_frames(row.video_path, stride=frame_stride, max_frames=max_frames)\n        if len(frames) < sequence_length:\n            continue\n        seq_count = 0\n        for start in range(0, len(frames) - sequence_length + 1, sequence_length):\n            seq_frames = frames[start : start + sequence_length]\n            seq = np.stack([resize_and_preprocess(f, size) for f in seq_frames])\n            xs.append(seq)\n            ys.append(row.class_id)\n            seq_count += 1\n            if seq_count >= max_sequences_per_video:\n                break\n    if not xs:\n        raise RuntimeError(\"No sequences collected for evaluation\")\n    return np.stack(xs), np.array(ys, dtype=np.int64)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## MobileNetV2: load model + evaluate float\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "MOBILENET_MODEL_NAME = \"mobilenetv2_final.keras\"\n\n\ndef resolve_model_path(name: str) -> Path:\n    candidates = [\n        INFERENCE_DIR / name,\n        REPO_ROOT / \"models\" / name,\n        REPO_ROOT / name,\n    ]\n    runs_dir = REPO_ROOT / \"Runs\"\n    if runs_dir.exists():\n        candidates.extend(sorted(runs_dir.rglob(name), key=lambda p: p.stat().st_mtime, reverse=True))\n    for cand in candidates:\n        if cand.exists():\n            return cand\n    return Path(name)\n\n\nmobilenet_path = resolve_model_path(MOBILENET_MODEL_NAME)\nprint(\"MobileNet model path:\", mobilenet_path)\n\nmobilenet_model = tf.keras.models.load_model(\n    mobilenet_path,\n    custom_objects=CUSTOM_OBJECTS,\n    compile=False,\n    safe_mode=False,\n)\n\ninput_shape = mobilenet_model.input_shape\nif isinstance(input_shape, list):\n    input_shape = input_shape[0]\n\nif input_shape[1] is None or input_shape[2] is None:\n    input_size = cfg.IMG_SIZE\nelse:\n    input_size = (input_shape[2], input_shape[1])\nprint(\"MobileNet input size:\", input_size)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "FRAME_STRIDE = 2\nMAX_FRAMES_PER_VIDEO = 8\nMAX_TEST_VIDEOS = 60\n\nX_frames, y_frames = build_frame_dataset(\n    test_df,\n    size=input_size,\n    frame_stride=FRAME_STRIDE,\n    max_frames_per_video=MAX_FRAMES_PER_VIDEO,\n    max_videos=MAX_TEST_VIDEOS,\n)\n\nprint(\"Frame samples:\", X_frames.shape)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from sklearn.metrics import accuracy_score\n\n\ndef evaluate_keras(model, xs, ys, batch_size=32):\n    start = time.perf_counter()\n    preds = model.predict(xs, batch_size=batch_size, verbose=0)\n    elapsed = time.perf_counter() - start\n    y_pred = np.argmax(preds, axis=1)\n    acc = accuracy_score(ys, y_pred)\n    ms_per = (elapsed / len(ys)) * 1000.0\n    return {\"accuracy\": acc, \"ms_per_sample\": ms_per}\n\n\nmobilenet_float_metrics = evaluate_keras(mobilenet_model, X_frames, y_frames)\nprint(\"MobileNet float metrics:\", mobilenet_float_metrics)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## MobileNetV2: post-training int8 quantization\n\nSet `DISABLE_PER_CHANNEL=True` for ARTPEC-8 style per-tensor quantization.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from typing import Iterable\n\nDISABLE_PER_CHANNEL = False\n\n\ndef representative_dataset(xs, max_samples=200) -> Iterable[list[np.ndarray]]:\n    count = min(len(xs), max_samples)\n    for i in range(count):\n        yield [xs[i : i + 1].astype(np.float32)]\n\n\ndef convert_int8_tflite(model, xs, output_path: Path, allow_fallback=False, disable_per_channel=False):\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = lambda: representative_dataset(xs)\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n    converter.inference_input_type = tf.int8\n    converter.inference_output_type = tf.int8\n    if disable_per_channel:\n        converter._experimental_disable_per_channel = True\n    try:\n        tflite_model = converter.convert()\n        output_path.write_bytes(tflite_model)\n        return output_path, \"int8\"\n    except Exception as exc:\n        if not allow_fallback:\n            raise\n        print(\"Int8 conversion failed, falling back to dynamic range:\", exc)\n        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        tflite_model = converter.convert()\n        output_path.write_bytes(tflite_model)\n        return output_path, \"dynamic\"\n\n\nmobilenet_tflite_path = OUTPUT_DIR / \"mobilenetv2_int8.tflite\"\nmobile_path, mobile_quant_type = convert_int8_tflite(\n    mobilenet_model,\n    X_frames,\n    mobilenet_tflite_path,\n    allow_fallback=True,\n    disable_per_channel=DISABLE_PER_CHANNEL,\n)\nprint(\"Saved TFLite:\", mobile_path, \"quant:\", mobile_quant_type)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\n\n\ndef _quantize_input(x, input_detail):\n    dtype = input_detail[\"dtype\"]\n    if dtype == np.float32:\n        return x.astype(np.float32)\n    scale, zero_point = input_detail.get(\"quantization\", (0.0, 0))\n    if scale == 0:\n        return x.astype(dtype)\n    q = x / scale + zero_point\n    if dtype == np.int8:\n        q = np.clip(np.round(q), -128, 127).astype(np.int8)\n    elif dtype == np.uint8:\n        q = np.clip(np.round(q), 0, 255).astype(np.uint8)\n    else:\n        q = q.astype(dtype)\n    return q\n\n\ndef _dequantize_output(y, output_detail):\n    dtype = output_detail[\"dtype\"]\n    if dtype == np.float32:\n        return y.astype(np.float32)\n    scale, zero_point = output_detail.get(\"quantization\", (0.0, 0))\n    if scale == 0:\n        return y.astype(np.float32)\n    return (y.astype(np.float32) - zero_point) * scale\n\n\ndef evaluate_tflite(tflite_path: Path, xs, ys):\n    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n    interpreter.allocate_tensors()\n    input_detail = interpreter.get_input_details()[0]\n    output_detail = interpreter.get_output_details()[0]\n\n    preds = []\n    start = time.perf_counter()\n    for i in range(len(xs)):\n        x = xs[i : i + 1]\n        xq = _quantize_input(x, input_detail)\n        interpreter.set_tensor(input_detail[\"index\"], xq)\n        interpreter.invoke()\n        y = interpreter.get_tensor(output_detail[\"index\"])\n        y = _dequantize_output(y, output_detail)\n        preds.append(y)\n    elapsed = time.perf_counter() - start\n    preds = np.concatenate(preds, axis=0)\n    y_pred = np.argmax(preds, axis=1)\n    acc = accuracy_score(ys, y_pred)\n    ms_per = (elapsed / len(ys)) * 1000.0\n    return {\"accuracy\": acc, \"ms_per_sample\": ms_per}\n\n\ndef summarize_tflite(path: Path):\n    interpreter = tf.lite.Interpreter(model_path=str(path))\n    interpreter.allocate_tensors()\n    input_detail = interpreter.get_input_details()[0]\n    output_detail = interpreter.get_output_details()[0]\n    print(\"Input dtype:\", input_detail[\"dtype\"], \"quant:\", input_detail.get(\"quantization\"))\n    print(\"Output dtype:\", output_detail[\"dtype\"], \"quant:\", output_detail.get(\"quantization\"))\n    try:\n        from tensorflow.lite.experimental import Analyzer\n        print(Analyzer.analyze(model_path=str(path), show_details=False))\n    except Exception as exc:\n        print(\"Analyzer unavailable:\", exc)\n\n\nmobilenet_tflite_metrics = evaluate_tflite(mobilenet_tflite_path, X_frames, y_frames)\nprint(\"MobileNet TFLite metrics:\", mobilenet_tflite_metrics)\n\nsummarize_tflite(mobilenet_tflite_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Axis Model Zoo compatibility notes\n\nSummary from Axis Model Zoo documentation and model lists:\n\n- The Axis Model Zoo repository includes multiple quantized TFLite models (for example, MobileNetV2, SSD MobileNet, and QAT SSDLite MobileDet) targeting ARTPEC-7/8/9 chips, which indicates that INT8 TFLite is supported on those cameras.\n- The README lists performance benchmarks per chip and links to TFLite models (many are quantized or QAT), implying the expected on-device format.\n- The larod-client workflow described in the repo uses .tflite (and some .bin) files and selects chip targets such as A9-DLPU, A8-DLPU, A7-GPU, or CPU, which is how Axis validates model compatibility.\n\nLinks used:\n- https://github.com/AxisCommunications/axis-model-zoo\n- https://github.com/AxisCommunications/axis-model-zoo/blob/main/README.md\n- https://github.com/AxisCommunications/axis-model-zoo/tree/main/scripts/auto-test-framework/larod-test\n\nPractical implication for this project:\n- An INT8 TFLite model composed of built-in ops has the best chance of running on Axis cameras.\n- If conversion requires SELECT_TF_OPS, the model may not be compatible with the LAROD runtime on camera. Validate with larod-client on the target chip.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## LSTM/GRU: load model + evaluate float\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "LSTM_MODEL_NAME = \"lstm_final.keras\"\n\nlstm_path = resolve_model_path(LSTM_MODEL_NAME)\nprint(\"LSTM model path:\", lstm_path)\n\nlstm_model = tf.keras.models.load_model(\n    lstm_path,\n    custom_objects=CUSTOM_OBJECTS,\n    compile=False,\n    safe_mode=False,\n)\n\nlstm_input_shape = lstm_model.input_shape\nif isinstance(lstm_input_shape, list):\n    lstm_input_shape = lstm_input_shape[0]\n\nif len(lstm_input_shape) < 5:\n    raise ValueError(f\"Expected sequence input shape, got {lstm_input_shape}\")\n\nsequence_length = lstm_input_shape[1] or cfg.SEQUENCE_LENGTH\nif sequence_length is None:\n    sequence_length = cfg.SEQUENCE_LENGTH\n\nif lstm_input_shape[2] is None or lstm_input_shape[3] is None:\n    seq_input_size = cfg.IMG_SIZE\nelse:\n    seq_input_size = (lstm_input_shape[3], lstm_input_shape[2])\n\nprint(\"LSTM sequence length:\", sequence_length)\nprint(\"LSTM input size:\", seq_input_size)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "SEQ_FRAME_STRIDE = 2\nMAX_SEQUENCES_PER_VIDEO = 3\nMAX_TEST_VIDEOS_LSTM = 40\n\nX_seq, y_seq = build_sequence_dataset(\n    test_df,\n    size=seq_input_size,\n    sequence_length=sequence_length,\n    frame_stride=SEQ_FRAME_STRIDE,\n    max_sequences_per_video=MAX_SEQUENCES_PER_VIDEO,\n    max_videos=MAX_TEST_VIDEOS_LSTM,\n)\n\nprint(\"Sequence samples:\", X_seq.shape)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "lstm_float_metrics = evaluate_keras(lstm_model, X_seq, y_seq, batch_size=8)\nprint(\"LSTM float metrics:\", lstm_float_metrics)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## LSTM/GRU: post-training quantization\n\nNote: Sequence models often rely on ops that are not supported on the DLPU.\nIf full INT8 conversion fails, the notebook falls back to dynamic range quantization.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "lstm_tflite_path = OUTPUT_DIR / \"lstm_int8.tflite\"\n\nlstm_path_out, lstm_quant_type = convert_int8_tflite(\n    lstm_model,\n    X_seq,\n    lstm_tflite_path,\n    allow_fallback=True,\n)\nprint(\"Saved LSTM TFLite:\", lstm_path_out, \"quant:\", lstm_quant_type)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "lstm_tflite_metrics = evaluate_tflite(lstm_tflite_path, X_seq, y_seq)\nprint(\"LSTM TFLite metrics:\", lstm_tflite_metrics)\n\nsummarize_tflite(lstm_tflite_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Comparison summary\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "summary = pd.DataFrame([\n    {\"model\": \"MobileNetV2\", \"type\": \"float\", **mobilenet_float_metrics},\n    {\"model\": \"MobileNetV2\", \"type\": f\"tflite_{mobile_quant_type}\", **mobilenet_tflite_metrics},\n    {\"model\": \"LSTM/GRU\", \"type\": \"float\", **lstm_float_metrics},\n    {\"model\": \"LSTM/GRU\", \"type\": f\"tflite_{lstm_quant_type}\", **lstm_tflite_metrics},\n])\nsummary\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}