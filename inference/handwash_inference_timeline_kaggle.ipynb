{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwash inference + post-training scaling (Kaggle)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Kaggle runtimes usually include the required packages.\n",
    "# If something is missing, uncomment and run:\n",
    "# !pip -q install opencv-python-headless tqdm requests\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "import importlib.util\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Video, display\n",
    "\n",
    "IS_KAGGLE = bool(os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"))\n",
    "\n",
    "def find_repo_root(start=None):\n",
    "    start = Path.cwd() if start is None else Path(start)\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"inference\" / \"config.py\").exists() or (parent / \"training\" / \"config.py\").exists():\n",
    "            return parent\n",
    "    if IS_KAGGLE:\n",
    "        inputs_dir = Path(\"/kaggle/input\")\n",
    "        if inputs_dir.exists():\n",
    "            for candidate in inputs_dir.iterdir():\n",
    "                if (candidate / \"inference\" / \"config.py\").exists() or (candidate / \"training\" / \"config.py\").exists():\n",
    "                    return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "def _load_module(path: Path, name: str):\n",
    "    spec = importlib.util.spec_from_file_location(name, path)\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise ImportError(f\"Cannot load module from {path}\")\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "\n",
    "def load_cfg(repo_root: Path):\n",
    "    inference_cfg = repo_root / \"inference\" / \"config.py\"\n",
    "    training_cfg = repo_root / \"training\" / \"config.py\"\n",
    "    if inference_cfg.exists():\n",
    "        return _load_module(inference_cfg, \"cfg\")\n",
    "    if training_cfg.exists():\n",
    "        return _load_module(training_cfg, \"cfg\")\n",
    "    # Fallback for standalone Kaggle notebook\n",
    "    return SimpleNamespace(\n",
    "        RANDOM_SEED=42,\n",
    "        IMG_SIZE=(224, 224),\n",
    "        VAL_RATIO=0.15,\n",
    "        TEST_RATIO=0.15,\n",
    "        ENABLE_SHADOW_AUG=True,\n",
    "        AUGMENTATION_CONFIG={\n",
    "            \"rotation_range\": 15,\n",
    "            \"width_shift_range\": 0.1,\n",
    "            \"height_shift_range\": 0.1,\n",
    "            \"shear_range\": 0.1,\n",
    "            \"zoom_range\": 0.1,\n",
    "            \"horizontal_flip\": True,\n",
    "            \"mid_flip\": True,\n",
    "            \"brightness_range\": (0.9, 1.1),\n",
    "            \"reverse_sequence\": True,\n",
    "            \"fill_mode\": \"nearest\",\n",
    "        },\n",
    "        CLASS_NAMES=[\n",
    "            \"Other\",\n",
    "            \"Step1_PalmToPalm\",\n",
    "            \"Step2_PalmOverDorsum\",\n",
    "            \"Step3_InterlacedFingers\",\n",
    "            \"Step4_BackOfFingers\",\n",
    "            \"Step5_ThumbRub\",\n",
    "            \"Step6_Fingertips\",\n",
    "        ],\n",
    "        KAGGLE_CLASS_MAPPING={\n",
    "            \"0\": 0,\n",
    "            \"1\": 1,\n",
    "            \"2\": 2,\n",
    "            \"3\": 3,\n",
    "            \"4\": 4,\n",
    "            \"5\": 5,\n",
    "            \"6\": 6,\n",
    "            \"step1\": 1,\n",
    "            \"step2\": 2,\n",
    "            \"step3\": 3,\n",
    "            \"step4\": 4,\n",
    "            \"step5\": 5,\n",
    "            \"step6\": 6,\n",
    "            \"other\": 0,\n",
    "        },\n",
    "        SEQUENCE_LENGTH=16,\n",
    "        BATCH_SIZE=32,\n",
    "    )\n",
    "\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "cfg = load_cfg(REPO_ROOT)\n",
    "\n",
    "np.random.seed(cfg.RANDOM_SEED)\n",
    "random.seed(cfg.RANDOM_SEED)\n",
    "\n",
    "WORK_DIR = Path(\"/kaggle/working\") if IS_KAGGLE else REPO_ROOT\n",
    "DATA_ROOT = WORK_DIR / \"handwash_data\"\n",
    "RAW_DIR = DATA_ROOT / \"raw\"\n",
    "PROCESSED_DIR = DATA_ROOT / \"processed\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_DIR = WORK_DIR / \"handwash_outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# User config\n",
    "MODEL_NAME = \"lstm_final.keras\"  # change to \"mobilenetv2_final.keras\" if needed\n",
    "MODEL_PATH = None  # set to explicit path if needed\n",
    "\n",
    "\n",
    "def resolve_model_path(model_name, explicit_path=None):\n",
    "    if explicit_path:\n",
    "        path = Path(explicit_path)\n",
    "        if path.exists():\n",
    "            return path\n",
    "\n",
    "    search_roots = [WORK_DIR, REPO_ROOT, REPO_ROOT / \"models\", REPO_ROOT / \"Runs\"]\n",
    "    if IS_KAGGLE:\n",
    "        search_roots.append(Path(\"/kaggle/input\"))\n",
    "\n",
    "    candidates = []\n",
    "    for base in search_roots:\n",
    "        if base.exists():\n",
    "            candidates.extend(base.rglob(model_name))\n",
    "\n",
    "    if candidates:\n",
    "        candidates = sorted(candidates, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        return candidates[0]\n",
    "    return Path(model_name)\n",
    "\n",
    "\n",
    "MODEL_PATH = resolve_model_path(MODEL_NAME, MODEL_PATH)\n",
    "\n",
    "print(\"Kaggle:\", IS_KAGGLE)\n",
    "print(\"Repo root:\", REPO_ROOT)\n",
    "print(\"Data root:\", DATA_ROOT)\n",
    "print(\"Output dir:\", OUTPUT_DIR)\n",
    "print(\"Model name:\", MODEL_NAME)\n",
    "print(\"Model path:\", MODEL_PATH)\n",
    "if not MODEL_PATH.exists():\n",
    "    print(\"Warning: model path does not exist. Set MODEL_PATH or upload the model to /kaggle/input.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _sample_aug_params() -> dict:\n",
    "    hflip_enabled = any(\n",
    "        cfg.AUGMENTATION_CONFIG.get(key, False)\n",
    "        for key in (\"horizontal_flip\", \"mid_flip\", \"hflip\")\n",
    "    )\n",
    "    params = {\n",
    "        \"hflip\": hflip_enabled and np.random.rand() > 0.5,\n",
    "        \"angle\": 0.0,\n",
    "        \"zoom\": 1.0,\n",
    "        \"shear\": 0.0,\n",
    "        \"tx\": 0,\n",
    "        \"ty\": 0,\n",
    "        \"brightness\": None,\n",
    "        \"contrast\": None,\n",
    "        \"gamma\": None,\n",
    "        \"shadow\": False,\n",
    "        \"reverse_sequence\": cfg.AUGMENTATION_CONFIG.get(\"reverse_sequence\", False) and np.random.rand() > 0.5,\n",
    "    }\n",
    "\n",
    "    if cfg.AUGMENTATION_CONFIG.get(\"rotation_range\", 0) > 0:\n",
    "        params[\"angle\"] = np.random.uniform(\n",
    "            -cfg.AUGMENTATION_CONFIG[\"rotation_range\"],\n",
    "            cfg.AUGMENTATION_CONFIG[\"rotation_range\"],\n",
    "        )\n",
    "\n",
    "    if cfg.AUGMENTATION_CONFIG.get(\"zoom_range\", 0) > 0:\n",
    "        params[\"zoom\"] = np.random.uniform(\n",
    "            1 - cfg.AUGMENTATION_CONFIG[\"zoom_range\"],\n",
    "            1 + cfg.AUGMENTATION_CONFIG[\"zoom_range\"],\n",
    "        )\n",
    "\n",
    "    if cfg.AUGMENTATION_CONFIG.get(\"shear_range\", 0) > 0:\n",
    "        params[\"shear\"] = np.random.uniform(\n",
    "            -cfg.AUGMENTATION_CONFIG[\"shear_range\"],\n",
    "            cfg.AUGMENTATION_CONFIG[\"shear_range\"],\n",
    "        )\n",
    "\n",
    "    if cfg.AUGMENTATION_CONFIG.get(\"width_shift_range\", 0) > 0 or cfg.AUGMENTATION_CONFIG.get(\"height_shift_range\", 0) > 0:\n",
    "        params[\"tx\"] = int(np.random.uniform(\n",
    "            -cfg.AUGMENTATION_CONFIG[\"width_shift_range\"],\n",
    "            cfg.AUGMENTATION_CONFIG[\"width_shift_range\"],\n",
    "        ) * cfg.IMG_SIZE[0])\n",
    "        params[\"ty\"] = int(np.random.uniform(\n",
    "            -cfg.AUGMENTATION_CONFIG[\"height_shift_range\"],\n",
    "            cfg.AUGMENTATION_CONFIG[\"height_shift_range\"],\n",
    "        ) * cfg.IMG_SIZE[1])\n",
    "\n",
    "    if \"brightness_range\" in cfg.AUGMENTATION_CONFIG:\n",
    "        params[\"brightness\"] = np.random.uniform(*cfg.AUGMENTATION_CONFIG[\"brightness_range\"])\n",
    "\n",
    "    if \"contrast_range\" in cfg.AUGMENTATION_CONFIG:\n",
    "        params[\"contrast\"] = np.random.uniform(*cfg.AUGMENTATION_CONFIG[\"contrast_range\"])\n",
    "\n",
    "    if \"gamma_range\" in cfg.AUGMENTATION_CONFIG:\n",
    "        params[\"gamma\"] = np.random.uniform(*cfg.AUGMENTATION_CONFIG[\"gamma_range\"])\n",
    "\n",
    "    if cfg.ENABLE_SHADOW_AUG and np.random.rand() < 0.5:\n",
    "        params[\"shadow\"] = True\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def _apply_aug(img: np.ndarray, params: dict) -> np.ndarray:\n",
    "    if params.get(\"hflip\"):\n",
    "        img = cv2.flip(img, 1)\n",
    "\n",
    "    angle = params.get(\"angle\", 0.0)\n",
    "    if angle:\n",
    "        h, w = img.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1.0)\n",
    "        img = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    zoom = params.get(\"zoom\", 1.0)\n",
    "    if zoom != 1.0:\n",
    "        h, w = img.shape[:2]\n",
    "        new_h, new_w = int(h * zoom), int(w * zoom)\n",
    "        img_resized = cv2.resize(img, (new_w, new_h))\n",
    "        if zoom > 1:\n",
    "            start_y = (new_h - h) // 2\n",
    "            start_x = (new_w - w) // 2\n",
    "            img = img_resized[start_y:start_y + h, start_x:start_x + w]\n",
    "        else:\n",
    "            pad_h = (h - new_h) // 2\n",
    "            pad_w = (w - new_w) // 2\n",
    "            img = cv2.copyMakeBorder(\n",
    "                img_resized,\n",
    "                pad_h, h - new_h - pad_h,\n",
    "                pad_w, w - new_w - pad_w,\n",
    "                cv2.BORDER_REFLECT,\n",
    "            )\n",
    "\n",
    "    tx, ty = params.get(\"tx\", 0), params.get(\"ty\", 0)\n",
    "    if tx or ty:\n",
    "        h, w = img.shape[:2]\n",
    "        M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        img = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    shear = params.get(\"shear\", 0.0)\n",
    "    if shear:\n",
    "        h, w = img.shape[:2]\n",
    "        M = np.float32([[1, shear, 0], [0, 1, 0]])\n",
    "        img = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    brightness = params.get(\"brightness\")\n",
    "    if brightness is not None:\n",
    "        img = np.clip(img.astype(np.float32) * brightness, 0, 255).astype(np.uint8)\n",
    "\n",
    "    contrast = params.get(\"contrast\")\n",
    "    if contrast is not None:\n",
    "        img = np.clip(128 + contrast * (img.astype(np.float32) - 128), 0, 255).astype(np.uint8)\n",
    "\n",
    "    gamma = params.get(\"gamma\")\n",
    "    if gamma is not None:\n",
    "        img = np.clip(((img.astype(np.float32) / 255.0) ** gamma) * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    if params.get(\"shadow\"):\n",
    "        h, w = img.shape[:2]\n",
    "        x1, y1 = np.random.randint(0, w), 0\n",
    "        x2, y2 = np.random.randint(0, w), h\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.fillPoly(mask, [np.array([[x1, y1], [x2, y2], [0, h], [w, h]])], 255)\n",
    "        shadow_intensity = np.random.uniform(0.5, 0.9)\n",
    "        shadow = np.stack([mask] * 3, axis=-1)\n",
    "        img = np.where(shadow > 0, (img * shadow_intensity).astype(np.uint8), img)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "KAGGLE_URL = \"https://github.com/atiselsts/data/raw/master/kaggle-dataset-6classes.tar\"\n",
    "KAGGLE_DIR = RAW_DIR / \"kaggle\"\n",
    "KAGGLE_EXTRACTED = KAGGLE_DIR / \"kaggle-dataset-6classes\"\n",
    "\n",
    "\n",
    "def download_with_progress(url, dest):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dest.exists():\n",
    "        print(\"skip\", dest)\n",
    "        return\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"content-length\", 0))\n",
    "        with open(dest, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, desc=dest.name) as pbar:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "\n",
    "\n",
    "def extract_tar(tar_path, out_dir):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with tarfile.open(tar_path) as tfp:\n",
    "        tfp.extractall(out_dir)\n",
    "    try:\n",
    "        tar_path.unlink()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def _find_kaggle_input_dataset():\n",
    "    if not IS_KAGGLE:\n",
    "        return None\n",
    "    input_root = Path(\"/kaggle/input\")\n",
    "    if not input_root.exists():\n",
    "        return None\n",
    "    for candidate in input_root.rglob(\"kaggle-dataset-6classes\"):\n",
    "        if candidate.is_dir():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def ensure_kaggle_dataset():\n",
    "    input_candidate = _find_kaggle_input_dataset()\n",
    "    if input_candidate:\n",
    "        print(\"Using Kaggle input dataset:\", input_candidate)\n",
    "        return input_candidate\n",
    "    if KAGGLE_EXTRACTED.exists() and any(KAGGLE_EXTRACTED.iterdir()):\n",
    "        print(\"Kaggle dataset already present:\", KAGGLE_EXTRACTED)\n",
    "        return KAGGLE_EXTRACTED\n",
    "    KAGGLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    tar_path = KAGGLE_DIR / \"kaggle-dataset-6classes.tar\"\n",
    "    download_with_progress(KAGGLE_URL, tar_path)\n",
    "    print(\"Extracting kaggle dataset...\")\n",
    "    extract_tar(tar_path, KAGGLE_DIR)\n",
    "    return KAGGLE_EXTRACTED\n",
    "\n",
    "\n",
    "kaggle_root = ensure_kaggle_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "VIDEO_EXTS = (\".mp4\", \".avi\", \".mov\", \".mkv\")\n",
    "\n",
    "def kaggle_class_id_from_folder(name):\n",
    "    name_lower = name.lower()\n",
    "    if name_lower in cfg.KAGGLE_CLASS_MAPPING:\n",
    "        return int(cfg.KAGGLE_CLASS_MAPPING[name_lower])\n",
    "    digits = \"\".join(ch for ch in name_lower if ch.isdigit())\n",
    "    if digits:\n",
    "        class_id = int(digits)\n",
    "        if 0 <= class_id < len(cfg.CLASS_NAMES):\n",
    "            return class_id\n",
    "    return 0\n",
    "\n",
    "def scan_kaggle_videos(root):\n",
    "    records = []\n",
    "    for class_dir in sorted(root.iterdir()):\n",
    "        if not class_dir.is_dir():\n",
    "            continue\n",
    "        class_id = kaggle_class_id_from_folder(class_dir.name)\n",
    "        class_name = cfg.CLASS_NAMES[class_id]\n",
    "        for video_path in sorted(class_dir.iterdir()):\n",
    "            if video_path.suffix.lower() in VIDEO_EXTS:\n",
    "                records.append({\n",
    "                    \"video_path\": str(video_path),\n",
    "                    \"class_id\": class_id,\n",
    "                    \"class_name\": class_name,\n",
    "                })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "videos_df = scan_kaggle_videos(kaggle_root)\n",
    "if videos_df.empty:\n",
    "    raise RuntimeError(f\"No videos found under {kaggle_root}\")\n",
    "display(videos_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    videos_df,\n",
    "    test_size=(cfg.VAL_RATIO + cfg.TEST_RATIO),\n",
    "    stratify=videos_df[\"class_id\"],\n",
    "    random_state=cfg.RANDOM_SEED,\n",
    ")\n",
    "val_size = cfg.VAL_RATIO / (cfg.VAL_RATIO + cfg.TEST_RATIO)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=(1.0 - val_size),\n",
    "    stratify=temp_df[\"class_id\"],\n",
    "    random_state=cfg.RANDOM_SEED,\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Videos:\", len(videos_df))\n",
    "print(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n",
    "display(videos_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_class_distribution(df, title):\n",
    "    order = list(range(len(cfg.CLASS_NAMES)))\n",
    "    counts = df.groupby(\"class_id\").size().reindex(order, fill_value=0)\n",
    "    labels = [cfg.CLASS_NAMES[i] for i in order]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(labels, counts.values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_class_distribution(videos_df, \"Kaggle WHO6 - All Videos\")\n",
    "plot_class_distribution(train_df, \"Kaggle WHO6 - Train Videos\")\n",
    "plot_class_distribution(val_df, \"Kaggle WHO6 - Val Videos\")\n",
    "plot_class_distribution(test_df, \"Kaggle WHO6 - Test Videos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_videos_by_class(df, max_per_class=1):\n",
    "    for class_id in sorted(df[\"class_id\"].unique().tolist()):\n",
    "        subset = df[df[\"class_id\"] == class_id]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "        label = cfg.CLASS_NAMES[int(class_id)]\n",
    "        samples = subset.sample(min(max_per_class, len(subset)), random_state=cfg.RANDOM_SEED)\n",
    "        for row in samples.itertuples():\n",
    "            print(f\"Class {class_id} | {label} | {row.video_path}\")\n",
    "            display(Video(row.video_path, embed=True, width=320))\n",
    "\n",
    "show_videos_by_class(train_df, max_per_class=1)\n",
    "show_videos_by_class(test_df, max_per_class=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_sample_videos(df, n=2):\n",
    "    sample = df.sample(min(n, len(df)), random_state=cfg.RANDOM_SEED)\n",
    "    for row in sample.itertuples():\n",
    "        print(f\"{row.class_name} | {row.video_path}\")\n",
    "        display(Video(row.video_path, embed=True, width=320))\n",
    "\n",
    "\n",
    "show_sample_videos(train_df, n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample_frames_from_video(video_path, num_frames=8, frame_stride=10):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frames = []\n",
    "    idx = 0\n",
    "    while len(frames) < num_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if idx % frame_stride == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def show_augmented_frames(video_path, num_frames=8, frame_stride=10, consistent=True):\n",
    "    frames = sample_frames_from_video(video_path, num_frames=num_frames, frame_stride=frame_stride)\n",
    "    if not frames:\n",
    "        print(\"No frames for\", video_path)\n",
    "        return\n",
    "    params = _sample_aug_params() if consistent else None\n",
    "    cols = 4\n",
    "    rows = int(math.ceil(len(frames) / cols))\n",
    "    plt.figure(figsize=(cols * 3, rows * 3))\n",
    "    for i, frame in enumerate(frames, 1):\n",
    "        if not consistent:\n",
    "            params = _sample_aug_params()\n",
    "        aug = _apply_aug(frame, params) if params else frame\n",
    "        plt.subplot(rows, cols, i)\n",
    "        plt.imshow(aug)\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(\"Augmented frames\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sample_video = train_df.iloc[0][\"video_path\"]\n",
    "show_augmented_frames(sample_video, num_frames=8, frame_stride=10, consistent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_objects = {}\n",
    "\n",
    "try:\n",
    "    from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_v2_preprocess\n",
    "    custom_objects[\"preprocess_input\"] = mobilenet_v2_preprocess\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "def load_model_safe(path, custom_objects):\n",
    "    kwargs = dict(custom_objects=custom_objects, compile=False)\n",
    "    try:\n",
    "        return tf.keras.models.load_model(path, safe_mode=False, **kwargs)\n",
    "    except TypeError as exc:\n",
    "        if \"safe_mode\" in str(exc):\n",
    "            return tf.keras.models.load_model(path, **kwargs)\n",
    "        raise\n",
    "\n",
    "\n",
    "try:\n",
    "    model = load_model_safe(MODEL_PATH, custom_objects)\n",
    "except (ValueError, TypeError) as exc:\n",
    "    if \"preprocess_input\" in str(exc) and \"preprocess_input\" not in custom_objects:\n",
    "        from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_v2_preprocess\n",
    "        custom_objects[\"preprocess_input\"] = mobilenet_v2_preprocess\n",
    "        model = load_model_safe(MODEL_PATH, custom_objects)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMG_SIZE = tuple(cfg.IMG_SIZE)\n",
    "SEQUENCE_LENGTH = int(getattr(cfg, \"SEQUENCE_LENGTH\", 16))\n",
    "SEQUENCE_STRIDE = 1\n",
    "SEQUENCE_BATCH_SIZE = max(1, int(getattr(cfg, \"BATCH_SIZE\", 32) // 4))\n",
    "CLASS_NAMES = list(cfg.CLASS_NAMES)\n",
    "\n",
    "\n",
    "def preprocess_frame(frame_rgb, img_size=IMG_SIZE):\n",
    "    resized = cv2.resize(frame_rgb, img_size)\n",
    "    return resized.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def collect_video_frames(\n",
    "    video_path,\n",
    "    frame_stride=1,\n",
    "    max_frames=None,\n",
    "    augment=False,\n",
    "    consistent_aug=True,\n",
    "    augment_params=None,\n",
    "):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Failed to open video: {video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps is None or fps <= 0:\n",
    "        fps = 30.0\n",
    "\n",
    "    inputs = []\n",
    "    frames_bgr = []\n",
    "    timestamps = []\n",
    "    idx = 0\n",
    "    apply_aug = augment or augment_params is not None\n",
    "    if augment_params is not None:\n",
    "        aug_params = augment_params\n",
    "    elif augment and consistent_aug:\n",
    "        aug_params = _sample_aug_params()\n",
    "    else:\n",
    "        aug_params = None\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if idx % frame_stride != 0:\n",
    "            idx += 1\n",
    "            continue\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        if apply_aug:\n",
    "            params = aug_params if (augment_params is not None or consistent_aug) else _sample_aug_params()\n",
    "            frame_rgb = _apply_aug(frame_rgb, params)\n",
    "\n",
    "        inputs.append(preprocess_frame(frame_rgb))\n",
    "        frames_bgr.append(cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR))\n",
    "        timestamps.append(idx / fps)\n",
    "\n",
    "        idx += 1\n",
    "        if max_frames and len(inputs) >= max_frames:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    if inputs:\n",
    "        inputs_arr = np.stack(inputs, axis=0)\n",
    "    else:\n",
    "        inputs_arr = np.zeros((0, *IMG_SIZE, 3), dtype=np.float32)\n",
    "    return float(fps), inputs_arr, frames_bgr, np.array(timestamps, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_batches(model, inputs, batch_size=32):\n",
    "    if len(inputs) == 0:\n",
    "        return np.zeros((0, len(CLASS_NAMES)), dtype=np.float32)\n",
    "    preds = []\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch = inputs[i : i + batch_size]\n",
    "        preds.append(model.predict(batch, verbose=0))\n",
    "    return np.concatenate(preds, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def _get_model_input_shape(model):\n",
    "    shape = model.input_shape\n",
    "    # Keras returns a tuple for single-input models and a list for multi-input.\n",
    "    if isinstance(shape, (list, tuple)) and shape and isinstance(shape[0], (list, tuple)):\n",
    "        shape = shape[0]\n",
    "    return shape\n",
    "\n",
    "\n",
    "def is_sequence_model(model):\n",
    "    shape = _get_model_input_shape(model)\n",
    "    return shape is not None and len(shape) == 5\n",
    "\n",
    "\n",
    "def get_sequence_length(model, fallback=SEQUENCE_LENGTH):\n",
    "    shape = _get_model_input_shape(model)\n",
    "    if shape is not None and len(shape) >= 2 and shape[1]:\n",
    "        return int(shape[1])\n",
    "    return int(fallback)\n",
    "\n",
    "\n",
    "def predict_sequence_probs(model, inputs, seq_len=None, stride=1, batch_size=8):\n",
    "    if len(inputs) == 0:\n",
    "        return np.zeros((0, len(CLASS_NAMES)), dtype=np.float32)\n",
    "    seq_len = int(seq_len or get_sequence_length(model))\n",
    "    stride = max(1, int(stride))\n",
    "    if len(inputs) < seq_len:\n",
    "        pad_len = seq_len - len(inputs)\n",
    "        pad = np.repeat(inputs[-1][None, ...], pad_len, axis=0)\n",
    "        sequences = np.expand_dims(np.concatenate([inputs, pad], axis=0), axis=0)\n",
    "        starts = [0]\n",
    "    else:\n",
    "        starts = list(range(0, len(inputs) - seq_len + 1, stride))\n",
    "        sequences = np.stack([inputs[s:s + seq_len] for s in starts], axis=0)\n",
    "    preds = []\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        batch = sequences[i:i + batch_size]\n",
    "        preds.append(model.predict(batch, verbose=0))\n",
    "    preds = np.concatenate(preds, axis=0) if preds else np.zeros((0, len(CLASS_NAMES)), dtype=np.float32)\n",
    "    probs = np.zeros((len(inputs), preds.shape[1] if len(preds) else len(CLASS_NAMES)), dtype=np.float32)\n",
    "    counts = np.zeros((len(inputs), 1), dtype=np.float32)\n",
    "    for pred, start in zip(preds, starts):\n",
    "        end = min(start + seq_len, len(inputs))\n",
    "        probs[start:end] += pred\n",
    "        counts[start:end] += 1.0\n",
    "    probs = probs / np.clip(counts, 1e-6, None)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def predict_model_probs(model, inputs, batch_size=32, sequence_length=None, sequence_stride=1, sequence_batch_size=None):\n",
    "    if is_sequence_model(model):\n",
    "        seq_len = sequence_length or get_sequence_length(model)\n",
    "        seq_bs = sequence_batch_size or batch_size\n",
    "        return predict_sequence_probs(model, inputs, seq_len=seq_len, stride=sequence_stride, batch_size=seq_bs)\n",
    "    return predict_batches(model, inputs, batch_size=batch_size)\n",
    "\n",
    "def smooth_probs_moving_avg(probs, window=7):\n",
    "    if window <= 1 or len(probs) == 0:\n",
    "        return probs\n",
    "    kernel = np.ones(window, dtype=np.float32) / float(window)\n",
    "    smoothed = np.zeros_like(probs)\n",
    "    for c in range(probs.shape[1]):\n",
    "        smoothed[:, c] = np.convolve(probs[:, c], kernel, mode=\"same\")\n",
    "    row_sums = smoothed.sum(axis=1, keepdims=True)\n",
    "    smoothed = smoothed / np.clip(row_sums, 1e-6, None)\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def smooth_preds_majority(pred_ids, window=7, num_classes=None):\n",
    "    if window <= 1 or len(pred_ids) == 0:\n",
    "        return pred_ids\n",
    "    num_classes = num_classes or len(CLASS_NAMES)\n",
    "    half = window // 2\n",
    "    out = []\n",
    "    for i in range(len(pred_ids)):\n",
    "        start = max(0, i - half)\n",
    "        end = min(len(pred_ids), i + half + 1)\n",
    "        window_ids = pred_ids[start:end]\n",
    "        counts = np.bincount(window_ids, minlength=num_classes)\n",
    "        out.append(int(np.argmax(counts)))\n",
    "    return np.array(out, dtype=np.int32)\n",
    "\n",
    "\n",
    "def apply_smoothing(probs, method=\"moving_avg\", window=7):\n",
    "    if method == \"none\":\n",
    "        pred_ids = np.argmax(probs, axis=1) if len(probs) else np.array([], dtype=np.int32)\n",
    "        return probs, pred_ids\n",
    "    if method == \"moving_avg\":\n",
    "        smoothed = smooth_probs_moving_avg(probs, window=window)\n",
    "        pred_ids = np.argmax(smoothed, axis=1) if len(smoothed) else np.array([], dtype=np.int32)\n",
    "        return smoothed, pred_ids\n",
    "    if method == \"majority\":\n",
    "        pred_ids = np.argmax(probs, axis=1) if len(probs) else np.array([], dtype=np.int32)\n",
    "        pred_ids = smooth_preds_majority(pred_ids, window=window, num_classes=probs.shape[1] if len(probs) else len(CLASS_NAMES))\n",
    "        return probs, pred_ids\n",
    "    raise ValueError(\"Unknown smoothing method: \" + str(method))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def build_segments(class_ids, timestamps, fps, frame_stride, class_names):\n",
    "    if len(class_ids) == 0:\n",
    "        return []\n",
    "    frame_dt = (frame_stride / fps) if fps else 0.0\n",
    "    segments = []\n",
    "    start = 0\n",
    "    for i in range(1, len(class_ids)):\n",
    "        if class_ids[i] != class_ids[i - 1]:\n",
    "            end = i - 1\n",
    "            segments.append({\n",
    "                \"class_id\": int(class_ids[start]),\n",
    "                \"class_name\": class_names[int(class_ids[start])],\n",
    "                \"start_frame\": int(start),\n",
    "                \"end_frame\": int(end),\n",
    "                \"start_s\": float(timestamps[start]),\n",
    "                \"end_s\": float(timestamps[end] + frame_dt),\n",
    "                \"duration_s\": float((end - start + 1) * frame_dt),\n",
    "            })\n",
    "            start = i\n",
    "    end = len(class_ids) - 1\n",
    "    segments.append({\n",
    "        \"class_id\": int(class_ids[start]),\n",
    "        \"class_name\": class_names[int(class_ids[start])],\n",
    "        \"start_frame\": int(start),\n",
    "        \"end_frame\": int(end),\n",
    "        \"start_s\": float(timestamps[start]),\n",
    "        \"end_s\": float(timestamps[end] + frame_dt),\n",
    "        \"duration_s\": float((end - start + 1) * frame_dt),\n",
    "    })\n",
    "    return segments\n",
    "\n",
    "\n",
    "def summarize_durations(segments, class_names):\n",
    "    totals = {name: 0.0 for name in class_names}\n",
    "    for seg in segments:\n",
    "        totals[seg[\"class_name\"]] += seg[\"duration_s\"]\n",
    "    total_wash = sum(totals[name] for name in class_names if name.lower() != \"other\")\n",
    "    return totals, total_wash\n",
    "\n",
    "\n",
    "def build_timeline_df(timestamps, probs_raw, pred_ids, class_names, gt_ids=None, probs_smoothed=None):\n",
    "    data = {\n",
    "        \"timestamp_s\": timestamps,\n",
    "        \"pred_class_id\": pred_ids,\n",
    "        \"pred_class_name\": [class_names[i] for i in pred_ids],\n",
    "    }\n",
    "    if gt_ids is not None:\n",
    "        data[\"gt_class_id\"] = gt_ids\n",
    "        data[\"gt_class_name\"] = [class_names[i] for i in gt_ids]\n",
    "    if probs_smoothed is None:\n",
    "        probs_smoothed = probs_raw\n",
    "    for i, name in enumerate(class_names):\n",
    "        data[f\"prob_raw_{name}\"] = probs_raw[:, i] if len(probs_raw) else []\n",
    "        data[f\"prob_smooth_{name}\"] = probs_smoothed[:, i] if len(probs_smoothed) else []\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def plot_time_series(timestamps, gt_ids, pred_ids, class_names):\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    if gt_ids is not None and len(gt_ids):\n",
    "        ax.step(timestamps, gt_ids, where=\"post\", label=\"GT\", linewidth=2)\n",
    "    ax.step(timestamps, pred_ids, where=\"post\", label=\"Pred\", linewidth=2)\n",
    "    ax.set_yticks(range(len(class_names)))\n",
    "    ax.set_yticklabels([name.replace(\"Step\", \"\") for name in class_names])\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.grid(True, axis=\"x\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compress_sequence(seq):\n",
    "    out = []\n",
    "    prev = None\n",
    "    for s in seq:\n",
    "        if prev is None or s != prev:\n",
    "            out.append(int(s))\n",
    "            prev = s\n",
    "    return out\n",
    "\n",
    "\n",
    "def edit_distance(seq_a, seq_b):\n",
    "    m, n = len(seq_a), len(seq_b)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if seq_a[i - 1] == seq_b[j - 1] else 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,\n",
    "                dp[i][j - 1] + 1,\n",
    "                dp[i - 1][j - 1] + cost,\n",
    "            )\n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "def dtw_distance(seq_a, seq_b):\n",
    "    m, n = len(seq_a), len(seq_b)\n",
    "    if m == 0 or n == 0:\n",
    "        return float(\"inf\")\n",
    "    dp = np.full((m + 1, n + 1), np.inf, dtype=np.float32)\n",
    "    dp[0, 0] = 0.0\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0.0 if seq_a[i - 1] == seq_b[j - 1] else 1.0\n",
    "            dp[i, j] = cost + min(dp[i - 1, j], dp[i, j - 1], dp[i - 1, j - 1])\n",
    "    return float(dp[m, n])\n",
    "\n",
    "\n",
    "def temporal_iou(gt_ids, pred_ids, num_classes):\n",
    "    ious = {}\n",
    "    gt_ids = np.asarray(gt_ids)\n",
    "    pred_ids = np.asarray(pred_ids)\n",
    "    for c in range(num_classes):\n",
    "        gt_mask = gt_ids == c\n",
    "        pred_mask = pred_ids == c\n",
    "        inter = np.sum(gt_mask & pred_mask)\n",
    "        union = np.sum(gt_mask | pred_mask)\n",
    "        ious[c] = inter / union if union > 0 else float(\"nan\")\n",
    "    return ious\n",
    "\n",
    "\n",
    "def evaluate_alignment(gt_ids, pred_ids, class_names):\n",
    "    if gt_ids is None or len(gt_ids) == 0:\n",
    "        return {}\n",
    "    metrics = {}\n",
    "    metrics[\"frame_accuracy\"] = float(accuracy_score(gt_ids, pred_ids))\n",
    "    metrics[\"edit_distance\"] = int(edit_distance(compress_sequence(gt_ids), compress_sequence(pred_ids)))\n",
    "    metrics[\"dtw_distance\"] = float(dtw_distance(gt_ids, pred_ids))\n",
    "    ious = temporal_iou(gt_ids, pred_ids, len(class_names))\n",
    "    metrics[\"temporal_iou_mean\"] = float(np.nanmean(list(ious.values())))\n",
    "    metrics[\"temporal_iou_by_class\"] = {\n",
    "        class_names[c]: (None if np.isnan(v) else float(v)) for c, v in ious.items()\n",
    "    }\n",
    "    metrics[\"classification_report\"] = classification_report(\n",
    "        gt_ids,\n",
    "        pred_ids,\n",
    "        labels=list(range(len(class_names))),\n",
    "        target_names=class_names,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, top_k_accuracy_score\n",
    "\n",
    "\n",
    "def _base_aug_params():\n",
    "    return {\n",
    "        \"hflip\": False,\n",
    "        \"angle\": 0.0,\n",
    "        \"zoom\": 1.0,\n",
    "        \"shear\": 0.0,\n",
    "        \"tx\": 0,\n",
    "        \"ty\": 0,\n",
    "        \"brightness\": None,\n",
    "        \"contrast\": None,\n",
    "        \"gamma\": None,\n",
    "        \"shadow\": False,\n",
    "        \"reverse_sequence\": False,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_aug_variants():\n",
    "    cfg_aug = cfg.AUGMENTATION_CONFIG\n",
    "    variants = [(\"none\", None)]\n",
    "    if cfg_aug.get(\"rotation_range\", 0) > 0:\n",
    "        params = _base_aug_params()\n",
    "        params[\"angle\"] = float(cfg_aug[\"rotation_range\"])\n",
    "        variants.append((\"rotation\", params))\n",
    "    if cfg_aug.get(\"zoom_range\", 0) > 0:\n",
    "        params = _base_aug_params()\n",
    "        params[\"zoom\"] = 1 + float(cfg_aug[\"zoom_range\"])\n",
    "        variants.append((\"zoom\", params))\n",
    "    if cfg_aug.get(\"shear_range\", 0) > 0:\n",
    "        params = _base_aug_params()\n",
    "        params[\"shear\"] = float(cfg_aug[\"shear_range\"])\n",
    "        variants.append((\"shear\", params))\n",
    "    if cfg_aug.get(\"width_shift_range\", 0) > 0 or cfg_aug.get(\"height_shift_range\", 0) > 0:\n",
    "        params = _base_aug_params()\n",
    "        params[\"tx\"] = int(float(cfg_aug.get(\"width_shift_range\", 0)) * IMG_SIZE[0])\n",
    "        params[\"ty\"] = int(float(cfg_aug.get(\"height_shift_range\", 0)) * IMG_SIZE[1])\n",
    "        variants.append((\"shift\", params))\n",
    "    if cfg_aug.get(\"horizontal_flip\", False) or cfg_aug.get(\"hflip\", False) or cfg_aug.get(\"mid_flip\", False):\n",
    "        params = _base_aug_params()\n",
    "        params[\"hflip\"] = True\n",
    "        variants.append((\"hflip\", params))\n",
    "    if cfg_aug.get(\"brightness_range\"):\n",
    "        params = _base_aug_params()\n",
    "        params[\"brightness\"] = float(cfg_aug[\"brightness_range\"][1])\n",
    "        variants.append((\"brightness\", params))\n",
    "    if cfg_aug.get(\"contrast_range\"):\n",
    "        params = _base_aug_params()\n",
    "        params[\"contrast\"] = float(cfg_aug[\"contrast_range\"][1])\n",
    "        variants.append((\"contrast\", params))\n",
    "    if cfg_aug.get(\"gamma_range\"):\n",
    "        params = _base_aug_params()\n",
    "        params[\"gamma\"] = float(cfg_aug[\"gamma_range\"][1])\n",
    "        variants.append((\"gamma\", params))\n",
    "    if cfg.ENABLE_SHADOW_AUG:\n",
    "        params = _base_aug_params()\n",
    "        params[\"shadow\"] = True\n",
    "        variants.append((\"shadow\", params))\n",
    "    return variants\n",
    "\n",
    "\n",
    "def _safe_name(text):\n",
    "    return \"\".join(ch if ch.isalnum() else \"_\" for ch in text)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title, out_path=None):\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Pred\")\n",
    "    plt.tight_layout()\n",
    "    if out_path:\n",
    "        plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_prob):\n",
    "    labels = list(range(len(CLASS_NAMES)))\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_w, recall_w, f1_w, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\", zero_division=0, labels=labels\n",
    "    )\n",
    "    precision_m, recall_m, f1_m, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"macro\", zero_division=0, labels=labels\n",
    "    )\n",
    "    top2 = top_k_accuracy_score(y_true, y_prob, k=2, labels=labels) if len(y_prob) else 0.0\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, labels=labels, target_names=CLASS_NAMES, output_dict=True, zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision_weighted\": float(precision_w),\n",
    "        \"recall_weighted\": float(recall_w),\n",
    "        \"f1_weighted\": float(f1_w),\n",
    "        \"precision_macro\": float(precision_m),\n",
    "        \"recall_macro\": float(recall_m),\n",
    "        \"f1_macro\": float(f1_m),\n",
    "        \"top2_accuracy\": float(top2),\n",
    "        \"report\": report,\n",
    "    }\n",
    "\n",
    "\n",
    "def _save_misclassified(frames_bgr, gt_ids, pred_ids, out_root, split_name, aug_name, video_id):\n",
    "    for idx, (gt, pred) in enumerate(zip(gt_ids, pred_ids)):\n",
    "        if int(gt) == int(pred):\n",
    "            continue\n",
    "        gt_name = _safe_name(CLASS_NAMES[int(gt)])\n",
    "        pred_name = _safe_name(CLASS_NAMES[int(pred)])\n",
    "        folder = out_root / split_name / aug_name / f\"gt_{gt}_{gt_name}\" / f\"pred_{pred}_{pred_name}\" / _safe_name(video_id)\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "        frame_path = folder / f\"frame_{idx:06d}.jpg\"\n",
    "        cv2.imwrite(str(frame_path), frames_bgr[idx])\n",
    "\n",
    "\n",
    "def evaluate_split(df, split_name, aug_name, aug_params, output_root, frame_stride=2, batch_size=32,\n",
    "                   save_misclassified_flag=True, max_videos=None, max_frames=None):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "    subset = df\n",
    "    if max_videos is not None:\n",
    "        subset = df.sample(min(max_videos, len(df)), random_state=cfg.RANDOM_SEED)\n",
    "    for row in subset.itertuples():\n",
    "        fps, inputs, frames_bgr, timestamps = collect_video_frames(\n",
    "            row.video_path,\n",
    "            frame_stride=frame_stride,\n",
    "            max_frames=max_frames,\n",
    "            augment=aug_params is not None,\n",
    "            consistent_aug=True,\n",
    "            augment_params=aug_params,\n",
    "        )\n",
    "        if len(inputs) == 0:\n",
    "            continue\n",
    "        probs = predict_model_probs(\n",
    "            model,\n",
    "            inputs,\n",
    "            batch_size=batch_size,\n",
    "            sequence_length=SEQUENCE_LENGTH,\n",
    "            sequence_stride=SEQUENCE_STRIDE,\n",
    "            sequence_batch_size=SEQUENCE_BATCH_SIZE,\n",
    "        )\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        gt_ids = np.full(len(preds), int(row.class_id), dtype=np.int32)\n",
    "        y_true.extend(gt_ids.tolist())\n",
    "        y_pred.extend(preds.tolist())\n",
    "        y_prob.append(probs)\n",
    "        if save_misclassified_flag:\n",
    "            _save_misclassified(\n",
    "                frames_bgr,\n",
    "                gt_ids,\n",
    "                preds,\n",
    "                output_root / \"misclassified\",\n",
    "                split_name,\n",
    "                aug_name,\n",
    "                Path(row.video_path).stem,\n",
    "            )\n",
    "\n",
    "    if not y_true:\n",
    "        return None\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_prob = np.concatenate(y_prob, axis=0) if y_prob else np.zeros((0, len(CLASS_NAMES)))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(CLASS_NAMES))))\n",
    "    metrics = compute_metrics(y_true, y_pred, y_prob)\n",
    "\n",
    "    out_dir = output_root / \"validation\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cm_path = out_dir / f\"cm_{split_name}_{aug_name}.png\"\n",
    "    plot_confusion_matrix(cm, CLASS_NAMES, f\"{split_name} | {aug_name}\", out_path=cm_path)\n",
    "\n",
    "    metrics_path = out_dir / f\"metrics_{split_name}_{aug_name}.csv\"\n",
    "    pd.DataFrame([{\n",
    "        \"split\": split_name,\n",
    "        \"augmentation\": aug_name,\n",
    "        **{k: v for k, v in metrics.items() if k != \"report\"},\n",
    "    }]).to_csv(metrics_path, index=False)\n",
    "\n",
    "    report_path = out_dir / f\"report_{split_name}_{aug_name}.csv\"\n",
    "    pd.DataFrame(metrics[\"report\"]).T.to_csv(report_path, index=True)\n",
    "\n",
    "    return {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"metrics\": metrics,\n",
    "        \"metrics_path\": metrics_path,\n",
    "        \"report_path\": report_path,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: Confusion Matrices + Misclassified Frames\n",
    "Run this after the model is loaded. It will iterate all videos in each split and each augmentation variant.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "VALIDATION_SPLITS = {\n",
    "    \"train\": train_df,\n",
    "    \"val\": val_df,\n",
    "    \"test\": test_df,\n",
    "}\n",
    "AUG_VARIANTS = build_aug_variants()\n",
    "SAVE_MISCLASSIFIED = True\n",
    "FRAME_STRIDE_VALID = 2\n",
    "BATCH_SIZE_VALID = 32\n",
    "MAX_VIDEOS_PER_SPLIT = None  # set to int to limit\n",
    "MAX_FRAMES_PER_VIDEO = None  # set to int to limit\n",
    "\n",
    "validation_results = {}\n",
    "for split_name, df in VALIDATION_SPLITS.items():\n",
    "    if df.empty:\n",
    "        print(f\"Skipping {split_name}: no videos\")\n",
    "        continue\n",
    "    for aug_name, aug_params in AUG_VARIANTS:\n",
    "        print(f\"Evaluating {split_name} | {aug_name}\")\n",
    "        res = evaluate_split(\n",
    "            df,\n",
    "            split_name=split_name,\n",
    "            aug_name=aug_name,\n",
    "            aug_params=aug_params,\n",
    "            output_root=OUTPUT_DIR,\n",
    "            frame_stride=FRAME_STRIDE_VALID,\n",
    "            batch_size=BATCH_SIZE_VALID,\n",
    "            save_misclassified_flag=SAVE_MISCLASSIFIED,\n",
    "            max_videos=MAX_VIDEOS_PER_SPLIT,\n",
    "            max_frames=MAX_FRAMES_PER_VIDEO,\n",
    "        )\n",
    "        if res is not None:\n",
    "            validation_results[(split_name, aug_name)] = res\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_text_block(\n",
    "    frame,\n",
    "    lines,\n",
    "    origin=(10, 30),\n",
    "    line_height=22,\n",
    "    font_scale=0.6,\n",
    "    thickness=1,\n",
    "    text_color=(255, 255, 255),\n",
    "    bg_color=(0, 0, 0),\n",
    "):\n",
    "    x, y = origin\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    for line in lines:\n",
    "        if not line:\n",
    "            y += line_height\n",
    "            continue\n",
    "        (text_w, text_h), baseline = cv2.getTextSize(line, font, font_scale, thickness)\n",
    "        cv2.rectangle(\n",
    "            frame,\n",
    "            (x - 4, y - text_h - 4),\n",
    "            (x + text_w + 4, y + baseline + 4),\n",
    "            bg_color,\n",
    "            -1,\n",
    "        )\n",
    "        cv2.putText(frame, line, (x, y), font, font_scale, text_color, thickness, cv2.LINE_AA)\n",
    "        y += line_height\n",
    "\n",
    "\n",
    "def write_annotated_video(\n",
    "    frames_bgr,\n",
    "    fps_out,\n",
    "    timestamps,\n",
    "    pred_ids,\n",
    "    probs,\n",
    "    class_names,\n",
    "    gt_ids=None,\n",
    "    summary=None,\n",
    "    out_path=None,\n",
    "    summary_seconds=3,\n",
    "):\n",
    "    if not frames_bgr:\n",
    "        return None\n",
    "    h, w = frames_bgr[0].shape[:2]\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(str(out_path), fourcc, fps_out, (w, h))\n",
    "\n",
    "    frame_dt = float(np.median(np.diff(timestamps))) if len(timestamps) > 1 else (1.0 / fps_out if fps_out else 0.0)\n",
    "    running = {i: 0.0 for i in range(len(class_names))}\n",
    "\n",
    "    for i, frame in enumerate(frames_bgr):\n",
    "        pred_id = int(pred_ids[i])\n",
    "        running[pred_id] += frame_dt\n",
    "\n",
    "        header_lines = [f\"t={timestamps[i]:.2f}s\", f\"Pred: {class_names[pred_id]}\"]\n",
    "        if gt_ids is not None:\n",
    "            header_lines.append(f\"GT: {class_names[int(gt_ids[i])]}\")\n",
    "        wash_total = sum(running[j] for j, name in enumerate(class_names) if name.lower() != \"other\")\n",
    "        header_lines.append(f\"Wash total: {wash_total:.1f}s\")\n",
    "        draw_text_block(frame, header_lines, origin=(10, 30))\n",
    "\n",
    "        prob_lines = []\n",
    "        for j, name in enumerate(class_names):\n",
    "            marker = \">\" if j == pred_id else \" \"\n",
    "            prob_lines.append(f\"{marker} {name}: {probs[i, j]:.2f}\")\n",
    "        x_right = max(10, w - 320)\n",
    "        draw_text_block(frame, prob_lines, origin=(x_right, 30))\n",
    "\n",
    "        writer.write(frame)\n",
    "\n",
    "    if summary and summary_seconds > 0:\n",
    "        summary_lines = [\"Summary\", f\"Total wash time: {summary['total_wash_s']:.1f}s\"]\n",
    "        for name in class_names:\n",
    "            done_tag = \"done\" if summary['steps_done'].get(name, False) else \"miss\"\n",
    "            summary_lines.append(f\"{name}: {summary['per_class_s'][name]:.1f}s ({done_tag})\")\n",
    "        base = frames_bgr[-1].copy()\n",
    "        num_frames = int(max(1, fps_out * summary_seconds))\n",
    "        for _ in range(num_frames):\n",
    "            frame = base.copy()\n",
    "            draw_text_block(frame, summary_lines, origin=(10, 30))\n",
    "            writer.write(frame)\n",
    "\n",
    "    writer.release()\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_video_pipeline(\n",
    "    model,\n",
    "    video_path,\n",
    "    gt_class_id=None,\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    augment=False,\n",
    "    consistent_aug=True,\n",
    "    frame_stride=1,\n",
    "    batch_size=32,\n",
    "    sequence_length=None,\n",
    "    sequence_stride=SEQUENCE_STRIDE,\n",
    "    sequence_batch_size=SEQUENCE_BATCH_SIZE,\n",
    "    smoothing_method=\"moving_avg\",\n",
    "    smoothing_window=7,\n",
    "    min_step_seconds=1.0,\n",
    "    max_frames=None,\n",
    "):\n",
    "    fps, inputs, frames_bgr, timestamps = collect_video_frames(\n",
    "        video_path,\n",
    "        frame_stride=frame_stride,\n",
    "        max_frames=max_frames,\n",
    "        augment=augment,\n",
    "        consistent_aug=consistent_aug,\n",
    "    )\n",
    "    if len(inputs) == 0:\n",
    "        print(\"No frames extracted from\", video_path)\n",
    "        return {}\n",
    "\n",
    "    probs_raw = predict_model_probs(\n",
    "        model,\n",
    "        inputs,\n",
    "        batch_size=batch_size,\n",
    "        sequence_length=sequence_length,\n",
    "        sequence_stride=sequence_stride,\n",
    "        sequence_batch_size=sequence_batch_size,\n",
    "    )\n",
    "    probs_smoothed, pred_ids = apply_smoothing(probs_raw, method=smoothing_method, window=smoothing_window)\n",
    "\n",
    "    gt_ids = None\n",
    "    if gt_class_id is not None:\n",
    "        if isinstance(gt_class_id, (list, tuple, np.ndarray)):\n",
    "            gt_ids = np.asarray(gt_class_id, dtype=np.int32)\n",
    "            if len(gt_ids) != len(pred_ids):\n",
    "                raise ValueError(\"gt_class_id length does not match predictions\")\n",
    "        else:\n",
    "            gt_ids = np.full(len(pred_ids), int(gt_class_id), dtype=np.int32)\n",
    "\n",
    "    segments = build_segments(pred_ids, timestamps, fps, frame_stride, CLASS_NAMES)\n",
    "    totals, total_wash = summarize_durations(segments, CLASS_NAMES)\n",
    "    summary = {\n",
    "        \"per_class_s\": totals,\n",
    "        \"total_wash_s\": total_wash,\n",
    "        \"steps_done\": {name: (totals[name] >= min_step_seconds) for name in CLASS_NAMES},\n",
    "    }\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    suffix = \"aug\" if augment else \"raw\"\n",
    "    out_path = out_dir / f\"{Path(video_path).stem}_{suffix}_pred.mp4\"\n",
    "    fps_out = fps / frame_stride if frame_stride else fps\n",
    "    output_video = write_annotated_video(\n",
    "        frames_bgr,\n",
    "        fps_out,\n",
    "        timestamps,\n",
    "        pred_ids,\n",
    "        probs_smoothed,\n",
    "        CLASS_NAMES,\n",
    "        gt_ids=gt_ids,\n",
    "        summary=summary,\n",
    "        out_path=out_path,\n",
    "    )\n",
    "\n",
    "    timeline_df = build_timeline_df(timestamps, probs_raw, pred_ids, CLASS_NAMES, gt_ids, probs_smoothed=probs_smoothed)\n",
    "    metrics = evaluate_alignment(gt_ids, pred_ids, CLASS_NAMES) if gt_ids is not None else {}\n",
    "\n",
    "    return {\n",
    "        \"video_path\": str(video_path),\n",
    "        \"output_path\": str(output_video) if output_video else None,\n",
    "        \"timeline\": timeline_df,\n",
    "        \"segments\": segments,\n",
    "        \"summary\": summary,\n",
    "        \"metrics\": metrics,\n",
    "        \"fps\": fps,\n",
    "        \"frame_stride\": frame_stride,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "FRAME_STRIDE = 2\n",
    "BATCH_SIZE = 32\n",
    "SMOOTHING_METHOD = \"moving_avg\"  # \"none\", \"moving_avg\", or \"majority\"\n",
    "SMOOTHING_WINDOW = 7\n",
    "MAX_VIDEOS_PER_SPLIT = 2\n",
    "\n",
    "sample_row = train_df.sample(1, random_state=cfg.RANDOM_SEED).iloc[0]\n",
    "result = run_video_pipeline(\n",
    "    model,\n",
    "    sample_row[\"video_path\"],\n",
    "    gt_class_id=sample_row[\"class_id\"],\n",
    "    augment=False,\n",
    "    frame_stride=FRAME_STRIDE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    smoothing_method=SMOOTHING_METHOD,\n",
    "    smoothing_window=SMOOTHING_WINDOW,\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(\"Output video:\", result[\"output_path\"])\n",
    "    display(Video(result[\"output_path\"], embed=True, width=360))\n",
    "    timeline = result[\"timeline\"]\n",
    "    plot_time_series(\n",
    "        timeline[\"timestamp_s\"].values,\n",
    "        timeline[\"gt_class_id\"].values if \"gt_class_id\" in timeline else None,\n",
    "        timeline[\"pred_class_id\"].values,\n",
    "        CLASS_NAMES,\n",
    "    )\n",
    "    print(\"Summary:\", result[\"summary\"])\n",
    "    print(\"Steps done:\", result[\"summary\"].get(\"steps_done\"))\n",
    "    if result.get(\"metrics\"):\n",
    "        print(\"Frame accuracy:\", result[\"metrics\"].get(\"frame_accuracy\"))\n",
    "        print(\"Edit distance:\", result[\"metrics\"].get(\"edit_distance\"))\n",
    "        print(\"DTW distance:\", result[\"metrics\"].get(\"dtw_distance\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_split(df, split_name, augment):\n",
    "    results = []\n",
    "    sample = df.sample(min(MAX_VIDEOS_PER_SPLIT, len(df)), random_state=cfg.RANDOM_SEED)\n",
    "    for row in sample.itertuples():\n",
    "        print(f\"[{split_name}] {row.video_path} ({row.class_name}) augment={augment}\")\n",
    "        res = run_video_pipeline(\n",
    "            model,\n",
    "            row.video_path,\n",
    "            gt_class_id=row.class_id,\n",
    "            augment=augment,\n",
    "            frame_stride=FRAME_STRIDE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            smoothing_method=SMOOTHING_METHOD,\n",
    "            smoothing_window=SMOOTHING_WINDOW,\n",
    "        )\n",
    "        if not res:\n",
    "            continue\n",
    "        results.append(res)\n",
    "        if res.get(\"output_path\"):\n",
    "            display(Video(res[\"output_path\"], embed=True, width=360))\n",
    "        timeline = res[\"timeline\"]\n",
    "        plot_time_series(\n",
    "            timeline[\"timestamp_s\"].values,\n",
    "            timeline[\"gt_class_id\"].values if \"gt_class_id\" in timeline else None,\n",
    "            timeline[\"pred_class_id\"].values,\n",
    "            CLASS_NAMES,\n",
    "        )\n",
    "        if res.get(\"metrics\"):\n",
    "            print(\"Frame accuracy:\", res[\"metrics\"].get(\"frame_accuracy\"))\n",
    "            print(\"Edit distance:\", res[\"metrics\"].get(\"edit_distance\"))\n",
    "            print(\"DTW distance:\", res[\"metrics\"].get(\"dtw_distance\"))\n",
    "            print(\"Mean temporal IoU:\", res[\"metrics\"].get(\"temporal_iou_mean\"))\n",
    "        print(\"Summary:\", res[\"summary\"])\n",
    "        print(\"Steps done:\", res[\"summary\"].get(\"steps_done\"))\n",
    "    return results\n",
    "\n",
    "\n",
    "all_results = []\n",
    "for split_name, df in [(\"train\", train_df), (\"val\", val_df)]:\n",
    "    for augment in (False, True):\n",
    "        all_results.extend(run_split(df, split_name, augment))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    all_gt = []\n",
    "    all_pred = []\n",
    "    for res in all_results:\n",
    "        timeline = res.get(\"timeline\")\n",
    "        if timeline is None or \"gt_class_id\" not in timeline:\n",
    "            continue\n",
    "        all_gt.extend(timeline[\"gt_class_id\"].tolist())\n",
    "        all_pred.extend(timeline[\"pred_class_id\"].tolist())\n",
    "\n",
    "    if all_gt:\n",
    "        print(\"Overall frame accuracy:\", accuracy_score(all_gt, all_pred))\n",
    "        report = classification_report(all_gt, all_pred, labels=list(range(len(CLASS_NAMES))), target_names=CLASS_NAMES, zero_division=0)\n",
    "        print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas to make single-frame inference more robust on time series\n",
    "- Sliding-window majority vote over predicted class IDs (temporal smoothing)\n",
    "- Moving average or exponential smoothing over logits/probabilities before argmax\n",
    "- Hysteresis thresholds: require a class to persist for N frames before switching\n",
    "- Enforce minimum duration per step (drop very short segments as noise)\n",
    "- Use a lightweight temporal model on top of frame features (GRU/LSTM/TCN)\n",
    "- Apply a hidden Markov model (HMM) with a transition prior for valid step order\n",
    "- Confidence calibration (temperature scaling) to reduce jitter\n",
    "- Train a change-point detector to explicitly segment step boundaries\n",
    "- Use optical-flow-based motion gating to suppress static false positives\n",
    "- Self-training: refine temporal consistency by re-labeling with smoothing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}