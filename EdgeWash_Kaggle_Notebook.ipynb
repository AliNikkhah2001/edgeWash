{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0875c8f",
   "metadata": {},
   "source": [
    "# EdgeWash Kaggle Notebook \n",
    "\n",
    "This notebook downloads the Kaggle hand-wash dataset subset, preprocesses frames, optionally computes optical flow, and trains the EdgeWash CNN classifiers. Each step is heavily commented so you can adapt hyperparameters or swap architectures quickly in a Kaggle environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a499f4",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "We install Python dependencies listed in `requirements.txt` and make sure `ffmpeg` is available for video processing. Kaggle images already ship with CUDA-enabled TensorFlow, so the install is fast.\n",
    "\n",
    "*Tip:* Re-run this cell if you change dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d79d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q -r requirements.txt\n",
    "which ffmpeg || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fac184",
   "metadata": {},
   "source": [
    "## 2. Define paths and hyperparameters\n",
    "\n",
    "We collect every important configurable value in one place. Setting environment variables keeps the training code aligned with the repository scripts (e.g., `classify_dataset.py`).\n",
    "\n",
    "* `USE_OPTICAL_FLOW`: toggle the two-stream model (RGB + optical flow).\n",
    "* `NUM_EPOCHS`, `NUM_LAYERS`, etc.: mirror the `HANDWASH_*` variables used by the training helpers.\n",
    "* `DATA_ROOT`: where we download and preprocess the dataset (defaults to `/kaggle/working`).\n",
    "\n",
    "You can edit values in the dictionary before running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pathlib\n",
    "\n",
    "CONFIG = {\n",
    "    \"DATA_ROOT\": \"/kaggle/working/edgewash_data\",\n",
    "    \"USE_OPTICAL_FLOW\": False,  # set True to train the merged two-stream network\n",
    "    \"HANDWASH_NN\": \"MobileNetV2\",  # options: MobileNetV2, InceptionV3, Xception\n",
    "    \"HANDWASH_NUM_LAYERS\": 0,\n",
    "    \"HANDWASH_NUM_EPOCHS\": 10,\n",
    "    \"HANDWASH_NUM_FRAMES\": 5,  # only used for the time-distributed GRU model\n",
    "    \"HANDWASH_EXTRA_LAYERS\": 0,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"IMG_HEIGHT\": 240,\n",
    "    \"IMG_WIDTH\": 320,\n",
    "}\n",
    "\n",
    "# Export environment variables so downstream scripts pick them up\n",
    "for key, value in CONFIG.items():\n",
    "    if key.startswith(\"HANDWASH_\"):\n",
    "        os.environ[key] = str(value)\n",
    "\n",
    "root = pathlib.Path(CONFIG[\"DATA_ROOT\"])\n",
    "root.mkdir(parents=True, exist_ok=True)\n",
    "print(json.dumps(CONFIG, indent=2))\n",
    "print(\"Environment variables applied.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2051b01",
   "metadata": {},
   "source": [
    "## 3. Download the Kaggle hand-wash subset\n",
    "\n",
    "The repository ships a helper script (`dataset-kaggle/get-and-preprocess-dataset.sh`) that fetches a reorganized 7-class subset of the public Kaggle hand-wash dataset. The cell below mirrors that logic with inline Python so the notebook stays self-contained.\n",
    "\n",
    "Artifacts created:\n",
    "* `kaggle-dataset-6classes.tar` — downloaded archive\n",
    "* `kaggle-dataset-6classes/` — raw videos sorted into 7 class folders\n",
    "\n",
    "Run the cell once; it skips work if files already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, tarfile, urllib.request\n",
    "\n",
    "data_root = pathlib.Path(CONFIG[\"DATA_ROOT\"]).resolve()\n",
    "raw_tar = data_root / \"kaggle-dataset-6classes.tar\"\n",
    "raw_dir = data_root / \"kaggle-dataset-6classes\"\n",
    "\n",
    "url = \"https://github.com/atiselsts/data/raw/master/kaggle-dataset-6classes.tar\"\n",
    "if not raw_tar.exists():\n",
    "    print(\"Downloading dataset archive...\")\n",
    "    urllib.request.urlretrieve(url, raw_tar)\n",
    "else:\n",
    "    print(\"Archive already present:\", raw_tar)\n",
    "\n",
    "if not raw_dir.exists():\n",
    "    print(\"Extracting archive...\")\n",
    "    with tarfile.open(raw_tar, \"r\") as tar:\n",
    "        tar.extractall(data_root)\n",
    "else:\n",
    "    print(\"Extracted directory already exists:\", raw_dir)\n",
    "\n",
    "print(\"Contents:\", list(raw_dir.iterdir())[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252d987",
   "metadata": {},
   "source": [
    "## 4. Frame extraction and train/validation/test split\n",
    "\n",
    "The repository's `dataset-kaggle/separate-frames.py` script splits each class into `trainval` and `test` partitions (70/30) and extracts every video frame. We reuse the same logic here, saving both full videos and per-frame JPEGs.\n",
    "\n",
    "Outputs (under `kaggle-dataset-6classes-preprocessed/`):\n",
    "* `videos/trainval` and `videos/test` — original clips split by partition\n",
    "* `frames/trainval` and `frames/test` — every decoded frame with a class label directory\n",
    "\n",
    "If you already preprocessed once, the cell will skip the heavy work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, cv2, shutil, pathlib\n",
    "\n",
    "random.seed(123)\n",
    "input_dir = raw_dir\n",
    "out_root = data_root / \"kaggle-dataset-6classes-preprocessed\"\n",
    "videos_dir = out_root / \"videos\"\n",
    "frames_dir = out_root / \"frames\"\n",
    "\n",
    "if out_root.exists():\n",
    "    print(\"Preprocessed data already exists at\", out_root)\n",
    "else:\n",
    "    print(\"Creating frame and video splits...\")\n",
    "    for subset in [\"trainval\", \"test\"]:\n",
    "        for base in [videos_dir, frames_dir]:\n",
    "            for cls in range(7):\n",
    "                (base / subset / str(cls)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for class_dir in sorted(os.listdir(input_dir)):\n",
    "        src_cls_path = input_dir / class_dir\n",
    "        if not src_cls_path.is_dir():\n",
    "            continue\n",
    "        for filename in os.listdir(src_cls_path):\n",
    "            if not filename.endswith(\".mp4\"):\n",
    "                continue\n",
    "            subset = \"test\" if random.random() < 0.3 else \"trainval\"\n",
    "            src = src_cls_path / filename\n",
    "            video_target = videos_dir / subset / class_dir / filename\n",
    "            shutil.copy2(src, video_target)\n",
    "\n",
    "            cap = cv2.VideoCapture(str(src))\n",
    "            success, frame = cap.read()\n",
    "            frame_num = 0\n",
    "            while success:\n",
    "                frame_name = f\"frame_{frame_num}_{os.path.splitext(filename)[0]}.jpg\"\n",
    "                frame_path = frames_dir / subset / class_dir / frame_name\n",
    "                cv2.imwrite(str(frame_path), frame)\n",
    "                success, frame = cap.read()\n",
    "                frame_num += 1\n",
    "            cap.release()\n",
    "    print(\"Finished preprocessing!\")\n",
    "\n",
    "print(\"Trainval frame examples:\", len(list((frames_dir/\"trainval\").glob(\"*/*.jpg\"))))\n",
    "print(\"Test frame examples:\", len(list((frames_dir/\"test\").glob(\"*/*.jpg\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd278a18",
   "metadata": {},
   "source": [
    "## 5. Build TensorFlow datasets and normalize inputs\n",
    "\n",
    "We now create TensorFlow `tf.data.Dataset` objects for training, validation, and testing. The helper `dataset_utilities.get_datasets` mirrors the repository code: it performs an 80/20 split of the `trainval` frames, sets labels, and computes class weights to handle imbalance.\n",
    "\n",
    "After loading, we map a normalization step that matches the MobileNet preprocessing used in the models. We also save a combined normalized dataset to disk (`tf.data.experimental.save`) so later runs can reload without repeating preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf868722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from dataset_utilities import get_datasets\n",
    "from classify_dataset import get_preprocessing_function\n",
    "\n",
    "frames_trainval = frames_dir / \"trainval\"\n",
    "frames_test = frames_dir / \"test\"\n",
    "\n",
    "train_ds, val_ds, test_ds, weights = get_datasets(str(frames_trainval), str(frames_test), batch_size=CONFIG[\"BATCH_SIZE\"])\n",
    "\n",
    "preprocess_fn = get_preprocessing_function()\n",
    "\n",
    "def normalize_batch(images, labels):\n",
    "    return preprocess_fn(images), labels\n",
    "\n",
    "train_ds_norm = train_ds.map(normalize_batch)\n",
    "val_ds_norm = val_ds.map(normalize_batch)\n",
    "test_ds_norm = test_ds.map(normalize_batch)\n",
    "\n",
    "norm_root = data_root / \"normalized_dataset\"\n",
    "if not norm_root.exists():\n",
    "    norm_root.mkdir(parents=True)\n",
    "    tf.data.experimental.save(train_ds_norm, norm_root/\"train\")\n",
    "    tf.data.experimental.save(val_ds_norm, norm_root/\"val\")\n",
    "    tf.data.experimental.save(test_ds_norm, norm_root/\"test\")\n",
    "    with open(norm_root/\"README.txt\", \"w\") as f:\n",
    "        f.write(\"Normalized datasets saved with MobileNet preprocessing and categorical labels.\n",
    "\")\n",
    "    print(\"Normalized dataset saved to\", norm_root)\n",
    "else:\n",
    "    print(\"Normalized dataset already saved; skipping save step.\")\n",
    "\n",
    "print(\"Class weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15f419",
   "metadata": {},
   "source": [
    "## 6. (Optional) Compute optical flow for the merged two-stream network\n",
    "\n",
    "Set `CONFIG['USE_OPTICAL_FLOW'] = True` if you want to train the RGB + optical flow model. This step computes Farnebäck dense optical flow between frames spaced by ~1/3 second (matching `calculate-optical-flow.py`).\n",
    "\n",
    "Outputs live in `kaggle-dataset-6classes-preprocessed/of/trainval` and `/test` alongside the RGB frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36434bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if CONFIG[\"USE_OPTICAL_FLOW\"]:\n",
    "    print(\"Optical flow enabled; proceed with extraction below.\")\n",
    "else:\n",
    "    print(\"Optical flow disabled; skip to training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af6e42",
   "metadata": {},
   "source": [
    "### Optical flow helper (embedded)\n",
    "\n",
    "The repository script references a global `input_dir`; the helper below keeps everything scoped inside the notebook and mirrors the same Farnebäck settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_optical_flow(dataset_root, frame_step=10):\n",
    "    import pathlib, numpy as np\n",
    "    videos_path = pathlib.Path(dataset_root)/\"videos\"\n",
    "    output_root = pathlib.Path(dataset_root)/\"of\"\n",
    "    output_root.mkdir(exist_ok=True)\n",
    "    for subset in [\"trainval\", \"test\"]:\n",
    "        subset_in = videos_path/subset\n",
    "        subset_out = output_root/subset\n",
    "        for cls_dir in subset_in.iterdir():\n",
    "            if not cls_dir.is_dir():\n",
    "                continue\n",
    "            (subset_out/cls_dir.name).mkdir(parents=True, exist_ok=True)\n",
    "            for video in tqdm(list(cls_dir.glob(\"*.mp4\")), desc=f\"{subset}-{cls_dir.name}\"):\n",
    "                cap = cv.VideoCapture(str(video))\n",
    "                frames = []\n",
    "                ret, frame = cap.read()\n",
    "                while ret:\n",
    "                    frames.append(cv.cvtColor(frame, cv.COLOR_BGR2GRAY))\n",
    "                    ret, frame = cap.read()\n",
    "                cap.release()\n",
    "                for idx in range(len(frames)-frame_step):\n",
    "                    flow = cv.calcOpticalFlowFarneback(frames[idx], frames[idx+frame_step], None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                    mag, ang = cv.cartToPolar(flow[...,0], flow[...,1])\n",
    "                    mask = np.zeros((frames[idx].shape[0], frames[idx].shape[1],3), dtype=np.float32)\n",
    "                    mask[...,0] = ang*180/np.pi/2\n",
    "                    mask[...,1] = 255\n",
    "                    mask[...,2] = cv.normalize(mag, None, 0, 255, cv.NORM_MINMAX)\n",
    "                    rgb = cv.cvtColor(mask, cv.COLOR_HSV2BGR)\n",
    "                    out_name = subset_out/cls_dir.name/f\"frame_{idx}_{video.stem}.jpg\"\n",
    "                    cv.imwrite(str(out_name), rgb)\n",
    "    print(\"Optical flow extraction complete at\", output_root)\n",
    "\n",
    "if CONFIG[\"USE_OPTICAL_FLOW\"]:\n",
    "    compute_optical_flow(out_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d50f23a",
   "metadata": {},
   "source": [
    "## 7. Train the classifier\n",
    "\n",
    "We reuse `classify_dataset.evaluate` to stay faithful to the repository logic. Choose one of the three architectures by importing the right training script: \n",
    "\n",
    "* Single-frame baseline (used below)\n",
    "* Time-distributed GRU (`kaggle-classify-videos.py`)\n",
    "* Two-stream merged network (`kaggle-classify-merged-network.py`, requires optical flow)\n",
    "\n",
    "Training artifacts saved:\n",
    "* `kaggle-single-framefinal-model/` — SavedModel directory\n",
    "* `results-<name>.txt` — metrics and F1 scores\n",
    "* `accuracy-<name>.pdf` — accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b139cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classify_dataset import evaluate\n",
    "\n",
    "model_name = \"kaggle-single-frame\" if not CONFIG[\"USE_OPTICAL_FLOW\"] else \"kaggle-merged\"\n",
    "\n",
    "evaluate(model_name, train_ds_norm, val_ds_norm, test_ds_norm, weights_dict=weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf45fa",
   "metadata": {},
   "source": [
    "## 8. Evaluate and visualize\n",
    "\n",
    "After training, we can inspect the saved accuracy plot and load the metrics log. We also print a few sample predictions to verify the label ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182504fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "results_file = pathlib.Path(f\"results-{model_name}.txt\")\n",
    "print(results_file.read_text())\n",
    "\n",
    "acc_plot = pathlib.Path(f\"accuracy-{model_name}.pdf\")\n",
    "print(\"Accuracy plot saved to\", acc_plot)\n",
    "\n",
    "sample_images, sample_labels = next(iter(test_ds_norm.take(1)))\n",
    "loaded_model = tf.keras.models.load_model(f\"{model_name}final-model\", custom_objects={\"MobileNetPreprocessingLayer\": None})\n",
    "preds = loaded_model.predict(sample_images)\n",
    "print(\"Predicted class indices (first 10):\", np.argmax(preds, axis=1)[:10])\n",
    "print(\"True class indices (first 10):\", np.argmax(sample_labels.numpy(), axis=1)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e0290",
   "metadata": {},
   "source": [
    "## 9. Export for inference (SavedModel + TFLite)\n",
    "\n",
    "Kaggle notebooks often deploy to mobile or lightweight environments. The following cell saves a TensorFlow Lite version compatible with the MobileNet preprocessing layer defined in `classify_dataset.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from classify_dataset import MobileNetPreprocessingLayer\n",
    "\n",
    "saved_dir = f\"{model_name}final-model\"\n",
    "model = tf.keras.models.load_model(saved_dir, custom_objects={\"MobileNetPreprocessingLayer\": MobileNetPreprocessingLayer})\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_dir)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_model = converter.convert()\n",
    "open(f\"{model_name}.tflite\", \"wb\").write(tflite_model)\n",
    "print(\"Saved TFLite model:\", f\"{model_name}.tflite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb3dc6",
   "metadata": {},
   "source": [
    "## 10. TensorBoard (optional)\n",
    "\n",
    "You can monitor training live by setting `HANDWASH_TENSORBOARD_LOGDIR` before training and launching TensorBoard inside the notebook. Uncomment the block below to enable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['HANDWASH_TENSORBOARD_LOGDIR'] = str(pathlib.Path(CONFIG['DATA_ROOT']) / 'logs')\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir $HANDWASH_TENSORBOARD_LOGDIR\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
