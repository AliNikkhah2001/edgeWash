{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0875c8f",
   "metadata": {},
   "source": [
    "# EdgeWash Kaggle Notebook \n",
    "\n",
    "This notebook downloads the Kaggle hand-wash dataset subset, preprocesses frames, optionally computes optical flow, and trains the EdgeWash CNN classifiers. Each step is heavily commented so you can adapt hyperparameters or swap architectures quickly in a Kaggle environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "We install Python dependencies directly (no external files needed) and make sure `ffmpeg` is available for video processing. Kaggle images already ship with CUDA-enabled TensorFlow, so the install is fast.\n",
    "\n",
    "*Tip:* Re-run this cell if you change dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q --no-input numpy tensorflow opencv-python keras matplotlib streamlit tqdm\n",
    "command -v ffmpeg || (apt-get update -y && apt-get install -y ffmpeg)\n",
    "which ffmpeg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fac184",
   "metadata": {},
   "source": [
    "## 2. Define paths and hyperparameters\n",
    "\n",
    "We collect every important configurable value in one place. Setting environment variables keeps the training code aligned with the repository scripts (e.g., `classify_dataset.py`).\n",
    "\n",
    "* `USE_OPTICAL_FLOW`: toggle the two-stream model (RGB + optical flow).\n",
    "* `NUM_EPOCHS`, `NUM_LAYERS`, etc.: mirror the `HANDWASH_*` variables used by the training helpers.\n",
    "* `DATA_ROOT`: where we download and preprocess the dataset (defaults to `/kaggle/working`).\n",
    "\n",
    "You can edit values in the dictionary before running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pathlib\n",
    "\n",
    "CONFIG = {\n",
    "    \"DATA_ROOT\": \"/kaggle/working/edgewash_data\",\n",
    "    \"USE_OPTICAL_FLOW\": False,  # set True to train the merged two-stream network\n",
    "    \"MODEL_VARIANT\": \"single_frame\",  # options: single_frame, merged\n",
    "    \"HANDWASH_NN\": \"MobileNetV2\",  # options: MobileNetV2, InceptionV3, Xception\n",
    "    \"HANDWASH_NUM_LAYERS\": 0,\n",
    "    \"HANDWASH_NUM_EPOCHS\": 10,\n",
    "    \"HANDWASH_NUM_FRAMES\": 5,  # kept for compatibility with time-distributed models\n",
    "    \"HANDWASH_EXTRA_LAYERS\": 0,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"IMG_HEIGHT\": 240,\n",
    "    \"IMG_WIDTH\": 320,\n",
    "}\n",
    "\n",
    "# Export environment variables so downstream scripts pick them up\n",
    "for key, value in CONFIG.items():\n",
    "    if key.startswith(\"HANDWASH_\"):\n",
    "        os.environ[key] = str(value)\n",
    "\n",
    "root = pathlib.Path(CONFIG[\"DATA_ROOT\"])\n",
    "root.mkdir(parents=True, exist_ok=True)\n",
    "print(json.dumps(CONFIG, indent=2))\n",
    "print(\"Environment variables applied.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# GPU safety: enable memory growth if available\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Pull hyperparameters from CONFIG\n",
    "batch_size = CONFIG[\"BATCH_SIZE\"]\n",
    "IMG_SIZE = (CONFIG[\"IMG_HEIGHT\"], CONFIG[\"IMG_WIDTH\"])\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "N_CLASSES = 7\n",
    "model_name = CONFIG[\"HANDWASH_NN\"]\n",
    "num_trainable_layers = int(CONFIG[\"HANDWASH_NUM_LAYERS\"])\n",
    "num_epochs = int(CONFIG[\"HANDWASH_NUM_EPOCHS\"])\n",
    "num_frames = int(CONFIG[\"HANDWASH_NUM_FRAMES\"])\n",
    "num_extra_layers = int(CONFIG[\"HANDWASH_EXTRA_LAYERS\"])\n",
    "log_dir = os.getenv(\"HANDWASH_TENSORBOARD_LOGDIR\", \"\")\n",
    "\n",
    "# Data augmentation block reused across models\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "def freeze_model(model):\n",
    "    if num_trainable_layers == 0:\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        return False\n",
    "    elif num_trainable_layers > 0:\n",
    "        for layer in model.layers[:-num_trainable_layers]:\n",
    "            layer.trainable = False\n",
    "        for layer in model.layers[-num_trainable_layers:]:\n",
    "            layer.trainable = True\n",
    "        return True\n",
    "    else:\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "        return True\n",
    "\n",
    "def get_preprocessing_function():\n",
    "    if model_name == \"MobileNetV2\":\n",
    "        return tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    elif model_name == \"InceptionV3\":\n",
    "        return tf.keras.applications.inception_v3.preprocess_input\n",
    "    elif model_name == \"Xception\":\n",
    "        return tf.keras.applications.xception.preprocess_input\n",
    "    return None\n",
    "\n",
    "class MobileNetPreprocessingLayer(Layer):\n",
    "    def call(self, x):\n",
    "        return (x / 127.5) - 1.0\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def get_default_model():\n",
    "    if model_name == \"MobileNetV2\":\n",
    "        base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "    elif model_name == \"InceptionV3\":\n",
    "        base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "    elif model_name == \"Xception\":\n",
    "        base_model = tf.keras.applications.Xception(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name {model_name}\")\n",
    "\n",
    "    training = freeze_model(base_model)\n",
    "    inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "    x = data_augmentation(inputs)\n",
    "    x = get_preprocessing_function()(x)\n",
    "    x = base_model(x, training=training)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    if num_extra_layers:\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    for _ in range(num_extra_layers):\n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(N_CLASSES, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def get_merged_model():\n",
    "    if model_name == \"MobileNetV2\":\n",
    "        rgb_base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "        of_base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "    elif model_name == \"InceptionV3\":\n",
    "        rgb_base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "        of_base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "    elif model_name == \"Xception\":\n",
    "        rgb_base_model = tf.keras.applications.Xception(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "        of_base_model = tf.keras.applications.Xception(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name {model_name}\")\n",
    "\n",
    "    training = freeze_model(rgb_base_model)\n",
    "    freeze_model(of_base_model)\n",
    "\n",
    "    rgb_input = tf.keras.Input(shape=IMG_SHAPE)\n",
    "    rgb_branch = data_augmentation(rgb_input)\n",
    "    rgb_branch = get_preprocessing_function()(rgb_branch)\n",
    "    rgb_branch = rgb_base_model(rgb_branch, training=training)\n",
    "    rgb_branch = tf.keras.layers.Flatten()(rgb_branch)\n",
    "\n",
    "    of_input = tf.keras.Input(shape=IMG_SHAPE)\n",
    "    of_branch = data_augmentation(of_input)\n",
    "    of_branch = get_preprocessing_function()(of_branch)\n",
    "    of_branch = of_base_model(of_branch, training=training)\n",
    "    of_branch = tf.keras.layers.Flatten()(of_branch)\n",
    "\n",
    "    merged = tf.keras.layers.concatenate([rgb_branch, of_branch], axis=1)\n",
    "    for _ in range(num_extra_layers):\n",
    "        merged = tf.keras.layers.Dense(128, activation='relu')(merged)\n",
    "        merged = tf.keras.layers.Dropout(0.2)(merged)\n",
    "    merged = tf.keras.layers.Dense(N_CLASSES, activation='softmax')(merged)\n",
    "    model = tf.keras.Model([rgb_input, of_input], merged)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def compute_class_weights(data_dir, class_names):\n",
    "    counts = []\n",
    "    for cls in class_names:\n",
    "        cls_dir = Path(data_dir)/cls\n",
    "        counts.append(len([p for p in cls_dir.glob('*.jpg')]))\n",
    "    counts = np.array(counts)\n",
    "    avg = np.average(counts) if counts.size else 1.0\n",
    "    weights = avg / counts\n",
    "    return {i: float(w) for i, w in enumerate(weights)}\n",
    "\n",
    "def build_single_stream_datasets(trainval_dir, test_dir, batch_size=batch_size, seed=123):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        trainval_dir, validation_split=0.2, subset=\"training\", seed=seed,\n",
    "        image_size=IMG_SIZE, label_mode='categorical', crop_to_aspect_ratio=False, batch_size=batch_size)\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        trainval_dir, validation_split=0.2, subset=\"validation\", seed=seed,\n",
    "        image_size=IMG_SIZE, label_mode='categorical', crop_to_aspect_ratio=False, batch_size=batch_size)\n",
    "    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        test_dir, seed=seed, image_size=IMG_SIZE, label_mode='categorical',\n",
    "        crop_to_aspect_ratio=False, batch_size=batch_size)\n",
    "    weights_dict = compute_class_weights(Path(trainval_dir), train_ds.class_names)\n",
    "    return (train_ds.prefetch(AUTOTUNE), val_ds.prefetch(AUTOTUNE), test_ds.prefetch(AUTOTUNE), weights_dict, train_ds.class_names)\n",
    "\n",
    "def _split_indices(n, seed=123, val_frac=0.2):\n",
    "    idxs = np.arange(n)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(idxs)\n",
    "    split = int(n * (1 - val_frac))\n",
    "    return idxs[:split], idxs[split:]\n",
    "\n",
    "def _load_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    return img\n",
    "\n",
    "def build_two_stream_datasets(rgb_root, of_root, batch_size=batch_size, seed=123):\n",
    "    rgb_root = Path(rgb_root)\n",
    "    of_root = Path(of_root)\n",
    "    class_names = sorted([p.name for p in (of_root/\"trainval\").iterdir() if p.is_dir()])\n",
    "\n",
    "    def gather_paths(split_root):\n",
    "        rgb_paths, of_paths, labels = [], [], []\n",
    "        for idx, cls in enumerate(class_names):\n",
    "            of_files = sorted((split_root/cls).glob('*.jpg'))\n",
    "            for of_path in of_files:\n",
    "                of_paths.append(str(of_path))\n",
    "                rgb_paths.append(str(Path(str(of_path)).as_posix().replace(str(of_root), str(rgb_root))))\n",
    "                labels.append(idx)\n",
    "        return np.array(rgb_paths), np.array(of_paths), np.array(labels, dtype=np.int32)\n",
    "\n",
    "    rgb_trainval, of_trainval, labels_trainval = gather_paths(of_root/\"trainval\")\n",
    "    rgb_test, of_test, labels_test = gather_paths(of_root/\"test\")\n",
    "\n",
    "    train_idx, val_idx = _split_indices(len(labels_trainval), seed=seed)\n",
    "\n",
    "    def make_dataset(rgb_list, of_list, label_list, shuffle=True):\n",
    "        ds = tf.data.Dataset.from_tensor_slices((rgb_list, of_list, label_list))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(label_list), seed=seed)\n",
    "        def load_pair(rgb_path, of_path, label):\n",
    "            rgb_img = _load_image(rgb_path)\n",
    "            of_img = _load_image(of_path)\n",
    "            return (rgb_img, of_img), tf.one_hot(label, depth=len(class_names))\n",
    "        return ds.map(load_pair, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds = make_dataset(rgb_trainval[train_idx], of_trainval[train_idx], labels_trainval[train_idx])\n",
    "    val_ds = make_dataset(rgb_trainval[val_idx], of_trainval[val_idx], labels_trainval[val_idx], shuffle=False)\n",
    "    test_ds = make_dataset(rgb_test, of_test, labels_test, shuffle=False)\n",
    "\n",
    "    weights_dict = compute_class_weights(of_root/\"trainval\", class_names)\n",
    "    return train_ds, val_ds, test_ds, weights_dict, class_names\n",
    "\n",
    "def fit_model(name, model, train_ds, val_ds, test_ds, weights_dict):\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "    callbacks = [es]\n",
    "    if len(log_dir):\n",
    "        callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch='5,10'))\n",
    "\n",
    "    history = model.fit(train_ds, epochs=num_epochs, validation_data=val_ds, class_weight=weights_dict, callbacks=callbacks)\n",
    "    model.save(name + \"final-model\")\n",
    "\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.grid(True, axis=\"y\")\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()), 1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.savefig(f\"accuracy-{name}.pdf\", format=\"pdf\")\n",
    "\n",
    "    measure_performance(\"validation\", name, model, val_ds)\n",
    "    test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "    result_str = f'Test loss: {test_loss} accuracy: {test_accuracy}\n",
    "'\n",
    "    print(result_str)\n",
    "    with open(f\"results-{name}.txt\", \"a+\") as f:\n",
    "        f.write(result_str)\n",
    "    measure_performance(\"test\", name, model, test_ds)\n",
    "\n",
    "\n",
    "def measure_performance(ds_name, name, model, ds, num_classes=N_CLASSES):\n",
    "    matrix = [[0] * num_classes for _ in range(num_classes)]\n",
    "    y_predicted, y_true = [], []\n",
    "    for images, labels in ds:\n",
    "        preds = model.predict(images)\n",
    "        for y_p, y_t in zip(preds, labels):\n",
    "            y_predicted.append(int(np.argmax(y_p)))\n",
    "            y_true.append(int(np.argmax(y_t)))\n",
    "        gc.collect()\n",
    "    for y_p, y_t in zip(y_predicted, y_true):\n",
    "        matrix[y_t][y_p] += 1\n",
    "    print(\"Confusion matrix:\")\n",
    "    for row in matrix:\n",
    "        print(row)\n",
    "    f1_scores = []\n",
    "    for i in range(num_classes):\n",
    "        total = sum(matrix[i])\n",
    "        true_predictions = matrix[i][i]\n",
    "        total_predictions = sum([matrix[j][i] for j in range(num_classes)])\n",
    "        precision = true_predictions / total if total else 0\n",
    "        recall = true_predictions / total_predictions if total_predictions else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        print(f\"{i} precision={100*precision:.2f}% recall={100*recall:.2f}% f1={f1:.2f}\")\n",
    "        f1_scores.append(f1)\n",
    "    summary = f\"Average {ds_name} F1 score: {np.mean(f1_scores):.2f}\n",
    "\"\n",
    "    print(summary)\n",
    "    with open(f\"results-{name}.txt\", \"a+\") as f:\n",
    "        f.write(summary)\n",
    "\n",
    "\n",
    "def evaluate(name, train_ds, val_ds, test_ds, weights_dict, model=None):\n",
    "    if model is None:\n",
    "        model = get_default_model()\n",
    "    model.compile(optimizer='Adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    with open(f\"results-{name}.txt\", \"a+\") as f:\n",
    "        pass\n",
    "    fit_model(name, model, train_ds, val_ds, test_ds, weights_dict)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2051b01",
   "metadata": {},
   "source": [
    "## 3. Download the Kaggle hand-wash subset\n",
    "\n",
    "The repository ships a helper script (`dataset-kaggle/get-and-preprocess-dataset.sh`) that fetches a reorganized 7-class subset of the public Kaggle hand-wash dataset. The cell below mirrors that logic with inline Python so the notebook stays self-contained.\n",
    "\n",
    "Artifacts created:\n",
    "* `kaggle-dataset-6classes.tar` — downloaded archive\n",
    "* `kaggle-dataset-6classes/` — raw videos sorted into 7 class folders\n",
    "\n",
    "Run the cell once; it skips work if files already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, tarfile, urllib.request\n",
    "\n",
    "data_root = pathlib.Path(CONFIG[\"DATA_ROOT\"]).resolve()\n",
    "raw_tar = data_root / \"kaggle-dataset-6classes.tar\"\n",
    "raw_dir = data_root / \"kaggle-dataset-6classes\"\n",
    "\n",
    "url = \"https://github.com/atiselsts/data/raw/master/kaggle-dataset-6classes.tar\"\n",
    "if not raw_tar.exists():\n",
    "    print(\"Downloading dataset archive...\")\n",
    "    urllib.request.urlretrieve(url, raw_tar)\n",
    "else:\n",
    "    print(\"Archive already present:\", raw_tar)\n",
    "\n",
    "if not raw_dir.exists():\n",
    "    print(\"Extracting archive...\")\n",
    "    with tarfile.open(raw_tar, \"r\") as tar:\n",
    "        tar.extractall(data_root)\n",
    "else:\n",
    "    print(\"Extracted directory already exists:\", raw_dir)\n",
    "\n",
    "print(\"Contents:\", list(raw_dir.iterdir())[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252d987",
   "metadata": {},
   "source": [
    "## 4. Frame extraction and train/validation/test split\n",
    "\n",
    "The repository's `dataset-kaggle/separate-frames.py` script splits each class into `trainval` and `test` partitions (70/30) and extracts every video frame. We reuse the same logic here, saving both full videos and per-frame JPEGs.\n",
    "\n",
    "Outputs (under `kaggle-dataset-6classes-preprocessed/`):\n",
    "* `videos/trainval` and `videos/test` — original clips split by partition\n",
    "* `frames/trainval` and `frames/test` — every decoded frame with a class label directory\n",
    "\n",
    "If you already preprocessed once, the cell will skip the heavy work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, cv2, shutil, pathlib\n",
    "\n",
    "random.seed(123)\n",
    "input_dir = raw_dir\n",
    "out_root = data_root / \"kaggle-dataset-6classes-preprocessed\"\n",
    "videos_dir = out_root / \"videos\"\n",
    "frames_dir = out_root / \"frames\"\n",
    "\n",
    "if out_root.exists():\n",
    "    print(\"Preprocessed data already exists at\", out_root)\n",
    "else:\n",
    "    print(\"Creating frame and video splits...\")\n",
    "    for subset in [\"trainval\", \"test\"]:\n",
    "        for base in [videos_dir, frames_dir]:\n",
    "            for cls in range(7):\n",
    "                (base / subset / str(cls)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for class_dir in sorted(os.listdir(input_dir)):\n",
    "        src_cls_path = input_dir / class_dir\n",
    "        if not src_cls_path.is_dir():\n",
    "            continue\n",
    "        for filename in os.listdir(src_cls_path):\n",
    "            if not filename.endswith(\".mp4\"):\n",
    "                continue\n",
    "            subset = \"test\" if random.random() < 0.3 else \"trainval\"\n",
    "            src = src_cls_path / filename\n",
    "            video_target = videos_dir / subset / class_dir / filename\n",
    "            shutil.copy2(src, video_target)\n",
    "\n",
    "            cap = cv2.VideoCapture(str(src))\n",
    "            success, frame = cap.read()\n",
    "            frame_num = 0\n",
    "            while success:\n",
    "                frame_name = f\"frame_{frame_num}_{os.path.splitext(filename)[0]}.jpg\"\n",
    "                frame_path = frames_dir / subset / class_dir / frame_name\n",
    "                cv2.imwrite(str(frame_path), frame)\n",
    "                success, frame = cap.read()\n",
    "                frame_num += 1\n",
    "            cap.release()\n",
    "    print(\"Finished preprocessing!\")\n",
    "\n",
    "print(\"Trainval frame examples:\", len(list((frames_dir/\"trainval\").glob(\"*/*.jpg\"))))\n",
    "print(\"Test frame examples:\", len(list((frames_dir/\"test\").glob(\"*/*.jpg\"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.data import AUTOTUNE\n",
    "\n",
    "frames_trainval = frames_dir / \"trainval\"\n",
    "frames_test = frames_dir / \"test\"\n",
    "\n",
    "preprocess_fn = get_preprocessing_function()\n",
    "\n",
    "if CONFIG[\"MODEL_VARIANT\"] == \"merged\" or CONFIG[\"USE_OPTICAL_FLOW\"]:\n",
    "    of_root = frames_dir.parent / \"of\"\n",
    "    if not of_root.exists():\n",
    "        raise FileNotFoundError(\"Optical flow not found. Run the optical flow cell with USE_OPTICAL_FLOW=True.\")\n",
    "    train_ds, val_ds, test_ds, weights, class_names = build_two_stream_datasets(frames_trainval, of_root, batch_size=CONFIG[\"BATCH_SIZE\"])\n",
    "    def normalize_two_stream(rgb_img, of_img, labels):\n",
    "        return (preprocess_fn(rgb_img), preprocess_fn(of_img)), labels\n",
    "    train_ds_norm = train_ds.map(lambda inputs, labels: normalize_two_stream(inputs[0], inputs[1], labels), num_parallel_calls=AUTOTUNE)\n",
    "    val_ds_norm = val_ds.map(lambda inputs, labels: normalize_two_stream(inputs[0], inputs[1], labels), num_parallel_calls=AUTOTUNE)\n",
    "    test_ds_norm = test_ds.map(lambda inputs, labels: normalize_two_stream(inputs[0], inputs[1], labels), num_parallel_calls=AUTOTUNE)\n",
    "else:\n",
    "    train_ds, val_ds, test_ds, weights, class_names = build_single_stream_datasets(str(frames_trainval), str(frames_test), batch_size=CONFIG[\"BATCH_SIZE\"])\n",
    "    def normalize_batch(images, labels):\n",
    "        return preprocess_fn(images), labels\n",
    "    train_ds_norm = train_ds.map(normalize_batch, num_parallel_calls=AUTOTUNE)\n",
    "    val_ds_norm = val_ds.map(normalize_batch, num_parallel_calls=AUTOTUNE)\n",
    "    test_ds_norm = test_ds.map(normalize_batch, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "print(\"Class names:\", class_names)\n",
    "print(\"Class weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15f419",
   "metadata": {},
   "source": [
    "## 6. (Optional) Compute optical flow for the merged two-stream network\n",
    "\n",
    "Set `CONFIG['USE_OPTICAL_FLOW'] = True` if you want to train the RGB + optical flow model. This step computes Farnebäck dense optical flow between frames spaced by ~1/3 second (matching `calculate-optical-flow.py`).\n",
    "\n",
    "Outputs live in `kaggle-dataset-6classes-preprocessed/of/trainval` and `/test` alongside the RGB frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36434bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if CONFIG[\"USE_OPTICAL_FLOW\"]:\n",
    "    print(\"Optical flow enabled; proceed with extraction below.\")\n",
    "else:\n",
    "    print(\"Optical flow disabled; skip to training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af6e42",
   "metadata": {},
   "source": [
    "### Optical flow helper (embedded)\n",
    "\n",
    "The repository script references a global `input_dir`; the helper below keeps everything scoped inside the notebook and mirrors the same Farnebäck settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_optical_flow(dataset_root, frame_step=10):\n",
    "    import pathlib, numpy as np\n",
    "    videos_path = pathlib.Path(dataset_root)/\"videos\"\n",
    "    output_root = pathlib.Path(dataset_root)/\"of\"\n",
    "    output_root.mkdir(exist_ok=True)\n",
    "    for subset in [\"trainval\", \"test\"]:\n",
    "        subset_in = videos_path/subset\n",
    "        subset_out = output_root/subset\n",
    "        for cls_dir in subset_in.iterdir():\n",
    "            if not cls_dir.is_dir():\n",
    "                continue\n",
    "            (subset_out/cls_dir.name).mkdir(parents=True, exist_ok=True)\n",
    "            for video in tqdm(list(cls_dir.glob(\"*.mp4\")), desc=f\"{subset}-{cls_dir.name}\"):\n",
    "                cap = cv.VideoCapture(str(video))\n",
    "                frames = []\n",
    "                ret, frame = cap.read()\n",
    "                while ret:\n",
    "                    frames.append(cv.cvtColor(frame, cv.COLOR_BGR2GRAY))\n",
    "                    ret, frame = cap.read()\n",
    "                cap.release()\n",
    "                for idx in range(len(frames)-frame_step):\n",
    "                    flow = cv.calcOpticalFlowFarneback(frames[idx], frames[idx+frame_step], None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                    mag, ang = cv.cartToPolar(flow[...,0], flow[...,1])\n",
    "                    mask = np.zeros((frames[idx].shape[0], frames[idx].shape[1],3), dtype=np.float32)\n",
    "                    mask[...,0] = ang*180/np.pi/2\n",
    "                    mask[...,1] = 255\n",
    "                    mask[...,2] = cv.normalize(mag, None, 0, 255, cv.NORM_MINMAX)\n",
    "                    rgb = cv.cvtColor(mask, cv.COLOR_HSV2BGR)\n",
    "                    out_name = subset_out/cls_dir.name/f\"frame_{idx}_{video.stem}.jpg\"\n",
    "                    cv.imwrite(str(out_name), rgb)\n",
    "    print(\"Optical flow extraction complete at\", output_root)\n",
    "\n",
    "if CONFIG[\"USE_OPTICAL_FLOW\"]:\n",
    "    compute_optical_flow(out_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d50f23a",
   "metadata": {},
   "source": [
    "## 7. Train the classifier\n",
    "\n",
    "We reuse `classify_dataset.evaluate` to stay faithful to the repository logic. Choose one of the three architectures by importing the right training script: \n",
    "\n",
    "* Single-frame baseline (used below)\n",
    "* Time-distributed GRU (`kaggle-classify-videos.py`)\n",
    "* Two-stream merged network (`kaggle-classify-merged-network.py`, requires optical flow)\n",
    "\n",
    "Training artifacts saved:\n",
    "* `kaggle-single-framefinal-model/` — SavedModel directory\n",
    "* `results-<name>.txt` — metrics and F1 scores\n",
    "* `accuracy-<name>.pdf` — accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant = \"merged\" if CONFIG[\"USE_OPTICAL_FLOW\"] else CONFIG.get(\"MODEL_VARIANT\", \"single_frame\")\n",
    "model_name = \"kaggle-\" + variant\n",
    "\n",
    "if variant == \"merged\":\n",
    "    model = get_merged_model()\n",
    "else:\n",
    "    model = get_default_model()\n",
    "\n",
    "trained_model = evaluate(model_name, train_ds_norm, val_ds_norm, test_ds_norm, weights_dict=weights, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf45fa",
   "metadata": {},
   "source": [
    "## 8. Evaluate and visualize\n",
    "\n",
    "After training, we can inspect the saved accuracy plot and load the metrics log. We also print a few sample predictions to verify the label ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "results_file = pathlib.Path(f\"results-{model_name}.txt\")\n",
    "print(results_file.read_text())\n",
    "\n",
    "acc_plot = pathlib.Path(f\"accuracy-{model_name}.pdf\")\n",
    "print(\"Accuracy plot saved to\", acc_plot)\n",
    "\n",
    "sample_images, sample_labels = next(iter(test_ds_norm.take(1)))\n",
    "loaded_model = tf.keras.models.load_model(f\"{model_name}final-model\", custom_objects={\"MobileNetPreprocessingLayer\": MobileNetPreprocessingLayer})\n",
    "preds = loaded_model.predict(sample_images)\n",
    "print(\"Sample prediction distribution:\", preds[0])\n",
    "print(\"True label one-hot:\", sample_labels[0].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e0290",
   "metadata": {},
   "source": [
    "## 9. Export for inference (SavedModel + TFLite)\n",
    "\n",
    "Kaggle notebooks often deploy to mobile or lightweight environments. The following cell saves a TensorFlow Lite version compatible with the MobileNet preprocessing layer defined in `classify_dataset.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "saved_dir = f\"{model_name}final-model\"\n",
    "model = tf.keras.models.load_model(saved_dir, custom_objects={\"MobileNetPreprocessingLayer\": MobileNetPreprocessingLayer})\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_dir)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_model = converter.convert()\n",
    "Path(f\"{model_name}.tflite\").write_bytes(tflite_model)\n",
    "print(\"TFLite model written to\", f\"{model_name}.tflite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb3dc6",
   "metadata": {},
   "source": [
    "## 10. TensorBoard (optional)\n",
    "\n",
    "You can monitor training live by setting `HANDWASH_TENSORBOARD_LOGDIR` before training and launching TensorBoard inside the notebook. Uncomment the block below to enable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['HANDWASH_TENSORBOARD_LOGDIR'] = str(pathlib.Path(CONFIG['DATA_ROOT']) / 'logs')\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir $HANDWASH_TENSORBOARD_LOGDIR\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}